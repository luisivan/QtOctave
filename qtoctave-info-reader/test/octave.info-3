This is octave.info, produced by makeinfo version 4.11 from ./octave.texi.

START-INFO-DIR-ENTRY
* Octave: (octave).	Interactive language for numerical computations.
END-INFO-DIR-ENTRY

   Copyright (C) 1996, 1997, 1999, 2000, 2001, 2002, 2005, 2006, 2007
John W. Eaton.

   Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.

   Permission is granted to copy and distribute modified versions of
this manual under the conditions for verbatim copying, provided that
the entire resulting derived work is distributed under the terms of a
permission notice identical to this one.

   Permission is granted to copy and distribute translations of this
manual into another language, under the above conditions for modified
versions.


File: octave.info,  Node: Data sources in object groups,  Next: Area series,  Up: Object Groups

15.2.8.1 Data sources in object groups
......................................

All of the group objects contain data source parameters.  There are
string parameters that contain an expression that is evaluated to update
the relevant data property of the group when the `refreshdata' function
is called.

 -- Function File:  refreshdata ()
 -- Function File:  refreshdata (H)
 -- Function File:  refreshdata (H, WS)
     Evaluates any datasource properties of the current figure and
     updates the corresponding data.  If call with one or more
     arguments H is a scalar or array of figure handles which to
     refresh.  The data sources are by default evaluated in the "base"
     workspace but can also be set in the "caller" workspace.

     An example of the use of refreshdata is

          x = 0:0.1:10;
          y = sin (x);
          plot (x, y, "ydatasource", "y");
          for i = 1 : 100
            pause(0.1)
            y = sin (x + 0.1 * i);
            refreshdata();
          endfor

     *See also:* *note linkdata: doc-linkdata.


File: octave.info,  Node: Area series,  Next: Bar series,  Prev: Data sources in object groups,  Up: Object Groups

15.2.8.2 Area series
....................

Area series objects are created by the `area' function.  Each of the
`hggroup' elements contains a single patch object.  The properties of
the area series are

`basevalue'
     The value where the base of the area plot is drawn.

`linewidth'
`linestyle'
     The line width and style of the edge of the patch objects making
     up the areas.  *Note Line Styles::.

`edgecolor'
`facecolor'
     The line and fill color of the patch objects making up the areas.
     *Note Colors::.

`xdata'
`ydata'
     The x and y coordinates of the original columns of the data passed
     to `area' prior to the cumulative summation used in the `area'
     function.

`xdatasource'
`ydatasource'
     Data source variables.


File: octave.info,  Node: Bar series,  Next: Contour groups,  Prev: Area series,  Up: Object Groups

15.2.8.3 Bar series
...................

Bar series objects are created by the `bar' or `barh' functions.  Each
`hggroup' element contains a single patch object.  The properties of
the bar series are

`showbaseline'
`baseline'
`basevalue'
     The property `showbaseline' flags whether the baseline of the bar
     series is displayed (default is "on").  The handle of the graphics
     object representing the baseline is given by the `baseline'
     property and the y-value of the baseline by the `basevalue'
     property.

     Changes to any of these property are propagated to the other
     members of the bar series and to the baseline itself.  Equally
     changes in the properties of the base line itself are propagated
     to the members of the corresponding bar series.

`barwidth'
`barlayout'
`horizontal'
     The property `barwidth' is the width of the bar corresponding to
     the WIDTH variable passed to `bar' or BARH.  Whether the bar
     series is "grouped" or "stacked" is determined by the `barlayout'
     property and whether the bars are horizontal or vertical by the
     `horizontal' property.

     Changes to any of these property are propagated to the other
     members of the bar series.

`linewidth'
`linestyle'
     The line width and style of the edge of the patch objects making
     up the bars.  *Note Line Styles::.

`edgecolor'
`facecolor'
     The line and fill color of the patch objects making up the bars.
     *Note Colors::.

`xdata'
     The nominal x positions of the bars.  Changes in this property and
     propagated to the other members of the bar series.

`ydata'
     The y value of the bars in the `hggroup'.

`xdatasource'
`ydatasource'
     Data source variables.


File: octave.info,  Node: Contour groups,  Next: Error bar series,  Prev: Bar series,  Up: Object Groups

15.2.8.4 Contour groups
.......................

Contour group objects are created by the `contour', `contourf' and
`contour3' functions.  The are equally one of the handles returned by
the `surfc' and `meshc' functions.  The properties of the contour group
are

`contourmatrix'
     A read only property that contains the data return by `contourc'
     used to create the contours of the plot.

`fill'
     A radio property that can have the values "on" or "off" that flags
     whether the contours to plot are to be filled.

`zlevelmode'
`zlevel'
     The radio property `zlevelmode' can have the values "none", "auto"
     or "manual".  When its value is "none" there is no z component to
     the plotted contours.  When its value is "auto" the z value of the
     plotted contours is at the same value as the contour itself.  If
     the value is "manual", then the z value at which to plot the
     contour is determined by the `zlevel' property.

`levellistmode'
`levellist'
`levelstepmode'
`levelstep'
     If `levellistmode' is "manual", then the levels at which to plot
     the contours is determined by `levellist'.  If `levellistmode' is
     set to "auto", then the distance between contours is determined by
     `levelstep'.  If both `levellistmode' and `levelstepmode' are set
     to "auto", then there are assumed to be 10 equal spaced contours.

`textlistmode'
`textlist'
`textstepmode'
`textstep'
     If `textlistmode' is "manual", then the labelled contours is
     determined by `textlist'.  If `textlistmode' is set to "auto",
     then the distance between labelled contours is determined by
     `textstep'.  If both `textlistmode' and `textstepmode' are set to
     "auto", then there are assumed to be 10 equal spaced labelled
     contours.

`showtext'
     Flag whether the contour labels are shown or not.

`labelspacing'
     The distance between labels on a single contour in points.

`linewidth'

`linestyle'

`linecolor'
     The properties of the contour lines.  The properties `linewidth'
     and `linestyle' are similar to the corresponding properties for
     lines.  The property `linecolor' is a color property (*note
     Colors::), that can also have the values of "none" or "auto".  If
     `linecolor' is "none", then no contour line is drawn.  If
     `linecolor' is "auto" then the line color is determined by the
     colormap.

`xdata'
`ydata'
`zdata'
     The original x, y, and z data of the contour lines.

`xdatasource'
`ydatasource'
`zdatasource'
     Data source variables.


File: octave.info,  Node: Error bar series,  Next: Line series,  Prev: Contour groups,  Up: Object Groups

15.2.8.5 Error bar series
.........................

Error bar series are created by the `errorbar' function.  Each
`hggroup' element contains two line objects representing the data and
the errorbars separately.  The properties of the error bar series are

`color'
     The RGB color or color name of the line objects of the error bars.
     *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the line objects of the error bars.
     *Note Line Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
`markersize'
     The line and fill color of the markers on the error bars.  *Note
     Colors::.

`xdata'
`ydata'
`ldata'
`udata'
`xldata'
`xudata'
     The original x, y, l, u, xl, xu data of the error bars.

`xdatasource'
`ydatasource'
`ldatasource'
`udatasource'
`xldatasource'
`xudatasource'
     Data source variables.


File: octave.info,  Node: Line series,  Next: Quiver group,  Prev: Error bar series,  Up: Object Groups

15.2.8.6 Line series
....................

Line series objects are created by the `plot'  and `plot3' functions
and are of the type `line'.  The properties of the line series with the
ability to add data sources.

`color'
     The RGB color or color name of the line objects.  *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the line objects.  *Note Line Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
`markersize'
     The line and fill color of the markers.  *Note Colors::.

`xdata'
`ydata'
`zdata'
     The original x, y and z data.

`xdatasource'
`ydatasource'
`zdatasource'
     Data source variables.


File: octave.info,  Node: Quiver group,  Next: Scatter group,  Prev: Line series,  Up: Object Groups

15.2.8.7 Quiver group
.....................

Quiver series objects are created by the `quiver' or `quiver3'
functions.  Each `hggroup' element of the series contains three line
objects as children representing the body and head of the arrow,
together with a marker as the point of original of the arrows.  The
properties of the quiver series are

`autoscale'
`autoscalefactor'
     Flag whether the length of the arrows is scaled or defined
     directly from the U, V and W data.  If the arrow length is flagged
     as being scaled by the `autoscale' property, then the length of the
     autoscaled arrow is controlled by the `autoscalefactor'.

`maxheadsize'
     This property controls the size of the head of the arrows in the
     quiver series.  The default value is 0.2.

`showarrowhead'
     Flag whether the arrow heads are displayed in the quiver plot.

`color'
     The RGB color or color name of the line objects of the quiver.
     *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the line objects of the quiver.  *Note
     Line Styles::.

`marker'
`markerfacecolor'
`markersize'
     The line and fill color of the marker objects at the original of
     the arrows.  *Note Colors::.

`xdata'
`ydata'
`zdata'
     The origins of the values of the vector field.

`udata'
`vdata'
`wdata'
     The values of the vector field to plot.

`xdatasource'
`ydatasource'
`zdatasource'
`udatasource'
`vdatasource'
`wdatasource'
     Data source variables.


File: octave.info,  Node: Scatter group,  Next: Stair group,  Prev: Quiver group,  Up: Object Groups

15.2.8.8 Scatter group
......................

Scatter series objects are created by the `scatter' or `scatter3'
functions.  A single hggroup element contains as many children as there
are points in the scatter plot, with each child representing one of the
points.  The properties of the stem series are

`linewidth'
     The line width of the line objects of the points.  *Note Line
     Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
     The line and fill color of the markers of the points.  *Note
     Colors::.

`xdata'
`ydata'
`zdata'
     The original x, y and z data of the stems.

`cdata'
     The color data for the points of the plot.  Each point can have a
     separate color, or a unique color can be specified.

`sizedata'
     The size data for the points of the plot.  Each point can its own
     size or a unique size can be specified.

`xdatasource'
`ydatasource'
`zdatasource'
`cdatasource'
`sizedatasource'
     Data source variables.


File: octave.info,  Node: Stair group,  Next: Stem Series,  Prev: Scatter group,  Up: Object Groups

15.2.8.9 Stair group
....................

Stair series objects are created by the `stair' function.  Each
`hggroup' element of the series contains a single line object as a
child representing the stair.  The properties of the stair series are

`color'
     The RGB color or color name of the line objects of the stairs.
     *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the line objects of the stairs.  *Note
     Line Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
`markersize'
     The line and fill color of the markers on the stairs.  *Note
     Colors::.

`xdata'
`ydata'
     The original x and y data of the stairs.

`xdatasource'
`ydatasource'
     Data source variables.


File: octave.info,  Node: Stem Series,  Next: Surface group,  Prev: Stair group,  Up: Object Groups

15.2.8.10 Stem Series
.....................

Stem series objects are created by the `stem' or `stem3' functions.
Each `hggroup' element contains a single line object as a child
representing the stems.  The properties of the stem series are

`showbaseline'
`baseline'
`basevalue'
     The property `showbaseline' flags whether the baseline of the stem
     series is displayed (default is "on").  The handle of the graphics
     object representing the baseline is given by the `baseline'
     property and the y-value (or z-value for `stem3') of the baseline
     by the `basevalue' property.

     Changes to any of these property are propagated to the other
     members of the stem series and to the baseline itself.  Equally
     changes in the properties of the base line itself are propagated
     to the members of the corresponding stem series.

`color'
     The RGB color or color name of the line objects of the stems.
     *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the line objects of the stems.  *Note
     Line Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
`markersize'
     The line and fill color of the markers on the stems.  *Note
     Colors::.

`xdata'
`ydata'
`zdata'
     The original x, y and z data of the stems.

`xdatasource'
`ydatasource'
`zdatasource'
     Data source variables.


File: octave.info,  Node: Surface group,  Prev: Stem Series,  Up: Object Groups

15.2.8.11 Surface group
.......................

Surface group objects are created by the `surf' or `mesh' functions,
but are equally one of the handles returned by the `surfc' or `meshc'
functions.  The surface group is of the type `surface'.

   The properties of the surface group are

`edgecolor'

`facecolor'
     The RGB color or color name of the edges or faces of the surface.
     *Note Colors::.

`linewidth'
`linestyle'
     The line width and style of the lines on the surface.  *Note Line
     Styles::.

`marker'
`markeredgecolor'
`markerfacecolor'
`markersize'
     The line and fill color of the markers on the surface.  *Note
     Colors::.

`xdata'
`ydata'
`zdata'

`cdata'
     The original x, y, z and c data.

`xdatasource'
`ydatasource'
`zdatasource'
`cdatasource'
     Data source variables.


File: octave.info,  Node: Graphics backends,  Prev: Object Groups,  Up: Advanced Plotting

15.2.9 Graphics backends
------------------------

 -- Function File:  backend (NAME)
 -- Function File:  backend (HLIST, NAME)
     Change the default graphics backend to NAME.  If the backend is
     not already loaded, it is first initialized (initialization is done
     through the execution of __init_NAME__).

     HLIST is a list of figure handles.  When given, this function only
     switches the default backend for the corresponding figures.

     *See also:* *note available_backends: doc-available_backends.

 -- Built-in Function:  available_backends ()
     Returns resgistered graphics backends.

* Menu:

* Interaction with gnuplot::


File: octave.info,  Node: Interaction with gnuplot,  Up: Graphics backends

15.2.9.1 Interaction with `gnuplot'
...................................

 -- Loadable Function: VAL = gnuplot_binary ()
 -- Loadable Function: OLD_VAL = gnuplot_binary (NEW_VAL)
     Query or set the name of the program invoked by the plot command.
     The default value `\"gnuplot\"'.  *Note Installation::.


File: octave.info,  Node: Matrix Manipulation,  Next: Arithmetic,  Prev: Plotting,  Up: Top

16 Matrix Manipulation
**********************

There are a number of functions available for checking to see if the
elements of a matrix meet some condition, and for rearranging the
elements of a matrix.  For example, Octave can easily tell you if all
the elements of a matrix are finite, or are less than some specified
value.  Octave can also rotate the elements, extract the upper- or
lower-triangular parts, or sort the columns of a matrix.

* Menu:

* Finding Elements and Checking Conditions::
* Rearranging Matrices::
* Applying a Function to an Array::
* Special Utility Matrices::
* Famous Matrices::


File: octave.info,  Node: Finding Elements and Checking Conditions,  Next: Rearranging Matrices,  Up: Matrix Manipulation

16.1 Finding Elements and Checking Conditions
=============================================

The functions `any' and `all' are useful for determining whether any or
all of the elements of a matrix satisfy some condition.  The `find'
function is also useful in determining which elements of a matrix meet
a specified condition.

 -- Built-in Function:  any (X, DIM)
     For a vector argument, return 1 if any element of the vector is
     nonzero.

     For a matrix argument, return a row vector of ones and zeros with
     each element indicating whether any of the elements of the
     corresponding column of the matrix are nonzero.  For example,

          any (eye (2, 4))
               => [ 1, 1, 0, 0 ]

     If the optional argument DIM is supplied, work along dimension
     DIM.  For example,

          any (eye (2, 4), 2)
               => [ 1; 1 ]

 -- Built-in Function:  all (X, DIM)
     The function `all' behaves like the function `any', except that it
     returns true only if all the elements of a vector, or all the
     elements along dimension DIM of a matrix, are nonzero.

   Since the comparison operators (*note Comparison Ops::) return
matrices of ones and zeros, it is easy to test a matrix for many
things, not just whether the elements are nonzero.  For example,

     all (all (rand (5) < 0.9))
          => 0

tests a random 5 by 5 matrix to see if all of its elements are less
than 0.9.

   Note that in conditional contexts (like the test clause of `if' and
`while' statements) Octave treats the test as if you had typed `all
(all (condition))'.

 -- Mapping Function:  xor (X, Y)
     Return the `exclusive or' of the entries of X and Y.  For boolean
     expressions X and Y, `xor (X, Y)' is true if and only if X or Y is
     true, but not if both X and Y are true.

 -- Function File:  is_duplicate_entry (X)
     Return non-zero if any entries in X are duplicates of one another.

 -- Function File:  diff (X, K, DIM)
     If X is a vector of length N, `diff (X)' is the vector of first
     differences X(2) - X(1), ..., X(n) - X(n-1).

     If X is a matrix, `diff (X)' is the matrix of column differences
     along the first non-singleton dimension.

     The second argument is optional.  If supplied, `diff (X, K)',
     where K is a non-negative integer, returns the K-th differences.
     It is possible that K is larger than then first non-singleton
     dimension of the matrix.  In this case, `diff' continues to take
     the differences along the next non-singleton dimension.

     The dimension along which to take the difference can be explicitly
     stated with the optional variable DIM.  In this case the K-th
     order differences are calculated along this dimension.  In the
     case where K exceeds `size (X, DIM)' then an empty matrix is
     returned.

 -- Mapping Function:  isinf (X)
     Return 1 for elements of X that are infinite and zero otherwise.
     For example,

          isinf ([13, Inf, NA, NaN])
               => [ 0, 1, 0, 0 ]

 -- Mapping Function:  isnan (X)
     Return 1 for elements of X that are NaN values and zero otherwise.
     NA values are also considered NaN values.  For example,

          isnan ([13, Inf, NA, NaN])
               => [ 0, 0, 1, 1 ]

 -- Mapping Function:  finite (X)
     Return 1 for elements of X that are finite values and zero
     otherwise.  For example,

          finite ([13, Inf, NA, NaN])
               => [ 1, 0, 0, 0 ]

 -- Loadable Function:  find (X)
 -- Loadable Function:  find (X, N)
 -- Loadable Function:  find (X, N, DIRECTION)
     Return a vector of indices of nonzero elements of a matrix, as a
     row if X is a row or as a column otherwise.  To obtain a single
     index for each matrix element, Octave pretends that the columns of
     a matrix form one long vector (like Fortran arrays are stored).
     For example,

          find (eye (2))
               => [ 1; 4 ]

     If two outputs are requested, `find' returns the row and column
     indices of nonzero elements of a matrix.  For example,

          [i, j] = find (2 * eye (2))
               => i = [ 1; 2 ]
               => j = [ 1; 2 ]

     If three outputs are requested, `find' also returns a vector
     containing the nonzero values.  For example,

          [i, j, v] = find (3 * eye (2))
               => i = [ 1; 2 ]
               => j = [ 1; 2 ]
               => v = [ 3; 3 ]

     If two inputs are given, N indicates the number of elements to
     find from the beginning of the matrix or vector.

     If three inputs are given, DIRECTION should be one of "first" or
     "last" indicating that it should start counting found elements
     from the first or last element.

     Note that this function is particularly useful for sparse
     matrices, as it extracts the non-zero elements as vectors, which
     can then be used to create the original matrix.  For example,

          sz = size(a);
          [i, j, v] = find (a);
          b = sparse(i, j, v, sz(1), sz(2));

     *See also:* *note sparse: doc-sparse.

 -- Function File: [ERR, Y1, ...] = common_size (X1, ...)
     Determine if all input arguments are either scalar or of common
     size.  If so, ERR is zero, and YI is a matrix of the common size
     with all entries equal to XI if this is a scalar or XI otherwise.
     If the inputs cannot be brought to a common size, errorcode is 1,
     and YI is XI.  For example,

          [errorcode, a, b] = common_size ([1 2; 3 4], 5)
               => errorcode = 0
               => a = [ 1, 2; 3, 4 ]
               => b = [ 5, 5; 5, 5 ]

     This is useful for implementing functions where arguments can
     either be scalars or of common size.


File: octave.info,  Node: Rearranging Matrices,  Next: Applying a Function to an Array,  Prev: Finding Elements and Checking Conditions,  Up: Matrix Manipulation

16.2 Rearranging Matrices
=========================

 -- Function File:  fliplr (X)
     Return a copy of X with the order of the columns reversed.  For
     example,

          fliplr ([1, 2; 3, 4])
               =>  2  1
                   4  3

     Note that `fliplr' only work with 2-D arrays.  To flip N-d arrays
     use `flipdim' instead.

     *See also:* *note flipud: doc-flipud, *note flipdim: doc-flipdim,
     *note rot90: doc-rot90, *note rotdim: doc-rotdim.

 -- Function File:  flipud (X)
     Return a copy of X with the order of the rows reversed.  For
     example,

          flipud ([1, 2; 3, 4])
               =>  3  4
                   1  2

     Due to the difficulty of defining which axis about which to flip
     the matrix `flipud' only work with 2-d arrays.  To flip N-d arrays
     use `flipdim' instead.

     *See also:* *note fliplr: doc-fliplr, *note flipdim: doc-flipdim,
     *note rot90: doc-rot90, *note rotdim: doc-rotdim.

 -- Function File:  flipdim (X, DIM)
     Return a copy of X flipped about the dimension DIM.  For example

          flipdim ([1, 2; 3, 4], 2)
               =>  2  1
                   4  3

     *See also:* *note fliplr: doc-fliplr, *note flipud: doc-flipud,
     *note rot90: doc-rot90, *note rotdim: doc-rotdim.

 -- Function File:  rot90 (X, N)
     Return a copy of X with the elements rotated counterclockwise in
     90-degree increments.  The second argument is optional, and
     specifies how many 90-degree rotations are to be applied (the
     default value is 1).  Negative values of N rotate the matrix in a
     clockwise direction.  For example,

          rot90 ([1, 2; 3, 4], -1)
               =>  3  1
                   4  2

     rotates the given matrix clockwise by 90 degrees.  The following
     are all equivalent statements:

          rot90 ([1, 2; 3, 4], -1)
          rot90 ([1, 2; 3, 4], 3)
          rot90 ([1, 2; 3, 4], 7)

     Due to the difficulty of defining an axis about which to rotate the
     matrix `rot90' only work with 2-D arrays.  To rotate N-d arrays
     use `rotdim' instead.

     *See also:* *note rotdim: doc-rotdim, *note flipud: doc-flipud,
     *note fliplr: doc-fliplr, *note flipdim: doc-flipdim.

 -- Function File:  rotdim (X, N, PLANE)
     Return a copy of X with the elements rotated counterclockwise in
     90-degree increments.  The second argument is optional, and
     specifies how many 90-degree rotations are to be applied (the
     default value is 1).  The third argument is also optional and
     defines the plane of the rotation.  As such PLANE is a two element
     vector containing two different valid dimensions of the matrix.
     If PLANE is not given Then the first two non-singleton dimensions
     are used.

     Negative values of N rotate the matrix in a clockwise direction.
     For example,

          rotdim ([1, 2; 3, 4], -1, [1, 2])
               =>  3  1
                   4  2

     rotates the given matrix clockwise by 90 degrees.  The following
     are all equivalent statements:

          rotdim ([1, 2; 3, 4], -1, [1, 2])
          rotdim ([1, 2; 3, 4], 3, [1, 2])
          rotdim ([1, 2; 3, 4], 7, [1, 2])

     *See also:* *note rot90: doc-rot90, *note flipud: doc-flipud,
     *note fliplr: doc-fliplr, *note flipdim: doc-flipdim.

 -- Built-in Function:  cat (DIM, ARRAY1, ARRAY2, ..., ARRAYN)
     Return the concatenation of N-d array objects, ARRAY1, ARRAY2,
     ..., ARRAYN along dimension DIM.

          A = ones (2, 2);
          B = zeros (2, 2);
          cat (2, A, B)
          => ans =

               1 1 0 0
               1 1 0 0

     Alternatively, we can concatenate A and B along the second
     dimension the following way:

          [A, B].

     DIM can be larger than the dimensions of the N-d array objects and
     the result will thus have DIM dimensions as the following example
     shows:
          cat (4, ones(2, 2), zeros (2, 2))
          => ans =

             ans(:,:,1,1) =

               1 1
               1 1

             ans(:,:,1,2) =
               0 0
               0 0

     *See also:* *note horzcat: doc-horzcat, *note vertcat: doc-vertcat.

 -- Built-in Function:  horzcat (ARRAY1, ARRAY2, ..., ARRAYN)
     Return the horizontal concatenation of N-d array objects, ARRAY1,
     ARRAY2, ..., ARRAYN along dimension 2.

     *See also:* *note cat: doc-cat, *note vertcat: doc-vertcat.

 -- Built-in Function:  vertcat (ARRAY1, ARRAY2, ..., ARRAYN)
     Return the vertical concatenation of N-d array objects, ARRAY1,
     ARRAY2, ..., ARRAYN along dimension 1.

     *See also:* *note cat: doc-cat, *note horzcat: doc-horzcat.

 -- Built-in Function:  permute (A, PERM)
     Return the generalized transpose for an N-d array object A.  The
     permutation vector PERM must contain the elements `1:ndims(a)' (in
     any order, but each element must appear just once).

     *See also:* *note ipermute: doc-ipermute.

 -- Built-in Function:  ipermute (A, IPERM)
     The inverse of the `permute' function.  The expression

          ipermute (permute (a, perm), perm)
     returns the original array A.

     *See also:* *note permute: doc-permute.

 -- Built-in Function:  reshape (A, M, N, ...)
 -- Built-in Function:  reshape (A, SIZE)
     Return a matrix with the given dimensions whose elements are taken
     from the matrix A.  The elements of the matrix are accessed in
     column-major order (like Fortran arrays are stored).

     For example,

          reshape ([1, 2, 3, 4], 2, 2)
               =>  1  3
                   2  4

     Note that the total number of elements in the original matrix must
     match the total number of elements in the new matrix.

     A single dimension of the return matrix can be unknown and is
     flagged by an empty argument.

 -- Built-in Function:  resize (X, M)
 -- Built-in Function:  resize (X, M, N)
 -- Built-in Function:  resize (X, M, N, ...)
     Resize X cutting off elements as necessary.

     In the result, element with certain indices is equal to the
     corresponding element of X if the indices are within the bounds of
     X; otherwise, the element is set to zero.

     In other words, the statement

            y = resize (x, dv);

     is equivalent to the following code:

            y = zeros (dv, class (x));
            sz = min (dv, size (x));
            for i = 1:length (sz), idx{i} = 1:sz(i); endfor
            y(idx{:}) = x(idx{:});

     but is performed more efficiently.

     If only M is supplied and it is a scalar, the dimension of the
     result is M-by-M.  If M is a vector, then the dimensions of the
     result are given by the elements of M.  If both M and N are
     scalars, then the dimensions of the result are M-by-N.

     An object can be resized to more dimensions than it has; in such
     case the missing dimensions are assumed to be 1.  Resizing an
     object to fewer dimensions is not possible.

     *See also:* *note reshape: doc-reshape, *note postpad: doc-postpad.

 -- Function File: Y = circshift (X, N)
     Circularly shifts the values of the array X.  N must be a vector
     of integers no longer than the number of dimensions in X.  The
     values of N can be either positive or negative, which determines
     the direction in which the values or X are shifted.  If an element
     of N is zero, then the corresponding dimension of X will not be
     shifted.  For example

          x = [1, 2, 3; 4, 5, 6; 7, 8, 9];
          circshift (x, 1)
          =>  7, 8, 9
              1, 2, 3
              4, 5, 6
          circshift (x, -2)
          =>  7, 8, 9
              1, 2, 3
              4, 5, 6
          circshift (x, [0,1])
          =>  3, 1, 2
              6, 4, 5
              9, 7, 8

     *See also:* permute, ipermute, shiftdim.

 -- Function File: Y = shiftdim (X, N)
 -- Function File: [Y, NS] = shiftdim (X)
     Shifts the dimension of X by N, where N must be an integer scalar.
     When N is positive, the dimensions of X are shifted to the left,
     with the leading dimensions circulated to the end.  If N is
     negative, then the dimensions of X are shifted to the right, with
     N leading singleton dimensions added.

     Called with a single argument, `shiftdim', removes the leading
     singleton dimensions, returning the number of dimensions removed
     in the second output argument NS.

     For example

          x = ones (1, 2, 3);
          size (shiftdim (x, -1))
               => [1, 1, 2, 3]
          size (shiftdim (x, 1))
               => [2, 3]
          [b, ns] = shiftdim (x);
               => b =  [1, 1, 1; 1, 1, 1]
               => ns = 1

     *See also:* reshape, permute, ipermute, circshift, squeeze.

 -- Function File:  shift (X, B)
 -- Function File:  shift (X, B, DIM)
     If X is a vector, perform a circular shift of length B of the
     elements of X.

     If X is a matrix, do the same for each column of X.  If the
     optional DIM argument is given, operate along this dimension

 -- Loadable Function: [S, I] = sort (X)
 -- Loadable Function: [S, I] = sort (X, DIM)
 -- Loadable Function: [S, I] = sort (X, MODE)
 -- Loadable Function: [S, I] = sort (X, DIM, MODE)
     Return a copy of X with the elements arranged in increasing order.
     For matrices, `sort' orders the elements in each column.

     For example,

          sort ([1, 2; 2, 3; 3, 1])
               =>  1  1
                   2  2
                   3  3

     The `sort' function may also be used to produce a matrix
     containing the original row indices of the elements in the sorted
     matrix.  For example,

          [s, i] = sort ([1, 2; 2, 3; 3, 1])
               => s = 1  1
                      2  2
                      3  3
               => i = 1  3
                      2  1
                      3  2

     If the optional argument DIM is given, then the matrix is sorted
     along the dimension defined by DIM.  The optional argument `mode'
     defines the order in which the values will be sorted.  Valid
     values of `mode' are `ascend' or `descend'.

     For equal elements, the indices are such that the equal elements
     are listed in the order that appeared in the original list.

     The `sort' function may also be used to sort strings and cell
     arrays of strings, in which case the dictionary order of the
     strings is used.

     The algorithm used in `sort' is optimized for the sorting of
     partially ordered lists.

 -- Function File:  sortrows (A, C)
     Sort the rows of the matrix A according to the order of the
     columns specified in C.  If C is omitted, a lexicographical sort
     is used.  By default ascending order is used however if elements
     of C are negative then the corresponding column is sorted in
     descending order.

 -- Function File:  issorted (A, ROWS)
     Returns true if the array is sorted, ascending or descending.
     NaNs are treated as by `sort'.  If ROWS is supplied and has the
     value "rows", checks whether the array is sorted by rows as if
     output by `sortrows' (with no options).

     This function does not yet support sparse matrices.

     *See also:* *note sortrows: doc-sortrows, *note sort: doc-sort.

   Since the `sort' function does not allow sort keys to be specified,
it can't be used to order the rows of a matrix according to the values
of the elements in various columns(1) in a single call.  Using the
second output, however, it is possible to sort all rows based on the
values in a given column.  Here's an example that sorts the rows of a
matrix based on the values in the second column.

     a = [1, 2; 2, 3; 3, 1];
     [s, i] = sort (a (:, 2));
     a (i, :)
          =>  3  1
              1  2
              2  3

 -- Function File:  tril (A, K)
 -- Function File:  triu (A, K)
     Return a new matrix formed by extracting the lower (`tril') or
     upper (`triu') triangular part of the matrix A, and setting all
     other elements to zero.  The second argument is optional, and
     specifies how many diagonals above or below the main diagonal
     should also be set to zero.

     The default value of K is zero, so that `triu' and `tril' normally
     include the main diagonal as part of the result matrix.

     If the value of K is negative, additional elements above (for
     `tril') or below (for `triu') the main diagonal are also selected.

     The absolute value of K must not be greater than the number of
     sub- or super-diagonals.

     For example,

          tril (ones (3), -1)
               =>  0  0  0
                   1  0  0
                   1  1  0

     and

          tril (ones (3), 1)
               =>  1  1  0
                   1  1  1
                   1  1  1

     *See also:* *note triu: doc-triu, *note diag: doc-diag.

 -- Function File:  vec (X)
     Return the vector obtained by stacking the columns of the matrix X
     one above the other.

 -- Function File:  vech (X)
     Return the vector obtained by eliminating all supradiagonal
     elements of the square matrix X and stacking the result one column
     above the other.

 -- Function File:  prepad (X, L, C)
 -- Function File:  prepad (X, L, C, DIM)
     Prepend (append) the scalar value C to the vector X until it is of
     length L.  If the third argument is not supplied, a value of 0 is
     used.

     If `length (X) > L', elements from the beginning (end) of X are
     removed until a vector of length L is obtained.

     If X is a matrix, elements are prepended or removed from each row.

     If the optional DIM argument is given, then operate along this
     dimension.

     *See also:* *note postpad: doc-postpad.

 -- Built-in Function:  diag (V, K)
     Return a diagonal matrix with vector V on diagonal K.  The second
     argument is optional.  If it is positive, the vector is placed on
     the K-th super-diagonal.  If it is negative, it is placed on the
     -K-th sub-diagonal.  The default value of K is 0, and the vector
     is placed on the main diagonal.  For example,

          diag ([1, 2, 3], 1)
               =>  0  1  0  0
                   0  0  2  0
                   0  0  0  3
                   0  0  0  0

     Given a matrix argument, instead of a vector, `diag' extracts the
     K-th diagonal of the matrix.

 -- Function File:  blkdiag (A, B, C, ...)
     Build a block diagonal matrix from A, B, C, ....  All the
     arguments must be numeric and are two-dimensional matrices or
     scalars.

     *See also:* *note diag: doc-diag, *note horzcat: doc-horzcat,
     *note vertcat: doc-vertcat.

   ---------- Footnotes ----------

   (1) For example, to first sort based on the values in column 1, and
then, for any values that are repeated in column 1, sort based on the
values found in column 2, etc.


File: octave.info,  Node: Applying a Function to an Array,  Next: Special Utility Matrices,  Prev: Rearranging Matrices,  Up: Matrix Manipulation

16.3 Applying a Function to an Array
====================================

 -- Function File:  arrayfun (FUNC, A)
 -- Function File: X = arrayfun (FUNC, A)
 -- Function File: X = arrayfun (FUNC, A, B, ...)
 -- Function File: [X, Y, ...] = arrayfun (FUNC, A, ...)
 -- Function File:  arrayfun (..., "UniformOutput", VAL)
 -- Function File:  arrayfun (..., "ErrorHandler", ERRFUNC)
     Execute a function on each element of an array.  This is useful for
     functions that do not accept array arguments.  If the function does
     accept array arguments it is better to call the function directly.

     The first input argument FUNC can be a string, a function handle,
     an inline function or an anonymous function.  The input argument A
     can be a logic array, a numeric array, a string array, a structure
     array or a cell array.  By a call of the function `arrayfun' all
     elements of A are passed on to the named function FUNC
     individually.

     The named function can also take more than two input arguments,
     with the input arguments given as third input argument B, fourth
     input argument C, ...  If given more than one array input argument
     then all input arguments must have the same sizes, for example

          arrayfun (@atan2, [1, 0], [0, 1])
          => ans = [1.5708   0.0000]

     If the parameter VAL after a further string input argument
     "UniformOutput" is set `true' (the default), then the named
     function FUNC must return a single element which then will be
     concatenated into the return value and is of type matrix.
     Otherwise, if that parameter is set to `false', then the outputs
     are concatenated in a cell array.  For example

          arrayfun (@(x,y) x:y, "abc", "def", "UniformOutput", false)
          => ans =
          {
            [1,1] = abcd
            [1,2] = bcde
            [1,3] = cdef
          }

     If more than one output arguments are given then the named function
     must return the number of return values that also are expected, for
     example

          [A, B, C] = arrayfun (@find, [10; 0], "UniformOutput", false)
          =>
          A =
          {
            [1,1] =  1
            [2,1] = [](0x0)
          }
          B =
          {
            [1,1] =  1
            [2,1] = [](0x0)
          }
          C =
          {
            [1,1] =  10
            [2,1] = [](0x0)
          }

     If the parameter ERRFUNC after a further string input argument
     "ErrorHandler" is another string, a function handle, an inline
     function or an anonymous function, then ERRFUNC defines a function
     to call in the case that FUNC generates an error.  The definition
     of the function must be of the form

          function [...] = errfunc (S, ...)

     where there is an additional input argument to ERRFUNC relative to
     FUNC, given by S.  This is a structure with the elements
     "identifier", "message" and "index", giving respectively the error
     identifier, the error message and the index of the array elements
     that caused the error.  The size of the output argument of ERRFUNC
     must have the same size as the output argument of FUNC, otherwise
     a real error is thrown.  For example

          function y = ferr (s, x), y = "MyString"; endfunction
          arrayfun (@str2num, [1234], \
                    "UniformOutput", false, "ErrorHandler", @ferr)
          => ans =
          {
           [1,1] = MyString
          }

     *See also:* *note cellfun: doc-cellfun, *note spfun: doc-spfun,
     *note structfun: doc-structfun.

 -- Loadable Function:  bsxfun (F, A, B)
     Applies a binary function F element-wise to two matrix arguments A
     and B.  The function F must be capable of accepting two column
     vector arguments of equal length, or one column vector argument
     and a scalar.

     The dimensions of A and B must be equal or singleton.  The
     singleton dimensions of the matrices will be expanded to the same
     dimensionality as the other matrix.

     *See also:* *note arrayfun: doc-arrayfun, *note cellfun:
     doc-cellfun.


File: octave.info,  Node: Special Utility Matrices,  Next: Famous Matrices,  Prev: Applying a Function to an Array,  Up: Matrix Manipulation

16.4 Special Utility Matrices
=============================

 -- Built-in Function:  eye (X)
 -- Built-in Function:  eye (N, M)
 -- Built-in Function:  eye (..., CLASS)
     Return an identity matrix.  If invoked with a single scalar
     argument, `eye' returns a square matrix with the dimension
     specified.  If you supply two scalar arguments, `eye' takes them
     to be the number of rows and columns.  If given a vector with two
     elements, `eye' uses the values of the elements as the number of
     rows and columns, respectively.  For example,

          eye (3)
               =>  1  0  0
                   0  1  0
                   0  0  1

     The following expressions all produce the same result:

          eye (2)
          ==
          eye (2, 2)
          ==
          eye (size ([1, 2; 3, 4])

     The optional argument CLASS, allows `eye' to return an array of
     the specified type, like

          val = zeros (n,m, "uint8")

     Calling `eye' with no arguments is equivalent to calling it with
     an argument of 1.  This odd definition is for compatibility with
     MATLAB.

 -- Built-in Function:  ones (X)
 -- Built-in Function:  ones (N, M)
 -- Built-in Function:  ones (N, M, K, ...)
 -- Built-in Function:  ones (..., CLASS)
     Return a matrix or N-dimensional array whose elements are all 1.
     The arguments are handled the same as the arguments for `eye'.

     If you need to create a matrix whose values are all the same, you
     should use an expression like

          val_matrix = val * ones (n, m)

     The optional argument CLASS, allows `ones' to return an array of
     the specified type, for example

          val = ones (n,m, "uint8")

 -- Built-in Function:  zeros (X)
 -- Built-in Function:  zeros (N, M)
 -- Built-in Function:  zeros (N, M, K, ...)
 -- Built-in Function:  zeros (..., CLASS)
     Return a matrix or N-dimensional array whose elements are all 0.
     The arguments are handled the same as the arguments for `eye'.

     The optional argument CLASS, allows `zeros' to return an array of
     the specified type, for example

          val = zeros (n,m, "uint8")

 -- Function File:  repmat (A, M, N)
 -- Function File:  repmat (A, [M N])
 -- Function File:  repmat (A, [M N P ...])
     Form a block matrix of size M by N, with a copy of matrix A as
     each element.  If N is not specified, form an M by M block matrix.

 -- Loadable Function:  rand (X)
 -- Loadable Function:  rand (N, M)
 -- Loadable Function:  rand ("state", X)
 -- Loadable Function:  rand ("seed", X)
     Return a matrix with random elements uniformly distributed on the
     interval (0, 1).  The arguments are handled the same as the
     arguments for `eye'.

     You can query the state of the random number generator using the
     form

          v = rand ("state")

     This returns a column vector V of length 625.  Later, you can
     restore the random number generator to the state V using the form

          rand ("state", v)

     You may also initialize the state vector from an arbitrary vector
     of length <= 625 for V.  This new state will be a hash based on the
     value of V, not V itself.

     By default, the generator is initialized from `/dev/urandom' if it
     is available, otherwise from cpu time, wall clock time and the
     current fraction of a second.

     To compute the pseudo-random sequence, `rand' uses the Mersenne
     Twister with a period of 2^19937-1 (See M. Matsumoto and T.
     Nishimura, `Mersenne Twister: A 623-dimensionally equidistributed
     uniform pseudorandom number generator', ACM Trans. on Modeling and
     Computer Simulation Vol. 8, No. 1, January pp.3-30 1998,
     `http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html').  Do
     *not* use for cryptography without securely hashing several
     returned values together, otherwise the generator state can be
     learned after reading 624 consecutive values.

     Older versions of Octave used a different random number generator.
     The new generator is used by default as it is significantly faster
     than the old generator, and produces random numbers with a
     significantly longer cycle time.  However, in some circumstances
     it might be desirable to obtain the same random sequences as used
     by the old generators.  To do this the keyword "seed" is used to
     specify that the old generators should be use, as in

          rand ("seed", val)

     which sets the seed of the generator to VAL.  The seed of the
     generator can be queried with

          s = rand ("seed")

     However, it should be noted that querying the seed will not cause
     `rand' to use the old generators, only setting the seed will.  To
     cause `rand' to once again use the new generators, the keyword
     "state" should be used to reset the state of the `rand'.

     *See also:* *note randn: doc-randn, *note rande: doc-rande, *note
     randg: doc-randg, *note randp: doc-randp.

 -- Loadable Function:  randn (X)
 -- Loadable Function:  randn (N, M)
 -- Loadable Function:  randn ("state", X)
 -- Loadable Function:  randn ("seed", X)
     Return a matrix with normally distributed pseudo-random elements
     having zero mean and variance one.  The arguments are handled the
     same as the arguments for `rand'.

     By default, `randn' uses the Marsaglia and Tsang "Ziggurat
     technique" to transform from a uniform to a normal distribution.
     (G. Marsaglia and W.W. Tsang, `Ziggurat method for generating
     random variables', J. Statistical Software, vol 5, 2000,
     `http://www.jstatsoft.org/v05/i08/')

     *See also:* *note rand: doc-rand, *note rande: doc-rande, *note
     randg: doc-randg, *note randp: doc-randp.

 -- Loadable Function:  rande (X)
 -- Loadable Function:  rande (N, M)
 -- Loadable Function:  rande ("state", X)
 -- Loadable Function:  rande ("seed", X)
     Return a matrix with exponentially distributed random elements.
     The arguments are handled the same as the arguments for `rand'.

     By default, `randn' uses the Marsaglia and Tsang "Ziggurat
     technique" to transform from a uniform to a exponential
     distribution.  (G. Marsaglia and W.W. Tsang, `Ziggurat method for
     generating random variables', J. Statistical Software, vol 5, 2000,
     `http://www.jstatsoft.org/v05/i08/')

     *See also:* *note rand: doc-rand, *note randn: doc-randn, *note
     randg: doc-randg, *note randp: doc-randp.

 -- Loadable Function:  randp (L, X)
 -- Loadable Function:  randp (L, N, M)
 -- Loadable Function:  randp ("state", X)
 -- Loadable Function:  randp ("seed", X)
     Return a matrix with Poisson distributed random elements with mean
     value parameter given by the first argument, L.  The arguments are
     handled the same as the arguments for `rand', except for the
     argument L.

     Five different algorithms are used depending on the range of L and
     whether or not L is a scalar or a matrix.

    For scalar L <= 12, use direct method.
          Press, et al., 'Numerical Recipes in C', Cambridge University
          Press, 1992.

    For scalar L > 12, use rejection method.[1]
          Press, et al., 'Numerical Recipes in C', Cambridge University
          Press, 1992.

    For matrix L <= 10, use inversion method.[2]
          Stadlober E., et al., WinRand source code, available via FTP.

    For matrix L > 10, use patchwork rejection method.
          Stadlober E., et al., WinRand source code, available via FTP,
          or H. Zechner, 'Efficient sampling from continuous and
          discrete unimodal distributions', Doctoral Dissertation,
          156pp., Technical University Graz, Austria, 1994.

    For L > 1e8, use normal approximation.
          L. Montanet, et al., 'Review of Particle Properties',
          Physical Review D 50 p1284, 1994

     *See also:* *note rand: doc-rand, *note randn: doc-randn, *note
     rande: doc-rande, *note randg: doc-randg.

 -- Loadable Function:  randg (A, X)
 -- Loadable Function:  randg (A, N, M)
 -- Loadable Function:  randg ("state", X)
 -- Loadable Function:  randg ("seed", X)
     Return a matrix with `gamma(A,1)' distributed random elements.
     The arguments are handled the same as the arguments for `rand',
     except for the argument A.

     This can be used to generate many distributions:

    `gamma (a, b)' for `a > -1', `b > 0'
               r = b * randg (a)

    `beta (a, b)' for `a > -1', `b > -1'
               r1 = randg (a, 1)
               r = r1 / (r1 + randg (b, 1))

    `Erlang (a, n)'
               r = a * randg (n)

    `chisq (df)' for `df > 0'
               r = 2 * randg (df / 2)

    `t(df)' for `0 < df < inf' (use randn if df is infinite)
               r = randn () / sqrt (2 * randg (df / 2) / df)

    `F (n1, n2)' for `0 < n1', `0 < n2'
               ## r1 equals 1 if n1 is infinite
               r1 = 2 * randg (n1 / 2) / n1
               ## r2 equals 1 if n2 is infinite
               r2 = 2 * randg (n2 / 2) / n2
               r = r1 / r2

    negative `binomial (n, p)' for `n > 0', `0 < p <= 1'
               r = randp ((1 - p) / p * randg (n))

    non-central `chisq (df, L)', for `df >= 0' and `L > 0'
          (use chisq if `L = 0')
               r = randp (L / 2)
               r(r > 0) = 2 * randg (r(r > 0))
               r(df > 0) += 2 * randg (df(df > 0)/2)

    `Dirichlet (a1, ... ak)'
               r = (randg (a1), ..., randg (ak))
               r = r / sum (r)

     *See also:* *note rand: doc-rand, *note randn: doc-randn, *note
     rande: doc-rande, *note randp: doc-randp.

   The generators operate in the new or old style together, it is not
possible to mix the two.  Initializing any generator with `"state"' or
`"seed"' causes the others to switch to the same style for future calls.

   The state of each generator is independent and calls to different
generators can be interleaved without affecting the final result.  For
example,

     rand ("state", [11, 22, 33]);
     randn ("state", [44, 55, 66]);
     u = rand (100, 1);
     n = randn (100, 1);

and

     rand ("state", [11, 22, 33]);
     randn ("state", [44, 55, 66]);
     u = zeros (100, 1);
     n = zeros (100, 1);
     for i = 1:100
       u(i) = rand ();
       n(i) = randn ();
     end

produce equivalent results.  When the generators are initialized in the
old style with `"seed"' only `rand' and `randn' are independent,
because the old `rande', `randg' and `randp' generators make calls to
`rand' and `randn'.

   The generators are initialized with random states at start-up, so
that the sequences of random numbers are not the same each time you run
Octave.(1) If you really do need to reproduce a sequence of numbers
exactly, you can set the state or seed to a specific value.

   If invoked without arguments, `rand' and `randn' return a single
element of a random sequence.

   The original `rand' and `randn' functions use Fortran code from
RANLIB, a library of fortran routines for random number generation,
compiled by Barry W. Brown and James Lovato of the Department of
Biomathematics at The University of Texas, M.D. Anderson Cancer Center,
Houston, TX 77030.

 -- Function File:  randperm (N)
     Return a row vector containing a random permutation of the
     integers from 1 to N.

   The functions `linspace' and `logspace' make it very easy to create
vectors with evenly or logarithmically spaced elements.  *Note Ranges::.

 -- Built-in Function:  linspace (BASE, LIMIT, N)
     Return a row vector with N linearly spaced elements between BASE
     and LIMIT.  If the number of elements is greater than one, then
     the BASE and LIMIT are always included in the range.  If BASE is
     greater than LIMIT, the elements are stored in decreasing order.
     If the number of points is not specified, a value of 100 is used.

     The `linspace' function always returns a row vector.

     For compatibility with MATLAB, return the second argument if fewer
     than two values are requested.

 -- Function File:  logspace (BASE, LIMIT, N)
     Similar to `linspace' except that the values are logarithmically
     spaced from 10^base to 10^limit.

     If LIMIT is equal to pi, the points are between 10^base and pi,
     _not_ 10^base and 10^pi, in order to be compatible with the
     corresponding MATLAB function.

     Also for compatibility, return the second argument if fewer than
     two values are requested.

     *See also:* *note linspace: doc-linspace.

   ---------- Footnotes ----------

   (1) The old versions of `rand' and `randn' obtain their initial
seeds from the system clock.


File: octave.info,  Node: Famous Matrices,  Prev: Special Utility Matrices,  Up: Matrix Manipulation

16.5 Famous Matrices
====================

The following functions return famous matrix forms.

 -- Function File:  hadamard (N)
     Construct a Hadamard matrix HN of size N-by-N.  The size N must be
     of the form `2 ^ K * P' in which P is one of 1, 12, 20 or 28.  The
     returned matrix is normalized, meaning `Hn(:,1) == 1' and `H(1,:)
     == 1'.

     Some of the properties of Hadamard matrices are:

        * `kron (HM, HN)' is a Hadamard matrix of size M-by-N.

        * `Hn * Hn' == N * eye (N)'.

        * The rows of HN are orthogonal.

        * `det (A) <= abs(det (HN))' for all A with `abs (A (I, J)) <=
          1'.

        * Multiply any row or column by -1 and still have a Hadamard
          matrix.


 -- Function File:  hankel (C, R)
     Return the Hankel matrix constructed given the first column C, and
     (optionally) the last row R.  If the last element of C is not the
     same as the first element of R, the last element of C is used.  If
     the second argument is omitted, it is assumed to be a vector of
     zeros with the same size as C.

     A Hankel matrix formed from an m-vector C, and an n-vector R, has
     the elements

          H(i,j) = c(i+j-1),  i+j-1 <= m;
          H(i,j) = r(i+j-m),  otherwise

     *See also:* *note vander: doc-vander, *note sylvester_matrix:
     doc-sylvester_matrix, *note hilb: doc-hilb, *note invhilb:
     doc-invhilb, *note toeplitz: doc-toeplitz.

 -- Function File:  hilb (N)
     Return the Hilbert matrix of order N.  The i, j element of a
     Hilbert matrix is defined as

          H (i, j) = 1 / (i + j - 1)

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note invhilb:
     doc-invhilb, *note toeplitz: doc-toeplitz.

 -- Function File:  invhilb (N)
     Return the inverse of a Hilbert matrix of order N.  This can be
     computed exactly using

                      (i+j)         /n+i-1\  /n+j-1\   /i+j-2\ 2
           A(i,j) = -1      (i+j-1)(       )(       ) (       )
                                    \ n-j /  \ n-i /   \ i-2 /

                  = p(i) p(j) / (i+j-1)
     where
                       k  /k+n-1\   /n\
              p(k) = -1  (       ) (   )
                          \ k-1 /   \k/

     The validity of this formula can easily be checked by expanding
     the binomial coefficients in both formulas as factorials.  It can
     be derived more directly via the theory of Cauchy matrices: see J.
     W. Demmel, Applied Numerical Linear Algebra, page 92.

     Compare this with the numerical calculation of `inverse (hilb
     (n))', which suffers from the ill-conditioning of the Hilbert
     matrix, and the finite precision of your computer's floating point
     arithmetic.

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note hilb:
     doc-hilb, *note toeplitz: doc-toeplitz.

 -- Function File:  magic (N)
     Create an N-by-N magic square.  Note that `magic (2)' is undefined
     since there is no 2-by-2 magic square.


 -- Function File:  pascal (N, T)
     Return the Pascal matrix of order N if `T = 0'.  T defaults to 0.
     Return lower triangular Cholesky factor of the Pascal matrix if `T
     = 1'.  This matrix is its own inverse, that is `pascal (N, 1) ^ 2
     == eye (N)'.  If `T = -1', return its absolute value.  This is the
     standard pascal triangle as a lower-triangular matrix.  If `T =
     2', return a transposed and permuted version of `pascal (N, 1)',
     which is the cube-root of the identity matrix.  That is `pascal
     (N, 2) ^ 3 == eye (N)'.

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note hilb:
     doc-hilb, *note invhilb: doc-invhilb, *note toeplitz:
     doc-toeplitz, *note hadamard: doc-hadamard, *note wilkinson:
     doc-wilkinson, *note compan: doc-compan, *note rosser: doc-rosser.

 -- Function File:  rosser ()
     Returns the Rosser matrix.  This is a difficult test case used to
     test eigenvalue algorithms.

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note hilb:
     doc-hilb, *note invhilb: doc-invhilb, *note toeplitz:
     doc-toeplitz, *note hadamard: doc-hadamard, *note wilkinson:
     doc-wilkinson, *note compan: doc-compan, *note pascal: doc-pascal.

 -- Function File:  sylvester_matrix (K)
     Return the Sylvester matrix of order n = 2^k.

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note hilb: doc-hilb, *note invhilb: doc-invhilb, *note toeplitz:
     doc-toeplitz.

 -- Function File:  toeplitz (C, R)
     Return the Toeplitz matrix constructed given the first column C,
     and (optionally) the first row R.  If the first element of C is
     not the same as the first element of R, the first element of C is
     used.  If the second argument is omitted, the first row is taken
     to be the same as the first column.

     A square Toeplitz matrix has the form:

          c(0)  r(1)   r(2)  ...  r(n)
          c(1)  c(0)   r(1)  ... r(n-1)
          c(2)  c(1)   c(0)  ... r(n-2)
           .     ,      ,   .      .
           .     ,      ,     .    .
           .     ,      ,       .  .
          c(n) c(n-1) c(n-2) ...  c(0)

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note hilb:
     doc-hilb, *note invhilb: doc-invhilb.

 -- Function File:  vander (C, N)
     Return the Vandermonde matrix whose next to last column is C.  If
     N is specified, it determines the number of columns; otherwise, N
     is taken to be equal to the length of C.

     A Vandermonde matrix has the form:

          c(1)^(n-1) ... c(1)^2  c(1)  1
          c(2)^(n-1) ... c(2)^2  c(2)  1
              .     .      .      .    .
              .       .    .      .    .
              .         .  .      .    .
          c(n)^(n-1) ... c(n)^2  c(n)  1

     *See also:* *note hankel: doc-hankel, *note sylvester_matrix:
     doc-sylvester_matrix, *note hilb: doc-hilb, *note invhilb:
     doc-invhilb, *note toeplitz: doc-toeplitz.

 -- Function File:  wilkinson (N)
     Return the Wilkinson matrix of order N.

     *See also:* *note hankel: doc-hankel, *note vander: doc-vander,
     *note sylvester_matrix: doc-sylvester_matrix, *note hilb:
     doc-hilb, *note invhilb: doc-invhilb, *note toeplitz:
     doc-toeplitz, *note hadamard: doc-hadamard, *note rosser:
     doc-rosser, *note compan: doc-compan, *note pascal: doc-pascal.


File: octave.info,  Node: Arithmetic,  Next: Linear Algebra,  Prev: Matrix Manipulation,  Up: Top

17 Arithmetic
*************

Unless otherwise noted, all of the functions described in this chapter
will work for real and complex scalar, vector, or matrix arguments.
Functions described as "mapping functions" apply the given operation
individually to each element when given a matrix argument.  For example,

     sin ([1, 2; 3, 4])
          =>  0.84147   0.90930
              0.14112  -0.75680

* Menu:

* Exponents and Logarithms::
* Complex Arithmetic::
* Trigonometry::
* Sums and Products::
* Utility Functions::
* Special Functions::
* Coordinate Transformations::
* Mathematical Constants::


File: octave.info,  Node: Exponents and Logarithms,  Next: Complex Arithmetic,  Up: Arithmetic

17.1 Exponents and Logarithms
=============================

 -- Mapping Function:  exp (X)
     Compute `e^x' for each element of X.  To compute the matrix
     exponential, see *note Linear Algebra::.

     *See also:* *note log: doc-log.

 -- Mapping Function:  expm1 (X)
     Compute `exp (X) - 1' accurately in the neighborhood of zero.

     *See also:* *note exp: doc-exp.

 -- Mapping Function:  log (X)
     Compute the natural logarithm, `ln (X)', for each element of X.
     To compute the matrix logarithm, see *note Linear Algebra::.

     *See also:* *note exp: doc-exp, *note log1p: doc-log1p, *note
     log2: doc-log2, *note log10: doc-log10, *note logspace:
     doc-logspace.

 -- Mapping Function:  log1p (X)
     Compute `log (1 + X)' accurately in the neighborhood of zero.

     *See also:* *note log: doc-log, *note exp: doc-exp, *note expm1:
     doc-expm1.

 -- Mapping Function:  log10 (X)
     Compute the base-10 logarithm of each element of X.

     *See also:* *note log: doc-log, *note log2: doc-log2, *note
     logspace: doc-logspace, *note exp: doc-exp.

 -- Mapping Function:  log2 (X)
 -- Mapping Function: [F, E] = log2 (X)
     Compute the base-2 logarithm of each element of X.

     If called with two output arguments, split X into binary mantissa
     and exponent so that `1/2 <= abs(f) < 1' and E is an integer.  If
     `x = 0', `f = e = 0'.

     *See also:* *note pow2: doc-pow2, *note log: doc-log, *note log10:
     doc-log10, *note exp: doc-exp.

 -- Function File:  nextpow2 (X)
     If X is a scalar, return the first integer N such that 2^n >= abs
     (x).

     If X is a vector, return `nextpow2 (length (X))'.

     *See also:* *note pow2: doc-pow2, *note log2: doc-log2.

 -- Function File:  nthroot (X, N)
     Compute the n-th root of X, returning real results for real
     components of X.  For example

          nthroot (-1, 3)
          => -1
          (-1) ^ (1 / 3)
          => 0.50000 - 0.86603i


 -- Mapping Function:  pow2 (X)
 -- Mapping Function:  pow2 (F, E)
     With one argument, computes 2 .^ x for each element of X.

     With two arguments, returns f .* (2 .^ e).

     *See also:* *note log2: doc-log2, *note nextpow2: doc-nextpow2.

 -- Function File:  reallog (X)
     Return the real-valued natural logarithm of each element of X.
     Report an error if any element results in a complex return value.

     *See also:* *note log: doc-log, *note realpow: doc-realpow, *note
     realsqrt: doc-realsqrt.

 -- Function File:  realpow (X, Y)
     Compute the real-valued, element-by-element power operator.  This
     is equivalent to `X .^ Y', except that `realpow' reports an error
     if any return value is complex.

     *See also:* *note reallog: doc-reallog, *note realsqrt:
     doc-realsqrt.

 -- Function File:  realsqrt (X)
     Return the real-valued square root of each element of X.  Report an
     error if any element results in a complex return value.

     *See also:* *note sqrt: doc-sqrt, *note realpow: doc-realpow,
     *note reallog: doc-reallog.

 -- Mapping Function:  sqrt (X)
     Compute the square root of each element of X.  If X is negative, a
     complex result is returned.  To compute the matrix square root, see
     *note Linear Algebra::.

     *See also:* *note realsqrt: doc-realsqrt.


File: octave.info,  Node: Complex Arithmetic,  Next: Trigonometry,  Prev: Exponents and Logarithms,  Up: Arithmetic

17.2 Complex Arithmetic
=======================

In the descriptions of the following functions, Z is the complex number
X + IY, where I is defined as `sqrt (-1)'.

 -- Mapping Function:  abs (Z)
     Compute the magnitude of Z, defined as |Z| = `sqrt (x^2 + y^2)'.

     For example,

          abs (3 + 4i)
               => 5

 -- Mapping Function:  arg (Z)
 -- Mapping Function:  angle (Z)
     Compute the argument of Z, defined as, THETA = `atan2 (Y, X)', in
     radians.

     For example,

          arg (3 + 4i)
               => 0.92730

 -- Mapping Function:  conj (Z)
     Return the complex conjugate of Z, defined as `conj (Z)' = X - IY.

     *See also:* *note real: doc-real, *note imag: doc-imag.

 -- Function File:  cplxpair (Z)
 -- Function File:  cplxpair (Z, TOL)
 -- Function File:  cplxpair (Z, TOL, DIM)
     Sort the numbers Z into complex conjugate pairs ordered by
     increasing real part.  Place the negative imaginary complex number
     first within each pair.  Place all the real numbers (those with
     `abs (imag (Z) / Z) < TOL)') after the complex pairs.

     If TOL is unspecified the default value is 100*`eps'.

     By default the complex pairs are sorted along the first
     non-singleton dimension of Z.  If DIM is specified, then the
     complex pairs are sorted along this dimension.

     Signal an error if some complex numbers could not be paired.
     Signal an error if all complex numbers are not exact conjugates
     (to within TOL).  Note that there is no defined order for pairs
     with identical real parts but differing imaginary parts.

          cplxpair (exp(2i*pi*[0:4]'/5)) == exp(2i*pi*[3; 2; 4; 1; 0]/5)

 -- Mapping Function:  imag (Z)
     Return the imaginary part of Z as a real number.

     *See also:* *note real: doc-real, *note conj: doc-conj.

 -- Mapping Function:  real (Z)
     Return the real part of Z.

     *See also:* *note imag: doc-imag, *note conj: doc-conj.


File: octave.info,  Node: Trigonometry,  Next: Sums and Products,  Prev: Complex Arithmetic,  Up: Arithmetic

17.3 Trigonometry
=================

Octave provides the following trigonometric functions where angles are
specified in radians.  To convert from degrees to radians multiply by
`pi/180' (e.g., `sin (30 * pi/180)' returns the sine of 30 degrees).  As
an alternative, Octave provides a number of trigonometric functions
which work directly on an argument specified in degrees.  These
functions are named after the base trigonometric function with a `d'
suffix.  For example, `sin' expects an angle in radians while `sind'
expects an angle in degrees.

 -- Mapping Function:  sin (X)
     Compute the sine for each element of X in radians.

     *See also:* *note asin: doc-asin, *note sind: doc-sind, *note
     sinh: doc-sinh.

 -- Mapping Function:  cos (X)
     Compute the cosine for each element of X in radians.

     *See also:* *note acos: doc-acos, *note cosd: doc-cosd, *note
     cosh: doc-cosh.

 -- Mapping Function:  tan (Z)
     Compute the tangent for each element of X in radians.

     *See also:* *note atan: doc-atan, *note tand: doc-tand, *note
     tanh: doc-tanh.

 -- Mapping Function:  sec (X)
     Compute the secant for each element of X in radians.

     *See also:* *note asec: doc-asec, *note secd: doc-secd, *note
     sech: doc-sech.

 -- Mapping Function:  csc (X)
     Compute the cosecant for each element of X in radians.

     *See also:* *note acsc: doc-acsc, *note cscd: doc-cscd, *note
     csch: doc-csch.

 -- Mapping Function:  cot (X)
     Compute the cotangent for each element of X in radians.

     *See also:* *note acot: doc-acot, *note cotd: doc-cotd, *note
     coth: doc-coth.

 -- Mapping Function:  asin (X)
     Compute the inverse sine in radians for each element of X.

     *See also:* *note sin: doc-sin, *note asind: doc-asind.

 -- Mapping Function:  acos (X)
     Compute the inverse cosine in radians for each element of X.

     *See also:* *note cos: doc-cos, *note acosd: doc-acosd.

 -- Mapping Function:  atan (X)
     Compute the inverse tangent in radians for each element of X.

     *See also:* *note tan: doc-tan, *note atand: doc-atand.

 -- Mapping Function:  asec (X)
     Compute the inverse secant in radians for each element of X.

     *See also:* *note sec: doc-sec, *note asecd: doc-asecd.

 -- Mapping Function:  acsc (X)
     Compute the inverse cosecant in radians for each element of X.

     *See also:* *note csc: doc-csc, *note acscd: doc-acscd.

 -- Mapping Function:  acot (X)
     Compute the inverse cotangent in radians for each element of X.

     *See also:* *note cot: doc-cot, *note acotd: doc-acotd.

 -- Mapping Function:  sinh (X)
     Compute the hyperbolic sine for each element of X.

     *See also:* *note asinh: doc-asinh, *note cosh: doc-cosh, *note
     tanh: doc-tanh.

 -- Mapping Function:  cosh (X)
     Compute the hyperbolic cosine for each element of X.

     *See also:* *note acosh: doc-acosh, *note sinh: doc-sinh, *note
     tanh: doc-tanh.

 -- Mapping Function:  tanh (X)
     Compute hyperbolic tangent for each element of X.

     *See also:* *note atanh: doc-atanh, *note sinh: doc-sinh, *note
     cosh: doc-cosh.

 -- Mapping Function:  sech (X)
     Compute the hyperbolic secant of each element of X.

     *See also:* *note asech: doc-asech.

 -- Mapping Function:  csch (X)
     Compute the hyperbolic cosecant of each element of X.

     *See also:* *note acsch: doc-acsch.

 -- Mapping Function:  coth (X)
     Compute the hyperbolic cotangent of each element of X.

     *See also:* *note acoth: doc-acoth.

 -- Mapping Function:  asinh (X)
     Compute the inverse hyperbolic sine for each element of X.

     *See also:* *note sinh: doc-sinh.

 -- Mapping Function:  acosh (X)
     Compute the inverse hyperbolic cosine for each element of X.

     *See also:* *note cosh: doc-cosh.

 -- Mapping Function:  atanh (X)
     Compute the inverse hyperbolic tangent for each element of X.

     *See also:* *note tanh: doc-tanh.

 -- Mapping Function:  asech (X)
     Compute the inverse hyperbolic secant of each element of X.

     *See also:* *note sech: doc-sech.

 -- Mapping Function:  acsch (X)
     Compute the inverse hyperbolic cosecant of each element of X.

     *See also:* *note csch: doc-csch.

 -- Mapping Function:  acoth (X)
     Compute the inverse hyperbolic cotangent of each element of X.

     *See also:* *note coth: doc-coth.

 -- Mapping Function:  atan2 (Y, X)
     Compute atan (Y / X) for corresponding elements of Y and X.
     Signal an error if Y and X do not match in size and orientation.

   Octave provides the following trigonometric functions where angles
are specified in degrees.  These functions produce true zeros at the
appropriate intervals rather than the small roundoff error that occurs
when using radians.  For example:
     cosd (90)
          => 0
     cos (pi/2)
          => 6.1230e-17

 -- Function File:  sind (X)
     Compute the sine for each element of X in degrees.  Returns zero
     for elements where `X/180' is an integer.

     *See also:* *note asind: doc-asind, *note sin: doc-sin.

 -- Function File:  cosd (X)
     Compute the cosine for each element of X in degrees.  Returns zero
     for elements where `(X-90)/180' is an integer.

     *See also:* *note acosd: doc-acosd, *note cos: doc-cos.

 -- Function File:  tand (X)
     Compute the tangent for each element of X in degrees.  Returns zero
     for elements where `X/180' is an integer and `Inf' for elements
     where `(X-90)/180' is an integer.

     *See also:* *note atand: doc-atand, *note tan: doc-tan.

 -- Function File:  secd (X)
     Compute the secant for each element of X in degrees.

     *See also:* *note asecd: doc-asecd, *note sec: doc-sec.

 -- Function File:  cscd (X)
     Compute the cosecant for each element of X in degrees.

     *See also:* *note acscd: doc-acscd, *note csc: doc-csc.

 -- Function File:  cotd (X)
     Compute the cotangent for each element of X in degrees.

     *See also:* *note acotd: doc-acotd, *note cot: doc-cot.

 -- Function File:  asind (X)
     Compute the inverse sine in degrees for each element of X.

     *See also:* *note sind: doc-sind, *note asin: doc-asin.

 -- Function File:  acosd (X)
     Compute the inverse cosine in degrees for each element of X.

     *See also:* *note cosd: doc-cosd, *note acos: doc-acos.

 -- Function File:  atand (X)
     Compute the inverse tangent in degrees for each element of X.

     *See also:* *note tand: doc-tand, *note atan: doc-atan.

 -- Function File:  asecd (X)
     Compute the inverse secant in degrees for each element of X.

     *See also:* *note secd: doc-secd, *note asec: doc-asec.

 -- Function File:  acscd (X)
     Compute the inverse cosecant in degrees for each element of X.

     *See also:* *note cscd: doc-cscd, *note acsc: doc-acsc.

 -- Function File:  acotd (X)
     Compute the inverse cotangent in degrees for each element of X.

     *See also:* *note cotd: doc-cotd, *note acot: doc-acot.


File: octave.info,  Node: Sums and Products,  Next: Utility Functions,  Prev: Trigonometry,  Up: Arithmetic

17.4 Sums and Products
======================

 -- Built-in Function:  sum (X)
 -- Built-in Function:  sum (X, DIM)
 -- Built-in Function:  sum (..., 'native')
     Sum of elements along dimension DIM.  If DIM is omitted, it
     defaults to 1 (column-wise sum).

     As a special case, if X is a vector and DIM is omitted, return the
     sum of the elements.

     If the optional argument 'native' is given, then the sum is
     performed in the same type as the original argument, rather than
     in the default double type.  For example

          sum ([true, true])
            => 2
          sum ([true, true], 'native')
            => true

     *See also:* *note cumsum: doc-cumsum, *note sumsq: doc-sumsq,
     *note prod: doc-prod.

 -- Built-in Function:  prod (X)
 -- Built-in Function:  prod (X, DIM)
     Product of elements along dimension DIM.  If DIM is omitted, it
     defaults to 1 (column-wise products).

     As a special case, if X is a vector and DIM is omitted, return the
     product of the elements.

     *See also:* *note cumprod: doc-cumprod, *note sum: doc-sum.

 -- Built-in Function:  cumsum (X)
 -- Built-in Function:  cumsum (X, DIM)
 -- Built-in Function:  cumsum (..., 'native')
     Cumulative sum of elements along dimension DIM.  If DIM is
     omitted, it defaults to 1 (column-wise cumulative sums).

     As a special case, if X is a vector and DIM is omitted, return the
     cumulative sum of the elements as a vector with the same
     orientation as X.

     The "native" argument implies the summation is performed in native
     type.   See `sum' for a complete description and example of the
     use of "native".

     *See also:* *note sum: doc-sum, *note cumprod: doc-cumprod.

 -- Built-in Function:  cumprod (X)
 -- Built-in Function:  cumprod (X, DIM)
     Cumulative product of elements along dimension DIM.  If DIM is
     omitted, it defaults to 1 (column-wise cumulative products).

     As a special case, if X is a vector and DIM is omitted, return the
     cumulative product of the elements as a vector with the same
     orientation as X.

     *See also:* *note prod: doc-prod, *note cumsum: doc-cumsum.

 -- Built-in Function:  sumsq (X)
 -- Built-in Function:  sumsq (X, DIM)
     Sum of squares of elements along dimension DIM.  If DIM is
     omitted, it defaults to 1 (column-wise sum of squares).

     As a special case, if X is a vector and DIM is omitted, return the
     sum of squares of the elements.

     This function is conceptually equivalent to computing
          sum (x .* conj (x), dim)
     but it uses less memory and avoids calling `conj' if X is real.

     *See also:* *note sum: doc-sum.

 -- Function File:  accumarray (SUBS, VALS, SZ, FUNC, FILLVAL, ISSPARSE)
 -- Function File:  accumarray (CSUBS, VALS, ...)
     Create an array by accumulating the elements of a vector into the
     positions defined by their subscripts.  The subscripts are defined
     by the rows of the matrix SUBS and the values by VALS.  Each row
     of SUBS corresponds to one of the values in VALS.

     The size of the matrix will be determined by the subscripts
     themselves.  However, if SZ is defined it determines the matrix
     size.  The length of SZ must correspond to the number of columns
     in SUBS.

     The default action of `accumarray' is to sum the elements with the
     same subscripts.  This behavior can be modified by defining the
     FUNC function.  This should be a function or function handle that
     accepts a column vector and returns a scalar.  The result of the
     function should not depend on the order of the subscripts.

     The elements of the returned array that have no subscripts
     associated with them are set to zero.  Defining FILLVAL to some
     other value allows these values to be defined.

     By default `accumarray' returns a full matrix.  If ISSPARSE is
     logically true, then a sparse matrix is returned instead.

     An example of the use of `accumarray' is:

          accumarray ([1,1,1;2,1,2;2,3,2;2,1,2;2,3,2], 101:105)
          => ans(:,:,1) = [101, 0, 0; 0, 0, 0]
             ans(:,:,2) = [0, 0, 0; 206, 0, 208]


File: octave.info,  Node: Utility Functions,  Next: Special Functions,  Prev: Sums and Products,  Up: Arithmetic

17.5 Utility Functions
======================

 -- Mapping Function:  ceil (X)
     Return the smallest integer not less than X.  This is equivalent to
     rounding towards positive infinity.  If X is complex, return `ceil
     (real (X)) + ceil (imag (X)) * I'.
          ceil ([-2.7, 2.7])
             =>  -2   3

     *See also:* *note floor: doc-floor, *note round: doc-round, *note
     fix: doc-fix.

 -- Function File:  cross (X, Y)
 -- Function File:  cross (X, Y, DIM)
     Compute the vector cross product of two 3-dimensional vectors X
     and Y.

          cross ([1,1,0], [0,1,1])
               => [ 1; -1; 1 ]

     If X and Y are matrices, the cross product is applied along the
     first dimension with 3 elements.  The optional argument DIM forces
     the cross product to be calculated along the specified dimension.

     *See also:* *note dot: doc-dot.

 -- Function File: D = del2 (M)
 -- Function File: D = del2 (M, H)
 -- Function File: D = del2 (M, DX, DY, ...)
     Calculate the discrete Laplace operator.  For a 2-dimensional
     matrix M this is defined as

                1    / d^2            d^2         \
          D  = --- * | ---  M(x,y) +  ---  M(x,y) |
                4    \ dx^2           dy^2        /

     For N-dimensional arrays the sum in parentheses is expanded to
     include second derivatives over the additional higher dimensions.

     The spacing between evaluation points may be defined by H, which
     is a scalar defining the equidistant spacing in all dimensions.
     Alternatively, the spacing in each dimension may be defined
     separately by DX, DY, etc.  A scalar spacing argument defines
     equidistant spacing, whereas a vector argument can be used to
     specify variable spacing.  The length of the spacing vectors must
     match the respective dimension of M.  The default spacing value is
     1.

     At least 3 data points are needed for each dimension.  Boundary
     points are calculated from the linear extrapolation of interior
     points.

     *See also:* *note gradient: doc-gradient, *note diff: doc-diff.

 -- Function File: P = factor (Q)
 -- Function File: [P, N] = factor (Q)
     Return prime factorization of Q.  That is, `prod (P) == Q' and
     every element of P is a prime number.  If `Q == 1', returns 1.

     With two output arguments, return the unique primes P and their
     multiplicities.  That is, `prod (P .^ N) == Q'.

     *See also:* *note gcd: doc-gcd, *note lcm: doc-lcm.

 -- Function File:  factorial (N)
     Return the factorial of N where N is a positive integer.  If N is
     a scalar, this is equivalent to `prod (1:N)'.  For vector or
     matrix arguments, return the factorial of each element in the
     array.  For non-integers see the generalized factorial function
     `gamma'.

     *See also:* *note prod: doc-prod, *note gamma: doc-gamma.

 -- Mapping Function:  fix (X)
     Truncate fractional portion of X and return the integer portion.
     This is equivalent to rounding towards zero.  If X is complex,
     return `fix (real (X)) + fix (imag (X)) * I'.
          fix ([-2.7, 2.7])
             => -2   2

     *See also:* *note ceil: doc-ceil, *note floor: doc-floor, *note
     round: doc-round.

 -- Mapping Function:  floor (X)
     Return the largest integer not greater than X.  This is equivalent
     to rounding towards negative infinity.  If X is complex, return
     `floor (real (X)) + floor (imag (X)) * I'.
          floor ([-2.7, 2.7])
               => -3   2

     *See also:* *note ceil: doc-ceil, *note round: doc-round, *note
     fix: doc-fix.

 -- Mapping Function:  fmod (X, Y)
     Compute the floating point remainder of dividing X by Y using the
     C library function `fmod'.  The result has the same sign as X.  If
     Y is zero, the result is implementation-dependent.

     *See also:* *note mod: doc-mod, *note rem: doc-rem.

 -- Loadable Function: G = gcd (A)
 -- Loadable Function: G = gcd (A1, A2, ...)
 -- Loadable Function: [G, V1, ...] = gcd (A1, A2, ...)
     Compute the greatest common divisor of the elements of A.  If more
     than one argument is given all arguments must be the same size or
     scalar.    In this case the greatest common divisor is calculated
     for each element individually.  All elements must be integers.
     For example,

          gcd ([15, 20])
              =>  5

     and

          gcd ([15, 9], [20, 18])
              =>  5  9

     Optional return arguments V1, etc., contain integer vectors such
     that,

          G = V1 .* A1 + V2 .* A2 + ...

     For backward compatibility with previous versions of this
     function, when all arguments are scalar, a single return argument
     V1 containing all of the values of V1, ... is acceptable.

     *See also:* *note lcm: doc-lcm, *note factor: doc-factor.

 -- Function File: DX = gradient (M)
 -- Function File: [DX, DY, DZ, ...] = gradient (M)
 -- Function File: [...] = gradient (M, S)
 -- Function File: [...] = gradient (M, X, Y, Z, ...)
 -- Function File: [...] = gradient (F, X0)
 -- Function File: [...] = gradient (F, X0, S)
 -- Function File: [...] = gradient (F, X0, X, Y, ...)
     Calculate the gradient of sampled data or a function.  If M is a
     vector, calculate the one-dimensional gradient of M.  If M is a
     matrix the gradient is calculated for each dimension.

     `[DX, DY] = gradient (M)' calculates the one dimensional gradient
     for X and Y direction if M is a matrix.  Additional return
     arguments can be use for multi-dimensional matrices.

     A constant spacing between two points can be provided by the S
     parameter.  If S is a scalar, it is assumed to be the spacing for
     all dimensions.  Otherwise, separate values of the spacing can be
     supplied by the X, ... arguments.  Scalar values specify an
     equidistant spacing.  Vector values for the X, ... arguments
     specify the coordinate for that dimension.  The length must match
     their respective dimension of M.

     At boundary points a linear extrapolation is applied.  Interior
     points are calculated with the first approximation of the
     numerical gradient

          y'(i) = 1/(x(i+1)-x(i-1)) * (y(i-1)-y(i+1)).

     If the first argument F is a function handle, the gradient of the
     function at the points in X0 is approximated using central
     difference.  For example, `gradient (@cos, 0)' approximates the
     gradient of the cosine function in the point x0 = 0.  As with
     sampled data, the spacing values between the points from which the
     gradient is estimated can be set via the S or DX, DY, ...
     arguments.  By default a spacing of 1 is used.

     *See also:* *note diff: doc-diff, *note del2: doc-del2.

 -- Built-in Function:  hypot (X, Y)
     Compute the element-by-element square root of the sum of the
     squares of X and Y.  This is equivalent to `sqrt (X.^2 + Y.^2)',
     but calculated in a manner that avoids overflows for large values
     of X or Y.

 -- Mapping Function:  lcm (X)
 -- Mapping Function:  lcm (X, ...)
     Compute the least common multiple of the elements of X, or of the
     list of all arguments.  For example,

          lcm (a1, ..., ak)

     is the same as

          lcm ([a1, ..., ak]).

     All elements must be the same size or scalar.

     *See also:* *note factor: doc-factor, *note gcd: doc-gcd.

 -- Function File:  list_primes (N)
     List the first N primes.  If N is unspecified, the first 25 primes
     are listed.

     The algorithm used is from page 218 of the TeXbook.

     *See also:* *note primes: doc-primes, *note isprime: doc-isprime.

 -- Loadable Function:  max (X)
 -- Loadable Function:  max (X, Y)
 -- Loadable Function:  max (X, Y, DIM)
 -- Loadable Function: [W, IW] = max (X)
     For a vector argument, return the maximum value.  For a matrix
     argument, return the maximum value from each column, as a row
     vector, or over the dimension DIM if defined.  For two matrices
     (or a matrix and scalar), return the pair-wise maximum.  Thus,

          max (max (X))

     returns the largest element of the matrix X, and

          max (2:5, pi)
              =>  3.1416  3.1416  4.0000  5.0000
     compares each element of the range `2:5' with `pi', and returns a
     row vector of the maximum values.

     For complex arguments, the magnitude of the elements are used for
     comparison.

     If called with one input and two output arguments, `max' also
     returns the first index of the maximum value(s).  Thus,

          [x, ix] = max ([1, 3, 5, 2, 5])
              =>  x = 5
                  ix = 3

     *See also:* *note min: doc-min, *note cummax: doc-cummax, *note
     cummin: doc-cummin.

 -- Loadable Function:  min (X)
 -- Loadable Function:  min (X, Y)
 -- Loadable Function:  min (X, Y, DIM)
 -- Loadable Function: [W, IW] = min (X)
     For a vector argument, return the minimum value.  For a matrix
     argument, return the minimum value from each column, as a row
     vector, or over the dimension DIM if defined.  For two matrices
     (or a matrix and scalar), return the pair-wise minimum.  Thus,

          min (min (X))

     returns the smallest element of X, and

          min (2:5, pi)
              =>  2.0000  3.0000  3.1416  3.1416
     compares each element of the range `2:5' with `pi', and returns a
     row vector of the minimum values.

     For complex arguments, the magnitude of the elements are used for
     comparison.

     If called with one input and two output arguments, `min' also
     returns the first index of the minimum value(s).  Thus,

          [x, ix] = min ([1, 3, 0, 2, 0])
              =>  x = 0
                  ix = 3

     *See also:* *note max: doc-max, *note cummin: doc-cummin, *note
     cummax: doc-cummax.

 -- Loadable Function:  cummax (X)
 -- Loadable Function:  cummax (X, DIM)
 -- Loadable Function: [W, IW] = cummax (X)
     Return the cumulative maximum values along dimension DIM.  If DIM
     is unspecified it defaults to column-wise operation.  For example,

          cummax ([1 3 2 6 4 5])
              =>  1  3  3  6  6  6

     The call
          [w, iw] = cummax (x, dim)

     is equivalent to the following code:
          w = iw = zeros (size (x));
          idxw = idxx = repmat ({':'}, 1, ndims (x));
          for i = 1:size (x, dim)
            idxw{dim} = i; idxx{dim} = 1:i;
            [w(idxw{:}), iw(idxw{:})] = max(x(idxx{:}), [], dim);
          endfor

     but computed in a much faster manner.

     *See also:* *note cummin: doc-cummin, *note max: doc-max, *note
     min: doc-min.

 -- Loadable Function:  cummin (X)
 -- Loadable Function:  cummin (X, DIM)
 -- Loadable Function: [W, IW] = cummin (X)
     Return the cumulative minimum values along dimension DIM.  If DIM
     is unspecified it defaults to column-wise operation.  For example,

          cummin ([5 4 6 2 3 1])
              =>  5  4  4  2  2  1

     The call
            [w, iw] = cummin (x, dim)

     is equivalent to the following code:
          w = iw = zeros (size (x));
          idxw = idxx = repmat ({':'}, 1, ndims (x));
          for i = 1:size (x, dim)
            idxw{dim} = i; idxx{dim} = 1:i;
            [w(idxw{:}), iw(idxw{:})] = min(x(idxx{:}), [], dim);
          endfor

     but computed in a much faster manner.

     *See also:* *note cummax: doc-cummax, *note min: doc-min, *note
     max: doc-max.

 -- Mapping Function:  mod (X, Y)
     Compute the modulo of X and Y.  Conceptually this is given by

          x - y .* floor (x ./ y)

     and is written such that the correct modulus is returned for
     integer types.  This function handles negative values correctly.
     That is, `mod (-1, 3)' is 2, not -1, as `rem (-1, 3)' returns.
     `mod (X, 0)' returns X.

     An error results if the dimensions of the arguments do not agree,
     or if either of the arguments is complex.

     *See also:* *note rem: doc-rem, *note fmod: doc-fmod.

 -- Function File:  primes (N)
     Return all primes up to N.

     The algorithm used is the Sieve of Erastothenes.

     Note that if you need a specific number of primes you can use the
     fact the distance from one prime to the next is, on average,
     proportional to the logarithm of the prime.  Integrating, one finds
     that there are about k primes less than k*log(5*k).

     *See also:* *note list_primes: doc-list_primes, *note isprime:
     doc-isprime.

 -- Mapping Function:  rem (X, Y)
     Return the remainder of the division `X / Y', computed using the
     expression

          x - y .* fix (x ./ y)

     An error message is printed if the dimensions of the arguments do
     not agree, or if either of the arguments is complex.

     *See also:* *note mod: doc-mod, *note fmod: doc-fmod.

 -- Mapping Function:  round (X)
     Return the integer nearest to X.  If X is complex, return `round
     (real (X)) + round (imag (X)) * I'.
          round ([-2.7, 2.7])
               => -3   3

     *See also:* *note ceil: doc-ceil, *note floor: doc-floor, *note
     fix: doc-fix.

 -- Mapping Function:  roundb (X)
     Return the integer nearest to X.  If there are two nearest
     integers, return the even one (banker's rounding).  If X is
     complex, return `roundb (real (X)) + roundb (imag (X)) * I'.

     *See also:* *note round: doc-round.

 -- Mapping Function:  sign (X)
     Compute the "signum" function, which is defined as

                     -1, x < 0;
          sign (x) =  0, x = 0;
                      1, x > 0.

     For complex arguments, `sign' returns `x ./ abs (X)'.


File: octave.info,  Node: Special Functions,  Next: Coordinate Transformations,  Prev: Utility Functions,  Up: Arithmetic

17.6 Special Functions
======================

 -- Loadable Function: [A, IERR] = airy (K, Z, OPT)
     Compute Airy functions of the first and second kind, and their
     derivatives.

           K   Function   Scale factor (if 'opt' is supplied)
          ---  --------   ---------------------------------------
           0   Ai (Z)     exp ((2/3) * Z * sqrt (Z))
           1   dAi(Z)/dZ  exp ((2/3) * Z * sqrt (Z))
           2   Bi (Z)     exp (-abs (real ((2/3) * Z *sqrt (Z))))
           3   dBi(Z)/dZ  exp (-abs (real ((2/3) * Z *sqrt (Z))))

     The function call `airy (Z)' is equivalent to `airy (0, Z)'.

     The result is the same size as Z.

     If requested, IERR contains the following status information and
     is the same size as the result.

       0. Normal return.

       1. Input error, return `NaN'.

       2. Overflow, return `Inf'.

       3. Loss of significance by argument reduction results in less
          than half  of machine accuracy.

       4. Complete loss of significance by argument reduction, return
          `NaN'.

       5. Error--no computation, algorithm termination condition not
          met, return `NaN'.

 -- Loadable Function: [J, IERR] = besselj (ALPHA, X, OPT)
 -- Loadable Function: [Y, IERR] = bessely (ALPHA, X, OPT)
 -- Loadable Function: [I, IERR] = besseli (ALPHA, X, OPT)
 -- Loadable Function: [K, IERR] = besselk (ALPHA, X, OPT)
 -- Loadable Function: [H, IERR] = besselh (ALPHA, K, X, OPT)
     Compute Bessel or Hankel functions of various kinds:

    `besselj'
          Bessel functions of the first kind.  If the argument OPT is
          supplied, the result is multiplied by `exp(-abs(imag(x)))'.

    `bessely'
          Bessel functions of the second kind.  If the argument OPT is
          supplied, the result is multiplied by `exp(-abs(imag(x)))'.

    `besseli'
          Modified Bessel functions of the first kind.  If the argument
          OPT is supplied, the result is multiplied by
          `exp(-abs(real(x)))'.

    `besselk'
          Modified Bessel functions of the second kind.  If the
          argument OPT is supplied, the result is multiplied by
          `exp(x)'.

    `besselh'
          Compute Hankel functions of the first (K = 1) or second (K =
          2) kind.  If the argument OPT is supplied, the result is
          multiplied by `exp (-I*X)' for K = 1 or `exp (I*X)' for K = 2.

     If ALPHA is a scalar, the result is the same size as X.  If X is a
     scalar, the result is the same size as ALPHA.  If ALPHA is a row
     vector and X is a column vector, the result is a matrix with
     `length (X)' rows and `length (ALPHA)' columns.  Otherwise, ALPHA
     and X must conform and the result will be the same size.

     The value of ALPHA must be real.  The value of X may be complex.

     If requested, IERR contains the following status information and
     is the same size as the result.

       0. Normal return.

       1. Input error, return `NaN'.

       2. Overflow, return `Inf'.

       3. Loss of significance by argument reduction results in less
          than half of machine accuracy.

       4. Complete loss of significance by argument reduction, return
          `NaN'.

       5. Error--no computation, algorithm termination condition not
          met, return `NaN'.

 -- Mapping Function:  beta (A, B)
     For real inputs, return the Beta function,

          beta (a, b) = gamma (a) * gamma (b) / gamma (a + b).

 -- Mapping Function:  betainc (X, A, B)
     Return the incomplete Beta function,

                                                x
                                               /
          betainc (x, a, b) = beta (a, b)^(-1) | t^(a-1) (1-t)^(b-1) dt.
                                               /
                                            t=0

     If x has more than one component, both A and B must be scalars.
     If X is a scalar, A and B must be of compatible dimensions.

 -- Mapping Function:  betaln (A, B)
     Return the log of the Beta function,

          betaln (a, b) = gammaln (a) + gammaln (b) - gammaln (a + b)

     *See also:* *note beta: doc-beta, *note betainc: doc-betainc,
     *note gammaln: doc-gammaln.

 -- Mapping Function:  bincoeff (N, K)
     Return the binomial coefficient of N and K, defined as

           /   \
           | n |    n (n-1) (n-2) ... (n-k+1)
           |   |  = -------------------------
           | k |               k!
           \   /

     For example,

          bincoeff (5, 2)
               => 10

     In most cases, the `nchoosek' function is faster for small scalar
     integer arguments.  It also warns about loss of precision for big
     arguments.

     *See also:* *note nchoosek: doc-nchoosek.

 -- Function File:  commutation_matrix (M, N)
     Return the commutation matrix  K(m,n)  which is the unique M*N by
     M*N  matrix such that K(m,n) * vec(A) = vec(A')  for all m by n
     matrices A.

     If only one argument M is given, K(m,m)  is returned.

     See Magnus and Neudecker (1988), Matrix differential calculus with
     applications in statistics and econometrics.

 -- Function File:  duplication_matrix (N)
     Return the duplication matrix Dn  which is the unique n^2 by
     n*(n+1)/2  matrix such that Dn vech (A) = vec (A)  for all
     symmetric n by n  matrices A.

     See Magnus and Neudecker (1988), Matrix differential calculus with
     applications in statistics and econometrics.

 -- Mapping Function:  erf (Z)
     Computes the error function,

                                   z
                                  /
          erf (z) = (2/sqrt (pi)) | e^(-t^2) dt
                                  /
                               t=0

     *See also:* *note erfc: doc-erfc, *note erfinv: doc-erfinv.

 -- Mapping Function:  erfc (Z)
     Computes the complementary error function, `1 - erf (Z)'.

     *See also:* *note erf: doc-erf, *note erfinv: doc-erfinv.

 -- Mapping Function:  erfinv (Z)
     Computes the inverse of the error function.

     *See also:* *note erf: doc-erf, *note erfc: doc-erfc.

 -- Mapping Function:  gamma (Z)
     Computes the Gamma function,

                      infinity
                      /
          gamma (z) = | t^(z-1) exp (-t) dt.
                      /
                   t=0

     *See also:* *note gammainc: doc-gammainc, *note lgamma: doc-lgamma.

 -- Mapping Function:  gammainc (X, A)
     Compute the normalized incomplete gamma function,

                                          x
                                1        /
          gammainc (x, a) = ---------    | exp (-t) t^(a-1) dt
                            gamma (a)    /
                                      t=0

     with the limiting value of 1 as X approaches infinity.  The
     standard notation is P(a,x), e.g., Abramowitz and Stegun (6.5.1).

     If A is scalar, then `gammainc (X, A)' is returned for each
     element of X and vice versa.

     If neither X nor A is scalar, the sizes of X and A must agree, and
     GAMMAINC is applied element-by-element.

     *See also:* *note gamma: doc-gamma, *note lgamma: doc-lgamma.

 -- Function File: L = legendre (N, X)
 -- Function File: L = legendre (N, X, NORMALIZATION)
     Compute the Legendre function of degree N and order M = 0 ... N.
     The optional argument, NORMALIZATION, may be one of `"unnorm"',
     `"sch"', or `"norm"'.  The default is `"unnorm"'.  The value of N
     must be a non-negative scalar integer.

     If the optional argument NORMALIZATION is missing or is
     `"unnorm"', compute the Legendre function of degree N and order M
     and return all values for M = 0 ... N.  The return value has one
     dimension more than X.

     The Legendre Function of degree N and order M:

           m        m       2  m/2   d^m
          P(x) = (-1) * (1-x  )    * ----  P (x)
           n                         dx^m   n

     with Legendre polynomial of degree N:

                    1     d^n   2    n
          P (x) = ------ [----(x - 1)  ]
           n      2^n n!  dx^n

     `legendre (3, [-1.0, -0.9, -0.8])' returns the matrix:

           x  |   -1.0   |   -0.9   |  -0.8
          ------------------------------------
          m=0 | -1.00000 | -0.47250 | -0.08000
          m=1 |  0.00000 | -1.99420 | -1.98000
          m=2 |  0.00000 | -2.56500 | -4.32000
          m=3 |  0.00000 | -1.24229 | -3.24000

     If the optional argument `normalization' is `"sch"', compute the
     Schmidt semi-normalized associated Legendre function.  The Schmidt
     semi-normalized associated Legendre function is related to the
     unnormalized Legendre functions by the following:

     For Legendre functions of degree n and order 0:

            0       0
          SP (x) = P (x)
            n       n

     For Legendre functions of degree n and order m:

            m       m          m    2(n-m)! 0.5
          SP (x) = P (x) * (-1)  * [-------]
            n       n               (n+m)!

     If the optional argument NORMALIZATION is `"norm"', compute the
     fully normalized associated Legendre function.  The fully
     normalized associated Legendre function is related to the
     unnormalized Legendre functions by the following:

     For Legendre functions of degree N and order M

            m       m          m    (n+0.5)(n-m)! 0.5
          NP (x) = P (x) * (-1)  * [-------------]
            n       n                   (n+m)!

 -- Mapping Function:  lgamma (X)
 -- Mapping Function:  gammaln (X)
     Return the natural logarithm of the gamma function of X.

     *See also:* *note gamma: doc-gamma, *note gammainc: doc-gammainc.


File: octave.info,  Node: Coordinate Transformations,  Next: Mathematical Constants,  Prev: Special Functions,  Up: Arithmetic

17.7 Coordinate Transformations
===============================

 -- Function File: [THETA, R] = cart2pol (X, Y)
 -- Function File: [THETA, R, Z] = cart2pol (X, Y, Z)
     Transform Cartesian to polar or cylindrical coordinates.  X, Y
     (and Z) must be the same shape, or scalar.  THETA describes the
     angle relative to the positive x-axis.  R is the distance to the
     z-axis (0, 0, z).

     *See also:* *note pol2cart: doc-pol2cart, *note cart2sph:
     doc-cart2sph, *note sph2cart: doc-sph2cart.

 -- Function File: [X, Y] = pol2cart (THETA, R)
 -- Function File: [X, Y, Z] = pol2cart (THETA, R, Z)
     Transform polar or cylindrical to Cartesian coordinates.  THETA, R
     (and Z) must be the same shape, or scalar.  THETA describes the
     angle relative to the positive x-axis.  R is the distance to the
     z-axis (0, 0, z).

     *See also:* *note cart2pol: doc-cart2pol, *note cart2sph:
     doc-cart2sph, *note sph2cart: doc-sph2cart.

 -- Function File: [THETA, PHI, R] = cart2sph (X, Y, Z)
     Transform Cartesian to spherical coordinates.  X, Y and Z must be
     the same shape, or scalar.  THETA describes the angle relative to
     the positive x-axis.  PHI is the angle relative to the xy-plane.
     R is the distance to the origin (0, 0, 0).

     *See also:* *note pol2cart: doc-pol2cart, *note cart2pol:
     doc-cart2pol, *note sph2cart: doc-sph2cart.

 -- Function File: [X, Y, Z] = sph2cart (THETA, PHI, R)
     Transform spherical to Cartesian coordinates.  X, Y and Z must be
     the same shape, or scalar.  THETA describes the angle relative to
     the positive x-axis.  PHI is the angle relative to the xy-plane.
     R is the distance to the origin (0, 0, 0).

     *See also:* *note pol2cart: doc-pol2cart, *note cart2pol:
     doc-cart2pol, *note cart2sph: doc-cart2sph.


File: octave.info,  Node: Mathematical Constants,  Prev: Coordinate Transformations,  Up: Arithmetic

17.8 Mathematical Constants
===========================

 -- Built-in Function:  e
 -- Built-in Function:  e (N)
 -- Built-in Function:  e (N, M)
 -- Built-in Function:  e (N, M, K, ...)
 -- Built-in Function:  e (..., CLASS)
     Return a scalar, matrix, or N-dimensional array whose elements are
     all equal to the base of natural logarithms.  The constant `e'
     satisfies the equation `log' (e) = 1.

     When called with no arguments, return a scalar with the value e.
     When called with a single argument, return a square matrix with
     the dimension specified.  When called with more than one scalar
     argument the first two arguments are taken as the number of rows
     and columns and any further arguments specify additional matrix
     dimensions.  The optional argument CLASS specifies the return type
     and may be either "double" or "single".

 -- Built-in Function:  pi
 -- Built-in Function:  pi (N)
 -- Built-in Function:  pi (N, M)
 -- Built-in Function:  pi (N, M, K, ...)
 -- Built-in Function:  pi (..., CLASS)
     Return a scalar, matrix, or N-dimensional array whose elements are
     all equal to the ratio of the circumference of a circle to its
     diameter.  Internally, `pi' is computed as `4.0 * atan (1.0)'.

     When called with no arguments, return a scalar with the value of
     pi.  When called with a single argument, return a square matrix
     with the dimension specified.  When called with more than one
     scalar argument the first two arguments are taken as the number of
     rows and columns and any further arguments specify additional
     matrix dimensions.  The optional argument CLASS specifies the
     return type and may be either "double" or "single".

 -- Built-in Function:  I
 -- Built-in Function:  I (N)
 -- Built-in Function:  I (N, M)
 -- Built-in Function:  I (N, M, K, ...)
 -- Built-in Function:  I (..., CLASS)
     Return a scalar, matrix, or N-dimensional array whose elements are
     all equal to the pure imaginary unit, defined as `sqrt (-1)'.   I,
     and its equivalents i, J, and j, are functions so any of the names
     may be reused for other purposes (such as i for a counter
     variable).

     When called with no arguments, return a scalar with the value i.
     When called with a single argument, return a square matrix with
     the dimension specified.  When called with more than one scalar
     argument the first two arguments are taken as the number of rows
     and columns and any further arguments specify additional matrix
     dimensions.  The optional argument CLASS specifies the return type
     and may be either "double" or "single".

 -- Built-in Function:  Inf
 -- Built-in Function:  Inf (N)
 -- Built-in Function:  Inf (N, M)
 -- Built-in Function:  Inf (N, M, K, ...)
 -- Built-in Function:  Inf (..., CLASS)
     Return a scalar, matrix or N-dimensional array whose elements are
     all equal to the IEEE representation for positive infinity.

     Infinity is produced when results are too large to be represented
     using the the IEEE floating point format for numbers.  Two common
     examples which produce infinity are division by zero and overflow.
          [1/0 e^800]
          =>
          Inf   Inf

     When called with no arguments, return a scalar with the value
     `Inf'.  When called with a single argument, return a square matrix
     with the dimension specified.  When called with more than one
     scalar argument the first two arguments are taken as the number of
     rows and columns and any further arguments specify additional
     matrix dimensions.  The optional argument CLASS specifies the
     return type and may be either "double" or "single".

 -- Built-in Function:  NaN
 -- Built-in Function:  NaN (N)
 -- Built-in Function:  NaN (N, M)
 -- Built-in Function:  NaN (N, M, K, ...)
 -- Built-in Function:  NaN (..., CLASS)
     Return a scalar, matrix, or N-dimensional array whose elements are
     all equal to the IEEE symbol NaN (Not a Number).  NaN is the
     result of operations which do not produce a well defined numerical
     result.  Common operations which produce a NaN are arithmetic with
     infinity (Inf - Inf), zero divided by zero (0/0), and any
     operation involving another NaN value (5 + NaN).

     Note that NaN always compares not equal to NaN (NaN != NaN).  This
     behavior is specified by the IEEE standard for floating point
     arithmetic.  To find NaN values, use the `isnan' function.

     When called with no arguments, return a scalar with the value
     `NaN'.  When called with a single argument, return a square matrix
     with the dimension specified.  When called with more than one
     scalar argument the first two arguments are taken as the number of
     rows and columns and any further arguments specify additional
     matrix dimensions.  The optional argument CLASS specifies the
     return type and may be either "double" or "single".

     *See also:* *note isnan: doc-isnan.

 -- Built-in Function:  eps
 -- Built-in Function:  eps (X)
 -- Built-in Function:  eps (N, M)
 -- Built-in Function:  eps (N, M, K, ...)
 -- Built-in Function:  eps (..., CLASS)
     Return a scalar, matrix or N-dimensional array whose elements are
     all eps, the machine precision.  More precisely, `eps' is the
     relative spacing between any two adjacent numbers in the machine's
     floating point system.  This number is obviously system dependent.
     On machines that support IEEE floating point arithmetic, `eps' is
     approximately 2.2204e-16 for double precision and 1.1921e-07 for
     single precision.

     When called with no arguments, return a scalar with the value
     `eps(1.0)'.  Given a single argument X, return the distance
     between X and the next largest value.  When called with more than
     one argument the first two arguments are taken as the number of
     rows and columns and any further arguments specify additional
     matrix dimensions.  The optional argument CLASS specifies the
     return type and may be either "double" or "single".

 -- Built-in Function:  realmax
 -- Built-in Function:  realmax (N)
 -- Built-in Function:  realmax (N, M)
 -- Built-in Function:  realmax (N, M, K, ...)
 -- Built-in Function:  realmax (..., CLASS)
     Return a scalar, matrix or N-dimensional array whose elements are
     all equal to the largest floating point number that is
     representable.  The actual value is system dependent.  On machines
     that support IEEE floating point arithmetic, `realmax' is
     approximately 1.7977e+308 for double precision and 3.4028e+38 for
     single precision.

     When called with no arguments, return a scalar with the value
     `realmax("double")'.  When called with a single argument, return a
     square matrix with the dimension specified.  When called with more
     than one scalar argument the first two arguments are taken as the
     number of rows and columns and any further arguments specify
     additional matrix dimensions.  The optional argument CLASS
     specifies the return type and may be either "double" or "single".

     *See also:* *note realmin: doc-realmin, *note intmax: doc-intmax,
     *note bitmax: doc-bitmax.

 -- Built-in Function:  realmin
 -- Built-in Function:  realmin (N)
 -- Built-in Function:  realmin (N, M)
 -- Built-in Function:  realmin (N, M, K, ...)
 -- Built-in Function:  realmin (..., CLASS)
     Return a scalar, matrix or N-dimensional array whose elements are
     all equal to the smallest normalized floating point number that is
     representable.  The actual value is system dependent.  On machines
     that support IEEE floating point arithmetic, `realmin' is
     approximately 2.2251e-308 for double precision and 1.1755e-38 for
     single precision.

     When called with no arguments, return a scalar with the value
     `realmin("double")'.  When called with a single argument, return a
     square matrix with the dimension specified.  When called with more
     than one scalar argument the first two arguments are taken as the
     number of rows and columns and any further arguments specify
     additional matrix dimensions.  The optional argument CLASS
     specifies the return type and may be either "double" or "single".

     *See also:* *note realmax: doc-realmax, *note intmin: doc-intmin.


File: octave.info,  Node: Linear Algebra,  Next: Nonlinear Equations,  Prev: Arithmetic,  Up: Top

18 Linear Algebra
*****************

This chapter documents the linear algebra functions of Octave.
Reference material for many of these functions may be found in Golub
and Van Loan, `Matrix Computations, 2nd Ed.', Johns Hopkins, 1989, and
in the `LAPACK Users' Guide', SIAM, 1992.

* Menu:

* Techniques used for Linear Algebra::
* Basic Matrix Functions::
* Matrix Factorizations::
* Functions of a Matrix::
* Specialized Solvers::


File: octave.info,  Node: Techniques used for Linear Algebra,  Next: Basic Matrix Functions,  Up: Linear Algebra

18.1 Techniques used for Linear Algebra
=======================================

Octave includes a polymorphic solver, that selects an appropriate
matrix factorization depending on the properties of the matrix itself.
Generally, the cost of determining the matrix type is small relative to
the cost of factorizing the matrix itself, but in any case the matrix
type is cached once it is calculated, so that it is not re-determined
each time it is used in a linear equation.

   The selection tree for how the linear equation is solve or a matrix
inverse is form is given by

  1. If the matrix is upper or lower triangular sparse a forward or
     backward substitution using the LAPACK xTRTRS function, and goto 4.

  2. If the matrix is square, hermitian with a real positive diagonal,
     attempt Cholesky factorization using the LAPACK xPOTRF function.

  3. If the Cholesky factorization failed or the matrix is not
     hermitian with a real positive diagonal, and the matrix is square,
     factorize using the LAPACK xGETRF function.

  4. If the matrix is not square, or any of the previous solvers flags
     a singular or near singular matrix, find a least squares solution
     using the LAPACK xGELSD function.

   The user can force the type of the matrix with the `matrix_type'
function.  This overcomes the cost of discovering the type of the
matrix.  However, it should be noted that identifying the type of the
matrix incorrectly will lead to unpredictable results, and so
`matrix_type' should be used with care.

   It should be noted that the test for whether a matrix is a candidate
for Cholesky factorization, performed above and by the `matrix_type'
function, does not give a certainty that the matrix is Hermitian.
However, the attempt to factorize the matrix will quickly flag a
non-Hermitian matrix.


File: octave.info,  Node: Basic Matrix Functions,  Next: Matrix Factorizations,  Prev: Techniques used for Linear Algebra,  Up: Linear Algebra

18.2 Basic Matrix Functions
===========================

 -- Loadable Function: AA = balance (A, OPT)
 -- Loadable Function: [DD, AA] = balance (A, OPT)
 -- Loadable Function: [D, P, AA] = balance (A, OPT)
 -- Loadable Function: [CC, DD, AA, BB] = balance (A, B, OPT)
     Compute `aa = dd \ a * dd' in which `aa' is a matrix whose row and
     column norms are roughly equal in magnitude, and `dd' = `p * d',
     in which `p' is a permutation matrix and `d' is a diagonal matrix
     of powers of two.  This allows the equilibration to be computed
     without roundoff.  Results of eigenvalue calculation are typically
     improved by balancing first.

     If two output values are requested, `balance' returns the diagonal
     `d' and the permutation `p' separately as vectors.  In this case,
     `dd = eye(n)(p,:) * diag (d)', where `n' is the matrix size.

     If four output values are requested, compute `aa = cc*a*dd' and
     `bb = cc*b*dd)', in which `aa' and `bb' have non-zero elements of
     approximately the same magnitude and `cc' and `dd' are permuted
     diagonal matrices as in `dd' for the algebraic eigenvalue problem.

     The eigenvalue balancing option `opt' may be one of:

    `"noperm"', `"S"'
          Scale only; do not permute.

    `"noscal"', `"P"'
          Permute only; do not scale.

     Algebraic eigenvalue balancing uses standard LAPACK routines.

     Generalized eigenvalue problem balancing uses Ward's algorithm
     (SIAM Journal on Scientific and Statistical Computing, 1981).

 -- Function File:  cond (A,P)
     Compute the P-norm condition number of a matrix.  `cond (A)' is
     defined as `norm (A, P) * norm (inv (A), P)'.  By default `P=2' is
     used which implies a (relatively slow) singular value
     decomposition.  Other possible selections are `P= 1, Inf, inf,
     'Inf', 'fro'' which are generally faster.

     *See also:* *note norm: doc-norm, *note inv: doc-inv, *note det:
     doc-det, *note svd: doc-svd, *note rank: doc-rank.

 -- Loadable Function: [D, RCOND] = det (A)
     Compute the determinant of A using LAPACK for full and UMFPACK for
     sparse matrices.  Return an estimate of the reciprocal condition
     number if requested.

 -- Function File:  dmult (A, B)
     This function has been deprecated.  Use the direct syntax
     `diag(A)*B' which is more readable and now also more efficient.

 -- Function File:  dot (X, Y, DIM)
     Computes the dot product of two vectors.  If X and Y are matrices,
     calculate the dot-product along the first non-singleton dimension.
     If the optional argument DIM is given, calculate the dot-product
     along this dimension.

 -- Loadable Function: LAMBDA = eig (A)
 -- Loadable Function: LAMBDA = eig (A, B)
 -- Loadable Function: [V, LAMBDA] = eig (A)
 -- Loadable Function: [V, LAMBDA] = eig (A, B)
     The eigenvalues (and eigenvectors) of a matrix are computed in a
     several step process which begins with a Hessenberg decomposition,
     followed by a Schur decomposition, from which the eigenvalues are
     apparent.  The eigenvectors, when desired, are computed by further
     manipulations of the Schur decomposition.

     The eigenvalues returned by `eig' are not ordered.

 -- Loadable Function: G = givens (X, Y)
 -- Loadable Function: [C, S] = givens (X, Y)
     Return a 2 by 2 orthogonal matrix `G = [C S; -S' C]' such that `G
     [X; Y] = [*; 0]' with X and Y scalars.

     For example,

          givens (1, 1)
               =>   0.70711   0.70711
                   -0.70711   0.70711

 -- Function File: [G, Y] = planerot (X)
     Given a two-element column vector, returns the 2 by 2 orthogonal
     matrix G such that `Y = G * X' and `Y(2) = 0'.

     *See also:* *note givens: doc-givens.

 -- Loadable Function: [X, RCOND] = inv (A)
 -- Loadable Function: [X, RCOND] = inverse (A)
     Compute the inverse of the square matrix A.  Return an estimate of
     the reciprocal condition number if requested, otherwise warn of an
     ill-conditioned matrix if the reciprocal condition number is small.

     If called with a sparse matrix, then in general X will be a full
     matrix, and so if possible forming the inverse of a sparse matrix
     should be avoided.  It is significantly more accurate and faster
     to do `Y = A \ B', rather than `Y = inv (A) * B'.

 -- Loadable Function: TYPE = matrix_type (A)
 -- Loadable Function: A = matrix_type (A, TYPE)
 -- Loadable Function: A = matrix_type (A, 'upper', PERM)
 -- Loadable Function: A = matrix_type (A, 'lower', PERM)
 -- Loadable Function: A = matrix_type (A, 'banded', NL, NU)
     Identify the matrix type or mark a matrix as a particular type.
     This allows rapid for solutions of linear equations involving A to
     be performed.  Called with a single argument, `matrix_type'
     returns the type of the matrix and caches it for future use.
     Called with more than one argument, `matrix_type' allows the type
     of the matrix to be defined.

     The possible matrix types depend on whether the matrix is full or
     sparse, and can be one of the following

    'unknown'
          Remove any previously cached matrix type, and mark type as
          unknown

    'full'
          Mark the matrix as full.

    'positive definite'
          Probable full positive definite matrix.

    'diagonal'
          Diagonal Matrix.  (Sparse matrices only)

    'permuted diagonal'
          Permuted Diagonal matrix.  The permutation does not need to
          be specifically indicated, as the structure of the matrix
          explicitly gives this.  (Sparse matrices only)

    'upper'
          Upper triangular.  If the optional third argument PERM is
          given, the matrix is assumed to be a permuted upper
          triangular with the permutations defined by the vector PERM.

    'lower'
          Lower triangular.  If the optional third argument PERM is
          given, the matrix is assumed to be a permuted lower
          triangular with the permutations defined by the vector PERM.

    'banded'
    'banded positive definite'
          Banded matrix with the band size of NL below the diagonal and
          NU above it.  If NL and NU are 1, then the matrix is
          tridiagonal and treated with specialized code.  In addition
          the matrix can be marked as probably a positive definite
          (Sparse matrices only)

    'singular'
          The matrix is assumed to be singular and will be treated with
          a minimum norm solution


     Note that the matrix type will be discovered automatically on the
     first attempt to solve a linear equation involving A.  Therefore
     `matrix_type' is only useful to give Octave hints of the matrix
     type.  Incorrectly defining the matrix type will result in
     incorrect results from solutions of linear equations, and so it is
     entirely the responsibility of the user to correctly identify the
     matrix type.

     Also the test for positive definiteness is a low-cost test for a
     hermitian matrix with a real positive diagonal.  This does not
     guarantee that the matrix is positive definite, but only that it
     is a probable candidate.  When such a matrix is factorized, a
     Cholesky factorization is first attempted, and if that fails the
     matrix is then treated with an LU factorization.  Once the matrix
     has been factorized, `matrix_type' will return the correct
     classification of the matrix.

 -- Function File:  norm (A, P, OPT)
     Compute the p-norm of the matrix A.  If the second argument is
     missing, `p = 2' is assumed.

     If A is a matrix (or sparse matrix):

    P = `1'
          1-norm, the largest column sum of the absolute values of A.

    P = `2'
          Largest singular value of A.

    P = `Inf' or `"inf"'
          Infinity norm, the largest row sum of the absolute values of
          A.

    P = `"fro"'
          Frobenius norm of A, `sqrt (sum (diag (A' * A)))'.

    other P, `P > 1'
          maximum `norm (A*x, p)' such that `norm (x, p) == 1'

     If A is a vector or a scalar:

    P = `Inf' or `"inf"'
          `max (abs (A))'.

    P = `-Inf'
          `min (abs (A))'.

    P = `"fro"'
          Frobenius norm of A, `sqrt (sumsq (abs (a)))'.

    P = 0
          Hamming norm - the number of nonzero elements.

    other P, `P > 1'
          p-norm of A, `(sum (abs (A) .^ P)) ^ (1/P)'.

    other P `P < 1'
          the p-pseudonorm defined as above.

     If `"rows"' is given as OPT, the norms of all rows of the matrix A
     are returned as a column vector.  Similarly, if `"columns"' or
     `"cols"' is passed column norms are computed.

     *See also:* *note cond: doc-cond, *note svd: doc-svd.

 -- Function File:  null (A, TOL)
     Return an orthonormal basis of the null space of A.

     The dimension of the null space is taken as the number of singular
     values of A not greater than TOL.  If the argument TOL is missing,
     it is computed as

          max (size (A)) * max (svd (A)) * eps

 -- Function File:  orth (A, TOL)
     Return an orthonormal basis of the range space of A.

     The dimension of the range space is taken as the number of singular
     values of A greater than TOL.  If the argument TOL is missing, it
     is computed as

          max (size (A)) * max (svd (A)) * eps

 -- Loadable Function:  pinv (X, TOL)
     Return the pseudoinverse of X.  Singular values less than TOL are
     ignored.

     If the second argument is omitted, it is assumed that

          tol = max (size (X)) * sigma_max (X) * eps,

     where `sigma_max (X)' is the maximal singular value of X.

 -- Function File:  rank (A, TOL)
     Compute the rank of A, using the singular value decomposition.
     The rank is taken to be the number of singular values of A that
     are greater than the specified tolerance TOL.  If the second
     argument is omitted, it is taken to be

          tol = max (size (A)) * sigma(1) * eps;

     where `eps' is machine precision and `sigma(1)' is the largest
     singular value of A.

 -- Loadable Function: C = rcond (A)
     Compute the 1-norm estimate of the reciprocal condition as returned
     by LAPACK.  If the matrix is well-conditioned then C will be near
     1 and if the matrix is poorly conditioned it will be close to zero.

     The matrix A must not be sparse.  If the matrix is sparse then
     `condest (A)' or `rcond (full (A))' should be used instead.

     *See also:* *note inv: doc-inv.

 -- Function File:  trace (A)
     Compute the trace of A, `sum (diag (A))'.

 -- Function File: [R, K] = rref (A, TOL)
     Returns the reduced row echelon form of A.  TOL defaults to `eps *
     max (size (A)) * norm (A, inf)'.

     Called with two return arguments, K returns the vector of "bound
     variables", which are those columns on which elimination has been
     performed.



File: octave.info,  Node: Matrix Factorizations,  Next: Functions of a Matrix,  Prev: Basic Matrix Functions,  Up: Linear Algebra

18.3 Matrix Factorizations
==========================

 -- Loadable Function: R = chol (A)
 -- Loadable Function: [R, P] = chol (A)
 -- Loadable Function: [R, P, Q] = chol (S)
 -- Loadable Function: [R, P, Q] = chol (S, 'vector')
 -- Loadable Function: [L, ...] = chol (..., 'lower')
     Compute the Cholesky factor, R, of the symmetric positive definite
     matrix A, where

          R' * R = A.

     Called with one output argument `chol' fails if A or S is not
     positive definite.  With two or more output arguments P flags
     whether the matrix was positive definite and `chol' does not fail.
     A zero value indicated that the matrix was positive definite and
     the R gives the factorization, and P will have a positive value
     otherwise.

     If called with 3 outputs then a sparsity preserving row/column
     permutation is applied to A prior to the factorization.  That is R
     is the factorization of `A(Q,Q)' such that

          R' * R = Q' * A * Q.

     The sparsity preserving permutation is generally returned as a
     matrix.  However, given the flag 'vector', Q will be returned as a
     vector such that

          R' * R = a (Q, Q).

     Called with either a sparse or full matrix and using the 'lower'
     flag, `chol' returns the lower triangular factorization such that

          L * L' = A.

     In general the lower triangular factorization is significantly
     faster for sparse matrices.

     *See also:* *note cholinv: doc-cholinv, *note chol2inv:
     doc-chol2inv.

 -- Loadable Function:  cholinv (A)
     Use the Cholesky factorization to compute the inverse of the
     symmetric positive definite matrix A.

     *See also:* *note chol: doc-chol, *note chol2inv: doc-chol2inv.

 -- Loadable Function:  chol2inv (U)
     Invert a symmetric, positive definite square matrix from its
     Cholesky decomposition, U.  Note that U should be an
     upper-triangular matrix with positive diagonal elements.
     `chol2inv (U)' provides `inv (U'*U)' but it is much faster than
     using `inv'.

     *See also:* *note chol: doc-chol, *note cholinv: doc-cholinv.

 -- Loadable Function: [R1, INFO] = cholupdate (R, U, OP)
     Update or downdate a Cholesky factorization.  Given an upper
     triangular matrix R and a column vector U, attempt to determine
     another upper triangular matrix R1 such that
        * R1'*R1 = R'*R + U*U' if OP is "+"

        * R1'*R1 = R'*R - U*U' if OP is "-"

     If OP is "-", INFO is set to
        * 0 if the downdate was successful,

        * 1 if R'*R - U*U' is not positive definite,

        * 2 if R is singular.

     If INFO is not present, an error message is printed in cases 1 and
     2.

     *See also:* *note chol: doc-chol, *note qrupdate: doc-qrupdate.

 -- Loadable Function: [R1, INFO] = cholinsert (R, J, U)
     Given a Cholesky factorization of a real symmetric or complex
     hermitian positive definite matrix A = R'*R, R upper triangular,
     return the Cholesky factorization of A1, where A1(p,p) = A,
     A1(:,j) = A1(j,:)' = u and p = [1:j-1,j+1:n+1].  u(j) should be
     positive.  On return, INFO is set to
        * 0 if the insertion was successful,

        * 1 if A1 is not positive definite,

        * 2 if R is singular.

     If INFO is not present, an error message is printed in cases 1 and
     2.

     *See also:* *note chol: doc-chol, *note cholupdate:
     doc-cholupdate, *note choldelete: doc-choldelete.

 -- Loadable Function: R1 = choldelete (R, J)
     Given a Cholesky factorization of a real symmetric or complex
     hermitian positive definite matrix A = R'*R, R upper triangular,
     return the Cholesky factorization of A(p,p), where
     p = [1:j-1,j+1:n+1].

     *See also:* *note chol: doc-chol, *note cholupdate:
     doc-cholupdate, *note cholinsert: doc-cholinsert.

 -- Loadable Function: R1 = cholshift (R, I, J)
     Given a Cholesky factorization of a real symmetric or complex
     hermitian positive definite matrix A = R'*R, R upper triangular,
     return the Cholesky factorization of A(p,p), where p is the
     permutation
     `p = [1:i-1, shift(i:j, 1), j+1:n]' if I < J
     or
     `p = [1:j-1, shift(j:i,-1), i+1:n]' if J < I.
     *See also:* *note chol: doc-chol, *note cholinsert:
     doc-cholinsert, *note choldelete: doc-choldelete.

 -- Loadable Function: H = hess (A)
 -- Loadable Function: [P, H] = hess (A)
     Compute the Hessenberg decomposition of the matrix A.

     The Hessenberg decomposition is usually used as the first step in
     an eigenvalue computation, but has other applications as well (see
     Golub, Nash, and Van Loan, IEEE Transactions on Automatic Control,
     1979).  The Hessenberg decomposition is `p * h * p' = a' where `p'
     is a square unitary matrix (`p' * p = I', using complex-conjugate
     transposition) and `h' is upper Hessenberg (`i >= j+1 => h (i, j)
     = 0').

 -- Loadable Function: [L, U, P] = lu (A)
 -- Loadable Function: [L, U, P, Q] = lu (S)
 -- Loadable Function: [L, U, P, Q, R] = lu (S)
 -- Loadable Function: [...] = lu (S, THRES)
 -- Loadable Function: Y = lu (...)
 -- Loadable Function: [...] = lu (..., 'vector')
     Compute the LU decomposition of A.  If A is full subroutines from
     LAPACK are used and if A is sparse then UMFPACK is used.  The
     result is returned in a permuted form, according to the optional
     return value P.  For example, given the matrix `a = [1, 2; 3, 4]',

          [l, u, p] = lu (a)

     returns

          l =

            1.00000  0.00000
            0.33333  1.00000

          u =

            3.00000  4.00000
            0.00000  0.66667

          p =

            0  1
            1  0

     The matrix is not required to be square.

     Called with two or three output arguments and a spare input matrix,
     then "lu" does not attempt to perform sparsity preserving column
     permutations.  Called with a fourth output argument, the sparsity
     preserving column transformation Q is returned, such that `P * A *
     Q = L * U'.

     Called with a fifth output argument and a sparse input matrix, then
     "lu" attempts to use a scaling factor R on the input matrix such
     that `P * (R \ A) * Q = L * U'.  This typically leads to a sparser
     and more stable factorization.

     An additional input argument THRES, that defines the pivoting
     threshold can be given.  THRES can be a scalar, in which case it
     defines UMFPACK pivoting tolerance for both symmetric and
     unsymmetric cases.  If THRES is a two element vector, then the
     first element defines the pivoting tolerance for the unsymmetric
     UMFPACK pivoting strategy and the second the symmetric strategy.
     By default, the values defined by `spparms' are used and are by
     default `[0.1, 0.001]'.

     Given the string argument 'vector', "lu" returns the values of P Q
     as vector values, such that for full matrix, `A (P,:) = L * U',
     and `R(P,:) * A (:, Q) = L * U'.

     With two output arguments, returns the permuted forms of the upper
     and lower triangular matrices, such that `A = L * U'.  With one
     output argument Y, then the matrix returned by the LAPACK routines
     is returned.  If the input matrix is sparse then the matrix L is
     embedded into U to give a return value similar to the full case.
     For both full and sparse matrices, "lu" looses the permutation
     information.

 -- Loadable Function: [Q, R, P] = qr (A)
 -- Loadable Function: [Q, R, P] = qr (A, '0')
     Compute the QR factorization of A, using standard LAPACK
     subroutines.  For example, given the matrix `a = [1, 2; 3, 4]',

          [q, r] = qr (a)

     returns

          q =

            -0.31623  -0.94868
            -0.94868   0.31623

          r =

            -3.16228  -4.42719
             0.00000  -0.63246

     The `qr' factorization has applications in the solution of least
     squares problems

          `min norm(A x - b)'

     for overdetermined systems of equations (i.e., `a'  is a tall,
     thin matrix).  The QR factorization is `q * r = a' where `q' is an
     orthogonal matrix and `r' is upper triangular.

     If given a second argument of '0', `qr' returns an economy-sized
     QR factorization, omitting zero rows of R and the corresponding
     columns of Q.

     If the matrix A is full, the permuted QR factorization `[Q, R, P]
     = qr (A)' forms the QR factorization such that the diagonal
     entries of `r' are decreasing in magnitude order.  For
     example,given the matrix `a = [1, 2; 3, 4]',

          [q, r, p] = qr(a)

     returns

          q =

            -0.44721  -0.89443
            -0.89443   0.44721

          r =

            -4.47214  -3.13050
             0.00000   0.44721

          p =

             0  1
             1  0

     The permuted `qr' factorization `[q, r, p] = qr (a)' factorization
     allows the construction of an orthogonal basis of `span (a)'.

     If the matrix A is sparse, then compute the sparse QR factorization
     of A, using CSPARSE.  As the matrix Q is in general a full matrix,
     this function returns the Q-less factorization R of A, such that
     `R = chol (A' * A)'.

     If the final argument is the scalar `0' and the number of rows is
     larger than the number of columns, then an economy factorization is
     returned.  That is R will have only `size (A,1)' rows.

     If an additional matrix B is supplied, then `qr' returns C, where
     `C = Q' * B'.  This allows the least squares approximation of `A \
     B' to be calculated as

          [C,R] = spqr (A,B)
          X = R \ C

 -- Loadable Function: [Q1, R1] = qrupdate (Q, R, U, V)
     Given a QR factorization of a real or complex matrix A = Q*R,
     Q unitary and R upper trapezoidal, return the QR factorization of
     A + U*V', where U and V are column vectors (rank-1 update) or
     matrices with equal number of columns (rank-k update).  Notice
     that the latter case is done as a sequence of rank-1 updates;
     thus, for k large enough, it will be both faster and more accurate
     to recompute the factorization from scratch.

     The QR factorization supplied may be either full (Q is square) or
     economized (R is square).

     *See also:* *note qr: doc-qr, *note qrinsert: doc-qrinsert, *note
     qrdelete: doc-qrdelete.

 -- Loadable Function: [Q1, R1] = qrinsert (Q, R, J, X, ORIENT)
     Given a QR factorization of a real or complex matrix A = Q*R,
     Q unitary and R upper trapezoidal, return the QR factorization of
     [A(:,1:j-1) x A(:,j:n)], where U is a column vector to be inserted
     into A (if ORIENT is `"col"'), or the QR factorization of
     [A(1:j-1,:);x;A(:,j:n)], where X is a row vector to be inserted
     into A (if ORIENT is `"row"').

     The default value of ORIENT is `"col"'.  If ORIENT is `"col"', U
     may be a matrix and J an index vector resulting in the
     QR factorization of a matrix B such that B(:,J) gives U and
     B(:,J) = [] gives A.  Notice that the latter case is done as a
     sequence of k insertions; thus, for k large enough, it will be
     both faster and more accurate to recompute the factorization from
     scratch.

     If ORIENT is `"col"', the QR factorization supplied may be either
     full (Q is square) or economized (R is square).

     If ORIENT is `"row"', full factorization is needed.

     *See also:* *note qr: doc-qr, *note qrupdate: doc-qrupdate, *note
     qrdelete: doc-qrdelete.

 -- Loadable Function: [Q1, R1] = qrdelete (Q, R, J, ORIENT)
     Given a QR factorization of a real or complex matrix A = Q*R,
     Q unitary and R upper trapezoidal, return the QR factorization of
     [A(:,1:j-1) A(:,j+1:n)], i.e., A with one column deleted (if
     ORIENT is "col"), or the QR factorization of
     [A(1:j-1,:);A(:,j+1:n)], i.e., A with one row deleted (if ORIENT
     is "row").

     The default value of ORIENT is "col".

     If ORIENT is `"col"', J may be an index vector resulting in the
     QR factorization of a matrix B such that A(:,J) = [] gives B.
     Notice that the latter case is done as a sequence of k deletions;
     thus, for k large enough, it will be both faster and more accurate
     to recompute the factorization from scratch.

     If ORIENT is `"col"', the QR factorization supplied may be either
     full (Q is square) or economized (R is square).

     If ORIENT is `"row"', full factorization is needed.

     *See also:* *note qr: doc-qr, *note qrinsert: doc-qrinsert, *note
     qrupdate: doc-qrupdate.

 -- Loadable Function: [Q1, R1] = qrshift (Q, R, I, J)
     Given a QR factorization of a real or complex matrix A = Q*R,
     Q unitary and R upper trapezoidal, return the QR factorization of
     A(:,p), where p is the permutation
     `p = [1:i-1, shift(i:j, 1), j+1:n]' if I < J
     or
     `p = [1:j-1, shift(j:i,-1), i+1:n]' if J < I.
     *See also:* *note qr: doc-qr, *note qrinsert: doc-qrinsert, *note
     qrdelete: doc-qrdelete.

 -- Loadable Function: LAMBDA = qz (A, B)
     Generalized eigenvalue problem A x = s B x, QZ decomposition.
     There are three ways to call this function:
       1. `lambda = qz(A,B)'

          Computes the generalized eigenvalues LAMBDA of (A - s B).

       2. `[AA, BB, Q, Z, V, W, lambda] = qz (A, B)'

          Computes qz decomposition, generalized eigenvectors, and
          generalized eigenvalues of (A - sB)

                   A*V = B*V*diag(lambda)
                   W'*A = diag(lambda)*W'*B
                   AA = Q'*A*Z, BB = Q'*B*Z
          with Q and Z orthogonal (unitary)= I

       3. `[AA,BB,Z{, lambda}] = qz(A,B,opt)'

          As in form [2], but allows ordering of generalized eigenpairs
          for (e.g.) solution of discrete time algebraic Riccati
          equations.  Form 3 is not available for complex matrices, and
          does not compute the generalized eigenvectors V, W, nor the
          orthogonal matrix Q.
         OPT
               for ordering eigenvalues of the GEP pencil.  The leading
               block of the revised pencil contains all eigenvalues
               that satisfy:
              `"N"'
                    = unordered (default)

              `"S"'
                    = small: leading block has all |lambda| <=1

              `"B"'
                    = big: leading block has all |lambda| >= 1

              `"-"'
                    = negative real part: leading block has all
                    eigenvalues in the open left half-plane

              `"+"'
                    = non-negative real part: leading block has all
                    eigenvalues in the closed right half-plane

     Note: qz performs permutation balancing, but not scaling (see
     balance).  Order of output arguments was selected for
     compatibility with MATLAB

     *See also:* *note balance: doc-balance, *note eig: doc-eig, *note
     schur: doc-schur.

 -- Function File: [AA, BB, Q, Z] = qzhess (A, B)
     Compute the Hessenberg-triangular decomposition of the matrix
     pencil `(A, B)', returning `AA = Q * A * Z', `BB = Q * B * Z',
     with Q and Z orthogonal.  For example,

          [aa, bb, q, z] = qzhess ([1, 2; 3, 4], [5, 6; 7, 8])
               => aa = [ -3.02244, -4.41741;  0.92998,  0.69749 ]
               => bb = [ -8.60233, -9.99730;  0.00000, -0.23250 ]
               =>  q = [ -0.58124, -0.81373; -0.81373,  0.58124 ]
               =>  z = [ 1, 0; 0, 1 ]

     The Hessenberg-triangular decomposition is the first step in Moler
     and Stewart's QZ decomposition algorithm.

     Algorithm taken from Golub and Van Loan, `Matrix Computations, 2nd
     edition'.

 -- Loadable Function: S = schur (A)
 -- Loadable Function: [U, S] = schur (A, OPT)
     The Schur decomposition is used to compute eigenvalues of a square
     matrix, and has applications in the solution of algebraic Riccati
     equations in control (see `are' and `dare').  `schur' always
     returns `s = u' * a * u' where `u'  is a unitary matrix (`u'* u'
     is identity) and `s' is upper triangular.  The eigenvalues of `a'
     (and `s') are the diagonal elements of `s'.  If the matrix `a' is
     real, then the real Schur decomposition is computed, in which the
     matrix `u' is orthogonal and `s' is block upper triangular with
     blocks of size at most `2 x 2' along the diagonal.  The diagonal
     elements of `s' (or the eigenvalues of the `2 x 2' blocks, when
     appropriate) are the eigenvalues of `a' and `s'.

     The eigenvalues are optionally ordered along the diagonal
     according to the value of `opt'.  `opt = "a"' indicates that all
     eigenvalues with negative real parts should be moved to the leading
     block of `s' (used in `are'), `opt = "d"' indicates that all
     eigenvalues with magnitude less than one should be moved to the
     leading block of `s' (used in `dare'), and `opt = "u"', the
     default, indicates that no ordering of eigenvalues should occur.
     The leading `k' columns of `u' always span the `a'-invariant
     subspace corresponding to the `k' leading eigenvalues of `s'.

 -- Function File: ANGLE = subspace (A, B)
     Determine the largest principal angle between two subspaces
     spanned by columns of matrices A and B.

 -- Loadable Function: S = svd (A)
 -- Loadable Function: [U, S, V] = svd (A)
     Compute the singular value decomposition of A

          A = U*S*V'

     The function `svd' normally returns the vector of singular values.
     If asked for three return values, it computes U, S, and V.  For
     example,

          svd (hilb (3))

     returns

          ans =

            1.4083189
            0.1223271
            0.0026873

     and

          [u, s, v] = svd (hilb (3))

     returns

          u =

            -0.82704   0.54745   0.12766
            -0.45986  -0.52829  -0.71375
            -0.32330  -0.64901   0.68867

          s =

            1.40832  0.00000  0.00000
            0.00000  0.12233  0.00000
            0.00000  0.00000  0.00269

          v =

            -0.82704   0.54745   0.12766
            -0.45986  -0.52829  -0.71375
            -0.32330  -0.64901   0.68867

     If given a second argument, `svd' returns an economy-sized
     decomposition, eliminating the unnecessary rows or columns of U or
     V.

 -- Function File: [HOUSV, BETA, ZER] = housh (X, J, Z)
     Compute Householder reflection vector HOUSV to reflect X to be the
     j-th column of identity, i.e.,

          (I - beta*housv*housv')x =  norm(x)*e(j) if x(1) < 0,
          (I - beta*housv*housv')x = -norm(x)*e(j) if x(1) >= 0

     Inputs

    X
          vector

    J
          index into vector

    Z
          threshold for zero  (usually should be the number 0)

     Outputs (see Golub and Van Loan):

    BETA
          If beta = 0, then no reflection need be applied (zer set to 0)

    HOUSV
          householder vector

 -- Function File: [U, H, NU] = krylov (A, V, K, EPS1, PFLG)
     Construct an orthogonal basis U of block Krylov subspace

          [v a*v a^2*v ... a^(k+1)*v]

     Using Householder reflections to guard against loss of
     orthogonality.

     If V is a vector, then H contains the Hessenberg matrix such that
     `a*u == u*h+rk*ek'', in which `rk = a*u(:,k)-u*h(:,k)', and `ek''
     is the vector `[0, 0, ..., 1]' of length `k'.  Otherwise, H is
     meaningless.

     If V is a vector and K is greater than `length(A)-1', then H
     contains the Hessenberg matrix such that `a*u == u*h'.

     The value of NU is the dimension of the span of the krylov
     subspace (based on EPS1).

     If B is a vector and K is greater than M-1, then H contains the
     Hessenberg decomposition of A.

     The optional parameter EPS1 is the threshold for zero.  The
     default value is 1e-12.

     If the optional parameter PFLG is nonzero, row pivoting is used to
     improve numerical behavior.  The default value is 0.

     Reference: Hodel and Misra, "Partial Pivoting in the Computation of
     Krylov Subspaces", to be submitted to Linear Algebra and its
     Applications


File: octave.info,  Node: Functions of a Matrix,  Next: Specialized Solvers,  Prev: Matrix Factorizations,  Up: Linear Algebra

18.4 Functions of a Matrix
==========================

 -- Function File:  expm (A)
     Return the exponential of a matrix, defined as the infinite Taylor
     series

          expm(a) = I + a + a^2/2! + a^3/3! + ...

     The Taylor series is _not_ the way to compute the matrix
     exponential; see Moler and Van Loan, `Nineteen Dubious Ways to
     Compute the Exponential of a Matrix', SIAM Review, 1978.  This
     routine uses Ward's diagonal Pade' approximation method with three
     step preconditioning (SIAM Journal on Numerical Analysis, 1977).
     Diagonal Pade'  approximations are rational polynomials of matrices

               -1
          D (a)   N (a)

     whose Taylor series matches the first `2q+1' terms of the Taylor
     series above; direct evaluation of the Taylor series (with the
     same preconditioning steps) may be desirable in lieu of the Pade'
     approximation when `Dq(a)' is ill-conditioned.

 -- Function File:  logm (A)
     Compute the matrix logarithm of the square matrix A.  Note that
     this is currently implemented in terms of an eigenvalue expansion
     and needs to be improved to be more robust.

 -- Loadable Function: [RESULT, ERROR_ESTIMATE] = sqrtm (A)
     Compute the matrix square root of the square matrix A.

     Ref: Nicholas J. Higham.  A new sqrtm for MATLAB.  Numerical
     Analysis Report No. 336, Manchester Centre for Computational
     Mathematics, Manchester, England, January 1999.

     *See also:* *note expm: doc-expm, *note logm: doc-logm.

 -- Loadable Function:  kron (A, B)
     Form the kronecker product of two matrices, defined block by block
     as

          x = [a(i, j) b]

     For example,

          kron (1:4, ones (3, 1))
                =>  1  2  3  4
                    1  2  3  4
                    1  2  3  4

 -- Loadable Function: X = syl (A, B, C)
     Solve the Sylvester equation

          A X + X B + C = 0
     using standard LAPACK subroutines.  For example,

          syl ([1, 2; 3, 4], [5, 6; 7, 8], [9, 10; 11, 12])
               => [ -0.50000, -0.66667; -0.66667, -0.50000 ]


File: octave.info,  Node: Specialized Solvers,  Prev: Functions of a Matrix,  Up: Linear Algebra

18.5 Specialized Solvers
========================

 -- Function File:  bicgstab (A, B)
 -- Function File:  bicgstab (A, B, TOL, MAXIT, M1, M2, X0)
     This procedure attempts to solve a system of linear equations A*x
     = b for x.  The A must be square, symmetric and positive definite
     real matrix N*N.  The B must be a one column vector with a length
     of N.  The TOL specifies the tolerance of the method, the default
     value is 1e-6.  The MAXIT specifies the maximum number of
     iterations, the default value is min(20,N).  The M1 specifies a
     preconditioner, can also be a function handler which returns M\X.
     The M2 combined with M1 defines preconditioner as
     preconditioner=M1*M2.  The X0 is the initial guess, the default
     value is zeros(N,1).

     The value X is a computed result of this procedure.  The value
     FLAG can be 0 when we reach tolerance in MAXIT iterations, 1 when
     we don't reach tolerance in MAXIT iterations and 3 when the
     procedure stagnates.  The value RELRES is a relative residual -
     norm(b-A*x)/norm(b).  The value ITER is an iteration number in
     which x was computed.  The value RESVEC is a vector of RELRES for
     each iteration.


 -- Function File:  cgs (A, B)
 -- Function File:  cgs (A, B, TOL, MAXIT, M1, M2, X0)
     This procedure attempts to solve a system of linear equations A*x
     = b for x.  The A must be square, symmetric and positive definite
     real matrix N*N.  The B must be a one column vector with a length
     of N.  The TOL specifies the tolerance of the method, default
     value is 1e-6.  The MAXIT specifies the maximum number of
     iteration, default value is MIN(20,N).  The M1 specifies a
     preconditioner, can also be a function handler which returns M\X.
     The M2 combined with M1 defines preconditioner as
     preconditioner=M1*M2.  The X0 is initial guess, default value is
     zeros(N,1).



File: octave.info,  Node: Nonlinear Equations,  Next: Diagonal and Permutation Matrices,  Prev: Linear Algebra,  Up: Top

19 Nonlinear Equations
**********************

Octave can solve sets of nonlinear equations of the form

     F (x) = 0

using the function `fsolve', which is based on the MINPACK subroutine
`hybrd'.  This is an iterative technique so a starting point will have
to be provided.  This also has the consequence that convergence is not
guaranteed even if a solution exists.

 -- Function File:  fsolve (FCN, X0, OPTIONS)
 -- Function File: [X, FVEC, INFO, OUTPUT, FJAC] = fsolve (FCN, ...)
     Solve a system of nonlinear equations defined by the function FCN.
     FCN should accepts a vector (array) defining the unknown variables,
     and return a vector of left-hand sides of the equations.
     Right-hand sides are defined to be zeros.  In other words, this
     function attempts to determine a vector X such that `FCN (X)'
     gives (approximately) all zeros.  X0 determines a starting guess.
     The shape of X0 is preserved in all calls to FCN, but otherwise it
     is treated as a column vector.  OPTIONS is a structure specifying
     additional options.  Currently, `fsolve' recognizes these options:
     `"FunValCheck"', `"OutputFcn"', `"TolX"', `"TolFun"', `"MaxIter"',
     `"MaxFunEvals"', `"Jacobian"', `"Updating"' and `"ComplexEqn"'.

     If `"Jacobian"' is `"on"', it specifies that FCN, called with 2
     output arguments, also returns the Jacobian matrix of right-hand
     sides at the requested point.  `"TolX"' specifies the termination
     tolerance in the unknown variables, while `"TolFun"' is a
     tolerance for equations.  Default is `1e-7' for both `"TolX"' and
     `"TolFun"'.  If `"Updating"' is "on", the function will attempt to
     use Broyden updates to update the Jacobian, in order to reduce the
     amount of jacobian calculations.  If your user function always
     calculates the Jacobian (regardless of number of output
     arguments), this option provides no advantage and should be set to
     false.

     `"ComplexEqn"' is `"on"', `fsolve' will attempt to solve complex
     equations in complex variables, assuming that the equations
     possess a complex derivative (i.e., are holomorphic).  If this is
     not what you want, should unpack the real and imaginary parts of
     the system to get a real system.

     For description of the other options, see `optimset'.

     On return, FVAL contains the value of the function FCN evaluated
     at X, and INFO may be one of the following values:

    1
          Converged to a solution point.  Relative residual error is
          less than specified by TolFun.

    2
          Last relative step size was less that TolX.

    3
          Last relative decrease in residual was less than TolF.

    0
          Iteration limit exceeded.

    -3
          The trust region radius became excessively small.

     Note: If you only have a single nonlinear equation of one
     variable, using `fzero' is usually a much better idea.

     *See also:* *note fzero: doc-fzero, *note optimset: doc-optimset.

     Note about user-supplied jacobians: As an inherent property of the
     algorithm, jacobian is always requested for a solution vector
     whose residual vector is already known, and it is the last
     accepted successful step.  Often this will be one of the last two
     calls, but not always.  If the savings by reusing intermediate
     results from residual calculation in jacobian calculation are
     significant, the best strategy is to employ OutputFcn: After a
     vector is evaluated for residuals, if OutputFcn is called with
     that vector, then the intermediate results should be saved for
     future jacobian evaluation, and should be kept until a jacobian
     evaluation is requested or until outputfcn is called with a
     different vector, in which case they should be dropped in favor of
     this most recent vector.  A short example how this can be achieved
     follows:

          function [fvec, fjac] = user_func (x, optimvalues, state)
          persistent sav = [], sav0 = [];
          if (nargin == 1)
            ## evaluation call
            if (nargout == 1)
              sav0.x = x; # mark saved vector
              ## calculate fvec, save results to sav0.
            elseif (nargout == 2)
              ## calculate fjac using sav.
            endif
          else
            ## outputfcn call.
            if (all (x == sav0.x))
              sav = sav0;
            endif
            ## maybe output iteration status, etc.
          endif
          endfunction

           ....

          fsolve (@user_func, x0, optimset ("OutputFcn", @user_func, ...))


   Here is a complete example.  To solve the set of equations

     -2x^2 + 3xy   + 4 sin(y) = 6
      3x^2 - 2xy^2 + 3 cos(x) = -4

you first need to write a function to compute the value of the given
function.  For example:

     function y = f (x)
       y(1) = -2*x(1)^2 + 3*x(1)*x(2)   + 4*sin(x(2)) - 6;
       y(2) =  3*x(1)^2 - 2*x(1)*x(2)^2 + 3*cos(x(1)) + 4;
     endfunction

   Then, call `fsolve' with a specified initial condition to find the
roots of the system of equations.  For example, given the function `f'
defined above,

     [x, fval, info] = fsolve (@f, [1; 2])

results in the solution

     x =

       0.57983
       2.54621

     fval =

       -5.7184e-10
        5.5460e-10

     info = 1

A value of `info = 1' indicates that the solution has converged.

   The function `perror' may be used to print English messages
corresponding to the numeric error codes.  For example,

     perror ("fsolve", 1)
          -| solution converged to requested tolerance

   When no Jacobian is supplied (as in the example above) it is
approximated numerically.  This requires more function evaluations, and
hence is less efficient.  In the example above we could compute the
Jacobian analytically as

     function J = jacobian(x)
       J(1,1) =  3*x(2) - 4*x(1);
       J(1,2) =  4*cos(x(2)) + 3*x(1);
       J(2,1) = -2*x(2)^2 - 3*sin(x(1)) + 6*x(1);
       J(2,2) = -4*x(1)*x(2);
     endfunction

The Jacobian can then be used with the following call to `fsolve':

     [x, fval, info] = fsolve ({@f, @jacobian}, [1; 2]);

which gives the same solution as before.

 -- Function File: [X, FVAL, INFO, OUTPUT] = fzero (FUN, X0, OPTIONS)
     Find a zero point of a univariate function.  FUN should be a
     function handle or name.  X0 specifies a starting point.  OPTIONS
     is a structure specifying additional options.  Currently, `fzero'
     recognizes these options: `"FunValCheck"', `"OutputFcn"',
     `"TolX"', `"MaxIter"', `"MaxFunEvals"'.  For description of these
     options, see *note optimset: doc-optimset.

     On exit, the function returns X, the approximate zero point and
     FVAL, the function value thereof.  INFO is an exit flag that can
     have these values:
        * 1 The algorithm converged to a solution.

        * 0 Maximum number of iterations or function evaluations has
          been exhausted.

        * -1 The algorithm has been terminated from user output
          function.

        * -2 A general unexpected error.

        * -3 A non-real value encountered.

        * -4 A NaN value encountered.

     *See also:* *note optimset: doc-optimset, *note fsolve: doc-fsolve.


File: octave.info,  Node: Diagonal and Permutation Matrices,  Next: Sparse Matrices,  Prev: Nonlinear Equations,  Up: Top

20 Diagonal and Permutation Matrices
************************************

* Menu:

* Basic Usage::          Creation and Manipulation of Diagonal and Permutation Matrices
* Matrix Algebra::       Linear Algebra with Diagonal and Permutation Matrices
* Function Support::     Functions That Are Aware of These Matrices
* Example Codes::        Some Examples of Usage
* Zeros Treatment::      The Differences in Treatment of Zero Elements


File: octave.info,  Node: Basic Usage,  Next: Matrix Algebra,  Up: Diagonal and Permutation Matrices

20.1 Creating and Manipulating Diagonal and Permutation Matrices
================================================================

A diagonal matrix is defined as a matrix that has zero entries outside
the main diagonal; that is, `D(i,j) == 0' if `i != j'.  Most often,
square diagonal matrices are considered; however, the definition can
equally be applied to nonsquare matrices, in which case we usually
speak of a rectangular diagonal matrix.

   A permutation matrix is defined as a square matrix that has a single
element equal to unity in each row and each column; all other elements
are zero.  That is, there exists a permutation (vector) `p' such that
`P(i,j) == 1' if `j == p(i)' and `P(i,j) == 0' otherwise.

   Octave provides special treatment of real and complex rectangular
diagonal matrices, as well as permutation matrices.  They are stored as
special objects, using efficient storage and algorithms, facilitating
writing both readable and efficient matrix algebra expressions in the
Octave language.

* Menu:

* Creating Diagonal Matrices::
* Creating Permutation Matrices::
* Explicit and Implicit Conversions::


File: octave.info,  Node: Creating Diagonal Matrices,  Next: Creating Permutation Matrices,  Up: Basic Usage

20.1.1 Creating Diagonal Matrices
---------------------------------

The most common and easiest way to create a diagonal matrix is using
the built-in function "diag".  The expression `diag (v)', with V a
vector, will create a square diagonal matrix with elements on the main
diagonal given by the elements of V, and size equal to the length of V.
`diag (v, m, n)' can be used to construct a rectangular diagonal matrix.
The result of these expressions will be a special diagonal matrix
object, rather than a general matrix object.

   Diagonal matrix with unit elements can be created using "eye".  Some
other built-in functions can also return diagonal matrices.  Examples
include "balance" or "inv".

   Example:
       diag (1:4)
     =>
     Diagonal Matrix

        1   0   0   0
        0   2   0   0
        0   0   3   0
        0   0   0   4

       diag(1:3,5,3)

     =>
     Diagonal Matrix

        1   0   0
        0   2   0
        0   0   3
        0   0   0
        0   0   0


File: octave.info,  Node: Creating Permutation Matrices,  Next: Explicit and Implicit Conversions,  Prev: Creating Diagonal Matrices,  Up: Basic Usage

20.1.2 Creating Permutation Matrices
------------------------------------

For creating permutation matrices, Octave does not introduce a new
function, but rather overrides an existing syntax: permutation matrices
can be conveniently created by indexing an identity matrix by
permutation vectors.  That is, if Q is a permutation vector of length
N, the expression
       P = eye (n) (:, q);
   will create a permutation matrix - a special matrix object.
     eye (n) (q, :)
   will also work (and create a row permutation matrix), as well as
     eye (n) (q1, q2).

   For example:
       eye (4) ([1,3,2,4],:)
     =>
     Permutation Matrix

        1   0   0   0
        0   0   1   0
        0   1   0   0
        0   0   0   1

       eye (4) (:,[1,3,2,4])
     =>
     Permutation Matrix

        1   0   0   0
        0   0   1   0
        0   1   0   0
        0   0   0   1

   Mathematically, an identity matrix is both diagonal and permutation
matrix.  In Octave, `eye (n)' returns a diagonal matrix, because a
matrix can only have one class.  You can convert this diagonal matrix
to a permutation matrix by indexing it by an identity permutation, as
shown below.  This is a special property of the identity matrix;
indexing other diagonal matrices generally produces a full matrix.

       eye (3)
     =>
     Diagonal Matrix

        1   0   0
        0   1   0
        0   0   1

       eye(3)(1:3,:)
     =>
     Permutation Matrix

        1   0   0
        0   1   0
        0   0   1

   Some other built-in functions can also return permutation matrices.
Examples include "inv" or "lu".


File: octave.info,  Node: Explicit and Implicit Conversions,  Prev: Creating Permutation Matrices,  Up: Basic Usage

20.1.3 Explicit and Implicit Conversions
----------------------------------------

The diagonal and permutation matrices are special objects in their own
right.  A number of operations and built-in functions are defined for
these matrices to use special, more efficient code than would be used
for a full matrix in the same place.  Examples are given in further
sections.

   To facilitate smooth mixing with full matrices, backward
compatibility, and compatibility with MATLAB, the diagonal and
permutation matrices should allow any operation that works on full
matrices, and will either treat it specially, or implicitly convert
themselves to full matrices.

   Instances include matrix indexing, except for extracting a single
element or a leading submatrix, indexed assignment, or applying most
mapper functions, such as "exp".

   An explicit conversion to a full matrix can be requested using the
built-in function "full".  It should also be noted that the diagonal
and permutation matrix objects will cache the result of the conversion
after it is first requested (explicitly or implicitly), so that
subsequent conversions will be very cheap.


File: octave.info,  Node: Matrix Algebra,  Next: Function Support,  Prev: Basic Usage,  Up: Diagonal and Permutation Matrices

20.2 Linear Algebra with Diagonal and Permutation Matrices
==========================================================

As has been already said, diagonal and permutation matrices make it
possible to use efficient algorithms while preserving natural linear
algebra syntax.  This section describes in detail the operations that
are treated specially when performed on these special matrix objects.

* Menu:

* Expressions Involving Diagonal Matrices::
* Expressions Involving Permutation Matrices::


File: octave.info,  Node: Expressions Involving Diagonal Matrices,  Next: Expressions Involving Permutation Matrices,  Up: Matrix Algebra

20.2.1 Expressions Involving Diagonal Matrices
----------------------------------------------

Assume D is a diagonal matrix.  If M is a full matrix, then `D*M' will
scale the rows of M.  That means, if `S = D*M', then for each pair of
indices i,j it holds
     S(i,j) = D(i,i) * M(i,j).
   Similarly, `M*D' will do a column scaling.

   The matrix D may also be rectangular, m-by-n where `m != n'.  If `m
< n', then the expression `D*M' is equivalent to
     D(:,1:m) * M(1:m,:),
   i.e., trailing `n-m' rows of M are ignored.  If `m > n', then `D*M'
is equivalent to
     [D(1:n,n) * M; zeros(m-n, columns (M))],
   i.e., null rows are appended to the result.  The situation for
right-multiplication `M*D' is analogous.

   The expressions `D \ M' and `M / D' perform inverse scaling.  They
are equivalent to solving a diagonal (or rectangular diagonal) in a
least-squares minimum-norm sense.  In exact arithmetics, this is
equivalent to multiplying by a pseudoinverse.  The pseudoinverse of a
rectangular diagonal matrix is again a rectangular diagonal matrix with
swapped dimensions, where each nonzero diagonal element is replaced by
its reciprocal.  The matrix division algorithms do, in fact, use
division rather than multiplication by reciprocals for better numerical
accuracy; otherwise, they honor the above definition.  Note that a
diagonal matrix is never truncated due to ill-conditioning; otherwise,
it would not be much useful for scaling.  This is typically consistent
with linear algebra needs.  A full matrix that only happens to be
diagonal (an is thus not a special object) is of course treated
normally.

   Multiplication and division by diagonal matrices works efficiently
also when combined with sparse matrices, i.e., `D*S', where D is a
diagonal matrix and S is a sparse matrix scales the rows of the sparse
matrix and returns a sparse matrix.  The expressions `S*D', `D\S',
`S/D' work analogically.

   If D1 and D2 are both diagonal matrices, then the expressions
     D1 + D2
     D1 - D2
     D1 * D2
     D1 / D2
     D1 \ D2
again produce diagonal matrices, provided that normal dimension
matching rules are obeyed.  The relations used are same as described
above.

   Also, a diagonal matrix D can be multiplied or divided by a scalar,
or raised to a scalar power if it is square, producing diagonal matrix
result in all cases.

   A diagonal matrix can also be transposed or conjugate-transposed,
giving the expected result.  Extracting a leading submatrix of a
diagonal matrix, i.e., `D(1:m,1:n)', will produce a diagonal matrix,
other indexing expressions will implicitly convert to full matrix.

   Adding a diagonal matrix to a full matrix only operates on the
diagonal elements.  Thus,
     A = A + eps * eye (n)
   is an efficient method of augmenting the diagonal of a matrix.
Subtraction works analogically.

   When involved in expressions with other element-by-element
operators, `.*', `./', `.\' or `.^', an implicit conversion to full
matrix will take place.  This is not always strictly necessary but
chosen to facilitate better consistency with MATLAB.


File: octave.info,  Node: Expressions Involving Permutation Matrices,  Prev: Expressions Involving Diagonal Matrices,  Up: Matrix Algebra

20.2.2 Expressions Involving Permutation Matrices
-------------------------------------------------

If P is a permutation matrix and M a matrix, the expression `P*M' will
permute the rows of M.  Similarly, `M*P' will yield a column
permutation.  Matrix division `P\M' and `M/P' can be used to do inverse
permutation.

   The previously described syntax for creating permutation matrices
can actually help an user to understand the connection between a
permutation matrix and a permuting vector.  Namely, the following
holds, where `I = eye (n)' is an identity matrix:
       I(p,:) * M = (I*M) (p,:) = M(p,:)
   Similarly,
       M * I(:,p) = (M*I) (:,p) = M(:,p)

   The expressions `I(p,:)' and `I(:,p)' are permutation matrices.

   A permutation matrix can be transposed (or conjugate-transposed,
which is the same, because a permutation matrix is never complex),
inverting the permutation, or equivalently, turning a row-permutation
matrix into a column-permutation one.  For permutation matrices,
transpose is equivalent to inversion, thus `P\M' is equivalent to
`P'*M'.  Transpose of a permutation matrix (or inverse) is a
constant-time operation, flipping only a flag internally, and thus the
choice between the two above equivalent expressions for inverse
permuting is completely up to the user's taste.

   Multiplication and division by permutation matrices works
efficiently also when combined with sparse matrices, i.e., `P*S', where
P is a permutation matrix and S is a sparse matrix permutes the rows of
the sparse matrix and returns a sparse matrix.  The expressions `S*P',
`P\S', `S/P' work analogically.

   Two permutation matrices can be multiplied or divided (if their
sizes match), performing a composition of permutations.  Also a
permutation matrix can be indexed by a permutation vector (or two
vectors), giving again a permutation matrix.  Any other operations do
not generally yield a permutation matrix and will thus trigger the
implicit conversion.


File: octave.info,  Node: Function Support,  Next: Example Codes,  Prev: Matrix Algebra,  Up: Diagonal and Permutation Matrices

20.3 Functions That Are Aware of These Matrices
===============================================

This section lists the built-in functions that are aware of diagonal and
permutation matrices on input, or can return them as output.  Passed to
other functions, these matrices will in general trigger an implicit
conversion.  (Of course, user-defined dynamically linked functions may
also work with diagonal or permutation matrices).

* Menu:

* Diagonal Matrix Functions::
* Permutation Matrix Functions::


File: octave.info,  Node: Diagonal Matrix Functions,  Next: Permutation Matrix Functions,  Up: Function Support

20.3.1 Diagonal Matrix Functions
--------------------------------

"inv" and "pinv" can be applied to a diagonal matrix, yielding again a
diagonal matrix.  "det" will use an efficient straightforward
calculation when given a diagonal matrix, as well as "cond".  The
following mapper functions can be applied to a diagonal matrix without
converting it to a full one: "abs", "real", "imag", "conj", "sqrt".  A
diagonal matrix can also be returned from the "balance" and "svd"
functions.  The "sparse" function will convert a diagonal matrix
efficiently to a sparse matrix.


File: octave.info,  Node: Permutation Matrix Functions,  Prev: Diagonal Matrix Functions,  Up: Function Support

20.3.2 Permutation Matrix Functions
-----------------------------------

"inv" and "pinv" will invert a permutation matrix, preserving its
specialness.  "det" can be applied to a permutation matrix, efficiently
calculating the sign of the permutation (which is equal to the
determinant).

   A permutation matrix can also be returned from the built-in functions
"lu" and "qr", if a pivoted factorization is requested.

   The "sparse" function will convert a permutation matrix efficiently
to a sparse matrix.  The "find" function will also work efficiently
with a permutation matrix, making it possible to conveniently obtain
the permutation indices.


File: octave.info,  Node: Example Codes,  Next: Zeros Treatment,  Prev: Function Support,  Up: Diagonal and Permutation Matrices

20.4 Some Examples of Usage
===========================

The following can be used to solve a linear system `A*x = b' using the
pivoted LU factorization:
       [L, U, P] = lu (A); ## now L*U = P*A
       x = U \ L \ P*b;

This is how you normalize columns of a matrix X to unit norm:
       s = norm (X, "columns");
       X = diag (s) \ X;

The following expression is a way to efficiently calculate the sign of a
permutation, given by a permutation vector P.  It will also work in
earlier versions of Octave, but slowly.
       det (eye (length (p))(p, :))

Finally, here's how you solve a linear system `A*x = b' with Tikhonov
regularization (ridge regression) using SVD (a skeleton only):
       m = rows (A); n = columns (A);
       [U, S, V] = svd (A);
       ## determine the regularization factor alpha
       ## alpha = ...
       ## transform to orthogonal basis
       b = U'*b;
       ## Use the standard formula, replacing A with S.
       ## S is diagonal, so the following will be very fast and accurate.
       x = (S'*S + alpha^2 * eye (n)) \ (S' * b);
       ## transform to solution basis
       x = V*x;


File: octave.info,  Node: Zeros Treatment,  Prev: Example Codes,  Up: Diagonal and Permutation Matrices

20.5 The Differences in Treatment of Zero Elements
==================================================

Making diagonal and permutation matrices special matrix objects in
their own right and the consequent usage of smarter algorithms for
certain operations implies, as a side effect, small differences in
treating zeros.  The contents of this section applies also to sparse
matrices, discussed in the following chapter.

   The IEEE standard defines the result of the expressions `0*Inf' and
`0*NaN' as `NaN', as it has been generally agreed that this is the best
compromise.  Numerical software dealing with structured and sparse
matrices (including Octave) however, almost always makes a distinction
between a "numerical zero" and an "assumed zero".  A "numerical zero"
is a zero value occurring in a place where any floating-point value
could occur.  It is normally stored somewhere in memory as an explicit
value.  An "assumed zero", on the contrary, is a zero matrix element
implied by the matrix structure (diagonal, triangular) or a sparsity
pattern; its value is usually not stored explicitly anywhere, but is
implied by the underlying data structure.

   The primary distinction is that an assumed zero, when multiplied by
any number, or divided by any nonzero number, yields *always* a zero,
even when, e.g., multiplied by `Inf' or divided by `NaN'.  The reason
for this behavior is that the numerical multiplication is not actually
performed anywhere by the underlying algorithm; the result is just
assumed to be zero.  Equivalently, one can say that the part of the
computation involving assumed zeros is performed symbolically, not
numerically.

   This behavior not only facilitates the most straightforward and
efficient implementation of algorithms, but also preserves certain
useful invariants, like:
   * scalar * diagonal matrix is a diagonal matrix

   * sparse matrix / scalar preserves the sparsity pattern

   * permutation matrix * matrix is equivalent to permuting rows
   all of these natural mathematical truths would be invalidated by
treating assumed zeros as numerical ones.

   Note that certain competing software does not strictly follow this
principle and converts assumed zeros to numerical zeros in certain
cases, while not doing so in other cases.  As of today, there are no
intentions to mimic such behavior in Octave.

   Examples of effects of assumed zeros vs. numerical zeros:
     Inf * eye (3)
     =>
        Inf     0     0
          0   Inf     0
          0     0   Inf

     Inf * speye (3)
     =>
     Compressed Column Sparse (rows = 3, cols = 3, nnz = 3 [33%])

       (1, 1) -> Inf
       (2, 2) -> Inf
       (3, 3) -> Inf

     Inf * full (eye (3))
     =>
        Inf   NaN   NaN
        NaN   Inf   NaN
        NaN   NaN   Inf

     diag(1:3) * [NaN; 1; 1]
     =>
        NaN
          2
          3

     sparse(1:3,1:3,1:3) * [NaN; 1; 1]
     =>
        NaN
          2
          3
     [1,0,0;0,2,0;0,0,3] * [NaN; 1; 1]
     =>
        NaN
        NaN
        NaN


File: octave.info,  Node: Sparse Matrices,  Next: Numerical Integration,  Prev: Diagonal and Permutation Matrices,  Up: Top

21 Sparse Matrices
******************

* Menu:

* Basics::                      Creation and Manipulation of Sparse Matrices
* Sparse Linear Algebra::       Linear Algebra on Sparse Matrices
* Iterative Techniques::        Iterative Techniques
* Real Life Example::           Using Sparse Matrices


File: octave.info,  Node: Basics,  Next: Sparse Linear Algebra,  Up: Sparse Matrices

21.1 The Creation and Manipulation of Sparse Matrices
=====================================================

The size of mathematical problems that can be treated at any particular
time is generally limited by the available computing resources.  Both,
the speed of the computer and its available memory place limitation on
the problem size.

   There are many classes of mathematical problems which give rise to
matrices, where a large number of the elements are zero.  In this case
it makes sense to have a special matrix type to handle this class of
problems where only the non-zero elements of the matrix are stored.
Not only does this reduce the amount of memory to store the matrix, but
it also means that operations on this type of matrix can take advantage
of the a-priori knowledge of the positions of the non-zero elements to
accelerate their calculations.

   A matrix type that stores only the non-zero elements is generally
called sparse.  It is the purpose of this document to discuss the
basics of the storage and creation of sparse matrices and the
fundamental operations on them.

* Menu:

* Storage of Sparse Matrices::
* Creating Sparse Matrices::
* Information::
* Operators and Functions::


File: octave.info,  Node: Storage of Sparse Matrices,  Next: Creating Sparse Matrices,  Up: Basics

21.1.1 Storage of Sparse Matrices
---------------------------------

It is not strictly speaking necessary for the user to understand how
sparse matrices are stored.  However, such an understanding will help
to get an understanding of the size of sparse matrices.  Understanding
the storage technique is also necessary for those users wishing to
create their own oct-files.

   There are many different means of storing sparse matrix data.  What
all of the methods have in common is that they attempt to reduce the
complexity and storage given a-priori knowledge of the particular class
of problems that will be solved.  A good summary of the available
techniques for storing sparse matrix is given by Saad (1).  With full
matrices, knowledge of the point of an element of the matrix within the
matrix is implied by its position in the computers memory.  However,
this is not the case for sparse matrices, and so the positions of the
non-zero elements of the matrix must equally be stored.

   An obvious way to do this is by storing the elements of the matrix as
triplets, with two elements being their position in the array (rows and
column) and the third being the data itself.  This is conceptually easy
to grasp, but requires more storage than is strictly needed.

   The storage technique used within Octave is the compressed column
format.  In this format the position of each element in a row and the
data are stored as previously.  However, if we assume that all elements
in the same column are stored adjacent in the computers memory, then we
only need to store information on the number of non-zero elements in
each column, rather than their positions.  Thus assuming that the
matrix has more non-zero elements than there are columns in the matrix,
we win in terms of the amount of memory used.

   In fact, the column index contains one more element than the number
of columns, with the first element always being zero.  The advantage of
this is a simplification in the code, in that there is no special case
for the first or last columns.  A short example, demonstrating this in
C is.

       for (j = 0; j < nc; j++)
         for (i = cidx (j); i < cidx(j+1); i++)
            printf ("non-zero element (%i,%i) is %d\n",
     	   ridx(i), j, data(i));

   A clear understanding might be had by considering an example of how
the above applies to an example matrix.  Consider the matrix

         1   2   0  0
         0   0   0  3
         0   0   0  4

   The non-zero elements of this matrix are

        (1, 1)  => 1
        (1, 2)  => 2
        (2, 4)  => 3
        (3, 4)  => 4

   This will be stored as three vectors CIDX, RIDX and DATA,
representing the column indexing, row indexing and data respectively.
The contents of these three vectors for the above matrix will be

       CIDX = [0, 1, 2, 2, 4]
       RIDX = [0, 0, 1, 2]
       DATA = [1, 2, 3, 4]

   Note that this is the representation of these elements with the
first row and column assumed to start at zero, while in Octave itself
the row and column indexing starts at one.  Thus the number of elements
in the I-th column is given by `CIDX (I + 1) - CIDX (I)'.

   Although Octave uses a compressed column format, it should be noted
that compressed row formats are equally possible.  However, in the
context of mixed operations between mixed sparse and dense matrices, it
makes sense that the elements of the sparse matrices are in the same
order as the dense matrices.  Octave stores dense matrices in column
major ordering, and so sparse matrices are equally stored in this
manner.

   A further constraint on the sparse matrix storage used by Octave is
that all elements in the rows are stored in increasing order of their
row index, which makes certain operations faster.  However, it imposes
the need to sort the elements on the creation of sparse matrices.
Having disordered elements is potentially an advantage in that it makes
operations such as concatenating two sparse matrices together easier
and faster, however it adds complexity and speed problems elsewhere.

   ---------- Footnotes ----------

   (1) Youcef Saad "SPARSKIT: A basic toolkit for sparse matrix
computation", 1994,
`http://www-users.cs.umn.edu/~saad/software/SPARSKIT/paper.ps'


File: octave.info,  Node: Creating Sparse Matrices,  Next: Information,  Prev: Storage of Sparse Matrices,  Up: Basics

21.1.2 Creating Sparse Matrices
-------------------------------

There are several means to create sparse matrix.

Returned from a function
     There are many functions that directly return sparse matrices.
     These include "speye", "sprand", "diag", etc.

Constructed from matrices or vectors
     The function "sparse" allows a sparse matrix to be constructed from
     three vectors representing the row, column and data.
     Alternatively, the function "spconvert" uses a three column matrix
     format to allow easy importation of data from elsewhere.

Created and then filled
     The function "sparse" or "spalloc" can be used to create an empty
     matrix that is then filled by the user

From a user binary program
     The user can directly create the sparse matrix within an oct-file.

   There are several basic functions to return specific sparse
matrices.  For example the sparse identity matrix, is a matrix that is
often needed.  It therefore has its own function to create it as `speye
(N)' or `speye (R, C)', which creates an N-by-N or R-by-C sparse
identity matrix.

   Another typical sparse matrix that is often needed is a random
distribution of random elements.  The functions "sprand" and "sprandn"
perform this for uniform and normal random distributions of elements.
They have exactly the same calling convention, where `sprand (R, C, D)',
creates an R-by-C sparse matrix with a density of filled elements of D.

   Other functions of interest that directly create sparse matrices, are
"diag" or its generalization "spdiags", that can take the definition of
the diagonals of the matrix and create the sparse matrix that
corresponds to this.  For example

     s = diag (sparse(randn(1,n)), -1);

   creates a sparse (N+1)-by-(N+1) sparse matrix with a single diagonal
defined.

 -- Function File: [B, C] = spdiags (A)
 -- Function File: B = spdiags (A, C)
 -- Function File: B = spdiags (V, C, A)
 -- Function File: B = spdiags (V, C, M, N)
     A generalization of the function `diag'.  Called with a single
     input argument, the non-zero diagonals C of A are extracted.  With
     two arguments the diagonals to extract are given by the vector C.

     The other two forms of `spdiags' modify the input matrix by
     replacing the diagonals.  They use the columns of V to replace the
     columns represented by the vector C.  If the sparse matrix A is
     defined then the diagonals of this matrix are replaced.  Otherwise
     a matrix of M by N is created with the diagonals given by V.

     Negative values of C represent diagonals below the main diagonal,
     and positive values of C diagonals above the main diagonal.

     For example

          spdiags (reshape (1:12, 4, 3), [-1 0 1], 5, 4)
          =>    5 10  0  0
                1  6 11  0
                0  2  7 12
                0  0  3  8
                0  0  0  4


 -- Function File: Y = speye (M)
 -- Function File: Y = speye (M, N)
 -- Function File: Y = speye (SZ)
     Returns a sparse identity matrix.  This is significantly more
     efficient than `sparse (eye (M))' as the full matrix is not
     constructed.

     Called with a single argument a square matrix of size M by M is
     created.  Otherwise a matrix of M by N is created.  If called with
     a single vector argument, this argument is taken to be the size of
     the matrix to create.

 -- Function File: Y = spfun (F,X)
     Compute `f(X)' for the non-zero values of X.  This results in a
     sparse matrix with the same structure as X.  The function F can be
     passed as a string, a function handle or an inline function.

 -- Mapping Function:  spmax (X, Y, DIM)
 -- Mapping Function: [W, IW] = spmax (X)
     This function has been deprecated.  Use `max' instead.

 -- Mapping Function:  spmin (X, Y, DIM)
 -- Mapping Function: [W, IW] = spmin (X)
     This function has been deprecated.  Use `min' instead.

 -- Function File: Y = spones (X)
     Replace the non-zero entries of X with ones.  This creates a
     sparse matrix with the same structure as X.

 -- Function File:  sprand (M, N, D)
 -- Function File:  sprand (S)
     Generate a random sparse matrix.  The size of the matrix will be M
     by N, with a density of values given by D.  D should be between 0
     and 1. Values will be uniformly distributed between 0 and 1.

     Note: sometimes the actual density may be a bit smaller than D.
     This is unlikely to happen for large really sparse matrices.

     If called with a single matrix argument, a random sparse matrix is
     generated wherever the matrix S is non-zero.

     *See also:* *note sprandn: doc-sprandn.

 -- Function File:  sprandn (M, N, D)
 -- Function File:  sprandn (S)
     Generate a random sparse matrix.  The size of the matrix will be M
     by N, with a density of values given by D.  D should be between 0
     and 1. Values will be normally distributed with mean of zero and
     variance 1.

     Note: sometimes the actual density may be a bit smaller than D.
     This is unlikely to happen for large really sparse matrices.

     If called with a single matrix argument, a random sparse matrix is
     generated wherever the matrix S is non-zero.

     *See also:* *note sprand: doc-sprand.

 -- Function File:  sprandsym (N, D)
 -- Function File:  sprandsym (S)
     Generate a symmetric random sparse matrix.  The size of the matrix
     will be N by N, with a density of values given by D.  D should be
     between 0 and 1. Values will be normally distributed with mean of
     zero and variance 1.

     Note: sometimes the actual density may be a bit smaller than D.
     This is unlikely to happen for large really sparse matrices.

     If called with a single matrix argument, a random sparse matrix is
     generated wherever the matrix S is non-zero in its lower
     triangular part.

     *See also:* *note sprand: doc-sprand, *note sprandn: doc-sprandn.

   The recommended way for the user to create a sparse matrix, is to
create two vectors containing the row and column index of the data and
a third vector of the same size containing the data to be stored.  For
example

       ri = ci = d = [];
       for j = 1:c
         ri = [ri; randperm(r)(1:n)'];
         ci = [ci; j*ones(n,1)];
         d = [d; rand(n,1)];
       endfor
       s = sparse (ri, ci, d, r, c);

   creates an R-by-C sparse matrix with a random distribution of N (<R)
elements per column.  The elements of the vectors do not need to be
sorted in any particular order as Octave will sort them prior to
storing the data.  However, pre-sorting the data will make the creation
of the sparse matrix faster.

   The function "spconvert" takes a three or four column real matrix.
The first two columns represent the row and column index respectively
and the third and four columns, the real and imaginary parts of the
sparse matrix.  The matrix can contain zero elements and the elements
can be sorted in any order.  Adding zero elements is a convenient way
to define the size of the sparse matrix.  For example

     s = spconvert ([1 2 3 4; 1 3 4 4; 1 2 3 0]')
     => Compressed Column Sparse (rows=4, cols=4, nnz=3)
           (1 , 1) -> 1
           (2 , 3) -> 2
           (3 , 4) -> 3

   An example of creating and filling a matrix might be

     k = 5;
     nz = r * k;
     s = spalloc (r, c, nz)
     for j = 1:c
       idx = randperm (r);
       s (:, j) = [zeros(r - k, 1); ...
             rand(k, 1)] (idx);
     endfor

   It should be noted, that due to the way that the Octave assignment
functions are written that the assignment will reallocate the memory
used by the sparse matrix at each iteration of the above loop.
Therefore the "spalloc" function ignores the NZ argument and does not
preassign the memory for the matrix.  Therefore, it is vitally
important that code using to above structure should be vectorized as
much as possible to minimize the number of assignments and reduce the
number of memory allocations.

 -- Loadable Function: FM = full (SM)
     returns a full storage matrix from a sparse, diagonal, permutation
     matrix or a range.

     *See also:* *note sparse: doc-sparse.

 -- Function File: S = spalloc (R, C, NZ)
     Returns an empty sparse matrix of size R-by-C.  As Octave resizes
     sparse matrices at the first opportunity, so that no additional
     space is needed, the argument NZ is ignored.  This function is
     provided only for compatibility reasons.

     It should be noted that this means that code like

          k = 5;
          nz = r * k;
          s = spalloc (r, c, nz)
          for j = 1:c
            idx = randperm (r);
            s (:, j) = [zeros(r - k, 1); rand(k, 1)] (idx);
          endfor

     will reallocate memory at each step.  It is therefore vitally
     important that code like this is vectorized as much as possible.

     *See also:* *note sparse: doc-sparse, *note nzmax: doc-nzmax.

 -- Loadable Function: S = sparse (A)
 -- Loadable Function: S = sparse (I, J, SV, M, N, NZMAX)
 -- Loadable Function: S = sparse (I, J, SV)
 -- Loadable Function: S = sparse (I, J, S, M, N, "unique")
 -- Loadable Function: S = sparse (M, N)
     Create a sparse matrix from the full matrix or row, column, value
     triplets.  If A is a full matrix, convert it to a sparse matrix
     representation, removing all zero values in the process.

     Given the integer index vectors I and J, a 1-by-`nnz' vector of
     real of complex values SV, overall dimensions M and N of the
     sparse matrix.  The argument `nzmax' is ignored but accepted for
     compatibility with MATLAB.  If M or N are not specified their
     values are derived from the maximum index in the vectors I and J
     as given by `M = max (I)', `N = max (J)'.

     *Note*: if multiple values are specified with the same I, J
     indices, the corresponding values in S will be added.

     The following are all equivalent:

          s = sparse (i, j, s, m, n)
          s = sparse (i, j, s, m, n, "summation")
          s = sparse (i, j, s, m, n, "sum")

     Given the option "unique". if more than two values are specified
     for the same I, J indices, the last specified value will be used.

     `sparse(M, N)' is equivalent to `sparse ([], [], [], M, N, 0)'

     If any of SV, I or J are scalars, they are expanded to have a
     common size.

     *See also:* *note full: doc-full.

 -- Function File: X = spconvert (M)
     This function converts for a simple sparse matrix format easily
     produced by other programs into Octave's internal sparse format.
     The input X is either a 3 or 4 column real matrix, containing the
     row, column, real and imaginary parts of the elements of the
     sparse matrix.  An element with a zero real and imaginary part can
     be used to force a particular matrix size.

   The above problem of memory reallocation can be avoided in
oct-files.  However, the construction of a sparse matrix from an
oct-file is more complex than can be discussed here, and you are
referred to chapter *note Dynamically Linked Functions::, to have a
full description of the techniques involved.


File: octave.info,  Node: Information,  Next: Operators and Functions,  Prev: Creating Sparse Matrices,  Up: Basics

21.1.3 Finding out Information about Sparse Matrices
----------------------------------------------------

There are a number of functions that allow information concerning
sparse matrices to be obtained.  The most basic of these is "issparse"
that identifies whether a particular Octave object is in fact a sparse
matrix.

   Another very basic function is "nnz" that returns the number of
non-zero entries there are in a sparse matrix, while the function
"nzmax" returns the amount of storage allocated to the sparse matrix.
Note that Octave tends to crop unused memory at the first opportunity
for sparse objects.  There are some cases of user created sparse
objects where the value returned by "nzmax" will not be the same as
"nnz", but in general they will give the same result.  The function
"spstats" returns some basic statistics on the columns of a sparse
matrix including the number of elements, the mean and the variance of
each column.

 -- Loadable Function:  issparse (EXPR)
     Return 1 if the value of the expression EXPR is a sparse matrix.

 -- Built-in Function: SCALAR = nnz (A)
     Returns the number of non zero elements in A.

     *See also:* *note sparse: doc-sparse.

 -- Function File:  nonzeros (S)
     Returns a vector of the non-zero values of the sparse matrix S.

 -- Built-in Function: SCALAR = nzmax (SM)
     Return the amount of storage allocated to the sparse matrix SM.
     Note that Octave tends to crop unused memory at the first
     opportunity for sparse objects.  There are some cases of user
     created sparse objects where the value returned by "nzmax" will
     not be the same as "nnz", but in general they will give the same
     result.

     *See also:* *note sparse: doc-sparse, *note spalloc: doc-spalloc.

 -- Function File: [COUNT, MEAN, VAR] = spstats (S)
 -- Function File: [COUNT, MEAN, VAR] = spstats (S, J)
     Return the stats for the non-zero elements of the sparse matrix S.
     COUNT is the number of non-zeros in each column, MEAN is the mean
     of the non-zeros in each column, and VAR is the variance of the
     non-zeros in each column.

     Called with two input arguments, if S is the data and J is the bin
     number for the data, compute the stats for each bin.  In this
     case, bins can contain data values of zero, whereas with `spstats
     (S)' the zeros may disappear.

   When solving linear equations involving sparse matrices Octave
determines the means to solve the equation based on the type of the
matrix as discussed in *note Sparse Linear Algebra::.  Octave probes the
matrix type when the div (/) or ldiv (\) operator is first used with
the matrix and then caches the type.  However the "matrix_type"
function can be used to determine the type of the sparse matrix prior
to use of the div or ldiv operators.  For example

     a = tril (sprandn(1024, 1024, 0.02), -1) ...
         + speye(1024);
     matrix_type (a);
     ans = Lower

   show that Octave correctly determines the matrix type for lower
triangular matrices.  "matrix_type" can also be used to force the type
of a matrix to be a particular type.  For example

     a = matrix_type (tril (sprandn (1024, ...
        1024, 0.02), -1) + speye(1024), 'Lower');

   This allows the cost of determining the matrix type to be avoided.
However, incorrectly defining the matrix type will result in incorrect
results from solutions of linear equations, and so it is entirely the
responsibility of the user to correctly identify the matrix type

   There are several graphical means of finding out information about
sparse matrices.  The first is the "spy" command, which displays the
structure of the non-zero elements of the matrix.  *Note
fig:spmatrix::, for an example of the use of "spy".  More advanced
graphical information can be obtained with the "treeplot", "etreeplot"
and "gplot" commands.

 [image src="spmatrix.png" text="
            |  * *
            |  * * * *
            |    * *   * *
            |    *   *     * *
          5 -      *   *       * *
            |      *     *         * *
            |        *     *           * *
            |        *       *             *
            |          *       *
         10 -          *         *
            |            *         *
            |            *           *
            |              *           *
            |              *             *
         15 -                *             *
            |----------|---------|---------|
                       5        10        15" ]
Figure 21.1: Structure of simple sparse matrix.

   One use of sparse matrices is in graph theory, where the
interconnections between nodes are represented as an adjacency matrix.
That is, if the i-th node in a graph is connected to the j-th node.
Then the ij-th node (and in the case of undirected graphs the ji-th
node) of the sparse adjacency matrix is non-zero.  If each node is then
associated with a set of coordinates, then the "gplot" command can be
used to graphically display the interconnections between nodes.

   As a trivial example of the use of "gplot", consider the example

     A = sparse([2,6,1,3,2,4,3,5,4,6,1,5],
         [1,1,2,2,3,3,4,4,5,5,6,6],1,6,6);
     xy = [0,4,8,6,4,2;5,0,5,7,5,7]';
     gplot(A,xy)

   which creates an adjacency matrix `A' where node 1 is connected to
nodes 2 and 6, node 2 with nodes 1 and 3, etc.  The coordinates of the
nodes are given in the n-by-2 matrix `xy'.

   The dependencies between the nodes of a Cholesky factorization can be
calculated in linear time without explicitly needing to calculate the
Cholesky factorization by the `etree' command.  This command returns
the elimination tree of the matrix and can be displayed graphically by
the command `treeplot(etree(A))' if `A' is symmetric or
`treeplot(etree(A+A'))' otherwise.

 -- Function File:  spy (X)
 -- Function File:  spy (..., MARKERSIZE)
 -- Function File:  spy (..., LINE_SPEC)
     Plot the sparsity pattern of the sparse matrix X.  If the argument
     MARKERSIZE is given as an scalar value, it is used to determine the
     point size in the plot.  If the string LINE_SPEC is given it is
     passed to `plot' and determines the appearance of the plot.

     *See also:* *note plot: doc-plot.

 -- Loadable Function: P = etree (S)
 -- Loadable Function: P = etree (S, TYP)
 -- Loadable Function: [P, Q] = etree (S, TYP)
     Returns the elimination tree for the matrix S.  By default S is
     assumed to be symmetric and the symmetric elimination tree is
     returned.  The argument TYP controls whether a symmetric or column
     elimination tree is returned.  Valid values of TYP are 'sym' or
     'col', for symmetric or column elimination tree respectively

     Called with a second argument, "etree" also returns the postorder
     permutations on the tree.

 -- Function File:  etreeplot (TREE)
 -- Function File:  etreeplot (TREE, NODE_STYLE, EDGE_STYLE)
     Plot the elimination tree of the matrix S or `S+S''  if S in
     non-symmetric.  The optional parameters LINE_STYLE and EDGE_STYLE
     define the output style.

     *See also:* *note treeplot: doc-treeplot, *note gplot: doc-gplot.

 -- Function File:  gplot (A, XY)
 -- Function File:  gplot (A, XY, LINE_STYLE)
 -- Function File: [X, Y] = gplot (A, XY)
     Plot a graph defined by A and XY in the graph theory sense.  A is
     the adjacency matrix of the array to be plotted and XY is an
     N-by-2 matrix containing the coordinates of the nodes of the graph.

     The optional parameter LINE_STYLE defines the output style for the
     plot.  Called with no output arguments the graph is plotted
     directly.  Otherwise, return the coordinates of the plot in X and
     Y.

     *See also:* *note treeplot: doc-treeplot, *note etreeplot:
     doc-etreeplot, *note spy: doc-spy.

 -- Function File:  treeplot (TREE)
 -- Function File:  treeplot (TREE, LINE_STYLE, EDGE_STYLE)
     Produces a graph of tree or forest.  The first argument is vector
     of predecessors, optional parameters LINE_STYLE and EDGE_STYLE
     define the output style.  The complexity of the algorithm is O(n)
     in terms of is time and memory requirements.

     *See also:* *note etreeplot: doc-etreeplot, *note gplot: doc-gplot.

 -- Function File:  treelayout (TREE)
 -- Function File:  treelayout (TREE, PERMUTATION)
     treelayout lays out a tree or a forest.  The first argument TREE
     is a vector of predecessors, optional parameter PERMUTATION is an
     optional postorder permutation.  The complexity of the algorithm
     is O(n) in terms of time and memory requirements.

     *See also:* *note etreeplot: doc-etreeplot, *note gplot:
     doc-gplot, *note treeplot: doc-treeplot.


File: octave.info,  Node: Operators and Functions,  Prev: Information,  Up: Basics

21.1.4 Basic Operators and Functions on Sparse Matrices
-------------------------------------------------------

* Menu:

* Sparse Functions::
* Return Types of Operators and Functions::
* Mathematical Considerations::


File: octave.info,  Node: Sparse Functions,  Next: Return Types of Operators and Functions,  Up: Operators and Functions

21.1.4.1 Sparse Functions
.........................

An important consideration in the use of the sparse functions of Octave
is that many of the internal functions of Octave, such as "diag",
cannot accept sparse matrices as an input.  The sparse implementation
in Octave therefore uses the "dispatch" function to overload the normal
Octave functions with equivalent functions that work with sparse
matrices.  However, at any time the sparse matrix specific version of
the function can be used by explicitly calling its function name.

   The table below lists all of the sparse functions of Octave.  Note
that the names of the specific sparse forms of the functions are
typically the same as the general versions with a "sp" prefix.  In the
table below, and the rest of this article the specific sparse versions
of the functions are used.

Generate sparse matrices:
     "spalloc", "spdiags", "speye", "sprand",   "sprandn", "sprandsym"

Sparse matrix conversion:
     "full", "sparse", "spconvert"

Manipulate sparse matrices
     "issparse", "nnz", "nonzeros", "nzmax",   "spfun", "spones", "spy"

Graph Theory:
     "etree", "etreeplot", "gplot",   "treeplot"

Sparse matrix reordering:
     "amd", "ccolamd", "colamd", "colperm", "csymamd",   "dmperm",
     "symamd", "randperm", "symrcm"

Linear algebra:
     "condest", "eigs", "matrix_type", "normest", "sprank",
     "spaugment", "svds"

Iterative techniques:
     "luinc", "pcg", "pcr"

Miscellaneous:
     "spparms", "symbfact", "spstats"

   In addition all of the standard Octave mapper functions (i.e., basic
math functions that take a single argument) such as "abs", etc.  can
accept sparse matrices.  The reader is referred to the documentation
supplied with these functions within Octave itself for further details.


File: octave.info,  Node: Return Types of Operators and Functions,  Next: Mathematical Considerations,  Prev: Sparse Functions,  Up: Operators and Functions

21.1.4.2 The Return Types of Operators and Functions
....................................................

The two basic reasons to use sparse matrices are to reduce the memory
usage and to not have to do calculations on zero elements.  The two are
closely related in that the computation time on a sparse matrix operator
or function is roughly linear with the number of non-zero elements.

   Therefore, there is a certain density of non-zero elements of a
matrix where it no longer makes sense to store it as a sparse matrix,
but rather as a full matrix.  For this reason operators and functions
that have a high probability of returning a full matrix will always
return one.  For example adding a scalar constant to a sparse matrix
will almost always make it a full matrix, and so the example

     speye(3) + 0
     =>   1  0  0
       0  1  0
       0  0  1

   returns a full matrix as can be seen.

   Additionally, if `sparse_auto_mutate' is true, all sparse functions
test the amount of memory occupied by the sparse matrix to see if the
amount of storage used is larger than the amount used by the full
equivalent.  Therefore `speye (2) * 1' will return a full matrix as the
memory used is smaller for the full version than the sparse version.

   As all of the mixed operators and functions between full and sparse
matrices exist, in general this does not cause any problems.  However,
one area where it does cause a problem is where a sparse matrix is
promoted to a full matrix, where subsequent operations would resparsify
the matrix.  Such cases are rare, but can be artificially created, for
example `(fliplr(speye(3)) + speye(3)) - speye(3)' gives a full matrix
when it should give a sparse one.  In general, where such cases occur,
they impose only a small memory penalty.

   There is however one known case where this behavior of Octave's
sparse matrices will cause a problem.  That is in the handling of the
"diag" function.  Whether "diag" returns a sparse or full matrix
depending on the type of its input arguments.  So

      a = diag (sparse([1,2,3]), -1);

   should return a sparse matrix.  To ensure this actually happens, the
"sparse" function, and other functions based on it like "speye", always
returns a sparse matrix, even if the memory used will be larger than
its full representation.

 -- Built-in Function: VAL = sparse_auto_mutate ()
 -- Built-in Function: OLD_VAL = sparse_auto_mutate (NEW_VAL)
     Query or set the internal variable that controls whether Octave
     will automatically mutate sparse matrices to real matrices to save
     memory.  For example,

          s = speye(3);
          sparse_auto_mutate (false)
          s (:, 1) = 1;
          typeinfo (s)
          => sparse matrix
          sparse_auto_mutate (true)
          s (1, :) = 1;
          typeinfo (s)
          => matrix

   Note that the `sparse_auto_mutate' option is incompatible with
MATLAB, and so it is off by default.


File: octave.info,  Node: Mathematical Considerations,  Prev: Return Types of Operators and Functions,  Up: Operators and Functions

21.1.4.3 Mathematical Considerations
....................................

The attempt has been made to make sparse matrices behave in exactly the
same manner as there full counterparts.  However, there are certain
differences and especially differences with other products sparse
implementations.

   Firstly, the "./" and ".^" operators must be used with care.
Consider what the examples

       s = speye (4);
       a1 = s .^ 2;
       a2 = s .^ s;
       a3 = s .^ -2;
       a4 = s ./ 2;
       a5 = 2 ./ s;
       a6 = s ./ s;

   will give.  The first example of S raised to the power of 2 causes
no problems.  However S raised element-wise to itself involves a large
number of terms `0 .^ 0' which is 1. There `S .^ S' is a full matrix.

   Likewise `S .^ -2' involves terms like `0 .^ -2' which is infinity,
and so `S .^ -2' is equally a full matrix.

   For the "./" operator `S ./ 2' has no problems, but `2 ./ S'
involves a large number of infinity terms as well and is equally a full
matrix.  The case of `S ./ S' involves terms like `0 ./ 0' which is a
`NaN' and so this is equally a full matrix with the zero elements of S
filled with `NaN' values.

   The above behavior is consistent with full matrices, but is not
consistent with sparse implementations in other products.

   A particular problem of sparse matrices comes about due to the fact
that as the zeros are not stored, the sign-bit of these zeros is
equally not stored.  In certain cases the sign-bit of zero is
important.  For example

      a = 0 ./ [-1, 1; 1, -1];
      b = 1 ./ a
      => -Inf            Inf
          Inf           -Inf
      c = 1 ./ sparse (a)
      =>  Inf            Inf
          Inf            Inf

   To correct this behavior would mean that zero elements with a
negative sign-bit would need to be stored in the matrix to ensure that
their sign-bit was respected.  This is not done at this time, for
reasons of efficiency, and so the user is warned that calculations
where the sign-bit of zero is important must not be done using sparse
matrices.

   In general any function or operator used on a sparse matrix will
result in a sparse matrix with the same or a larger number of non-zero
elements than the original matrix.  This is particularly true for the
important case of sparse matrix factorizations.  The usual way to
address this is to reorder the matrix, such that its factorization is
sparser than the factorization of the original matrix.  That is the
factorization of `L * U = P * S * Q' has sparser terms `L' and `U' than
the equivalent factorization `L * U = S'.

   Several functions are available to reorder depending on the type of
the matrix to be factorized.  If the matrix is symmetric
positive-definite, then "symamd" or "csymamd" should be used.  Otherwise
"amd", "colamd" or "ccolamd" should be used.  For completeness the
reordering functions "colperm" and "randperm" are also available.

   *Note fig:simplematrix::, for an example of the structure of a simple
positive definite matrix.

 [image src="spmatrix.png" text="
            |  * *
            |  * * * *
            |    * *   * *
            |    *   *     * *
          5 -      *   *       * *
            |      *     *         * *
            |        *     *           * *
            |        *       *             *
            |          *       *
         10 -          *         *
            |            *         *
            |            *           *
            |              *           *
            |              *             *
         15 -                *             *
            |----------|---------|---------|
                       5        10        15" ]
Figure 21.2: Structure of simple sparse matrix.

   The standard Cholesky factorization of this matrix can be obtained
by the same command that would be used for a full matrix.  This can be
visualized with the command `r = chol(A); spy(r);'.  *Note
fig:simplechol::.  The original matrix had 43 non-zero terms, while
this Cholesky factorization has 71, with only half of the symmetric
matrix being stored.  This is a significant level of fill in, and
although not an issue for such a small test case, can represents a
large overhead in working with other sparse matrices.

   The appropriate sparsity preserving permutation of the original
matrix is given by "symamd" and the factorization using this reordering
can be visualized using the command `q = symamd(A); r = chol(A(q,q));
spy(r)'.  This gives 29 non-zero terms which is a significant
improvement.

   The Cholesky factorization itself can be used to determine the
appropriate sparsity preserving reordering of the matrix during the
factorization, In that case this might be obtained with three return
arguments as r`[r, p, q] = chol(A); spy(r)'.

 [image src="spchol.png" text="
            |  * *
            |    * * *
            |      * * * *
            |        * * * * *
          5 -          * * * * * *
            |            * * * * * * *
            |              * * * * * * * *
            |                * * * * * * * *
            |                  * * * * * * *
         10 -                    * * * * * *
            |                      * * * * *
            |                        * * * *
            |                          * * *
            |                            * *
         15 -                              *
            |----------|---------|---------|
                       5        10        15" ]
Figure 21.3: Structure of the un-permuted Cholesky factorization of the
above matrix.

 [image src="spcholperm.png" text="
            |  * *
            |    *       *
            |      *   *
            |        * *
          5 -          * *
            |            *                 *
            |              *   *
            |                * *
            |                  *       *
         10 -                    *   *
            |                      * *
            |                        * *
            |                          *   *
            |                            * *
         15 -                              *
            |----------|---------|---------|
                       5        10        15" ]
Figure 21.4: Structure of the permuted Cholesky factorization of the
above matrix.

   In the case of an asymmetric matrix, the appropriate sparsity
preserving permutation is "colamd" and the factorization using this
reordering can be visualized using the command `q = colamd(A); [l, u,
p] = lu(A(:,q)); spy(l+u)'.

   Finally, Octave implicitly reorders the matrix when using the div (/)
and ldiv (\) operators, and so no the user does not need to explicitly
reorder the matrix to maximize performance.

 -- Loadable Function: P = amd (S)
 -- Loadable Function: P = amd (S, OPTS)
     Returns the approximate minimum degree permutation of a matrix.
     This permutation such that the Cholesky factorization of `S (P,
     P)' tends to be sparser than the Cholesky factorization of S
     itself.  `amd' is typically faster than `symamd' but serves a
     similar purpose.

     The optional parameter OPTS is a structure that controls the
     behavior of `amd'.  The fields of these structure are

    opts.dense
          Determines what `amd' considers to be a dense row or column
          of the input matrix.  Rows or columns with more than `max(16,
          (dense * sqrt (N)' entries, where N is the order of the
          matrix S, are ignored by `amd' during the calculation of the
          permutation The value of dense must be a positive scalar and
          its default value is 10.0

    opts.aggressive
          If this value is a non zero scalar, then `amd' performs
          aggressive absorption.  The default is not to perform
          aggressive absorption.

     The author of the code itself is Timothy A. Davis
     (davis@cise.ufl.edu), University of Florida (see
     `http://www.cise.ufl.edu/research/sparse/amd').

     *See also:* *note symamd: doc-symamd, *note colamd: doc-colamd.

 -- Loadable Function: P = ccolamd (S)
 -- Loadable Function: P = ccolamd (S, KNOBS)
 -- Loadable Function: P = ccolamd (S, KNOBS, CMEMBER)
 -- Loadable Function: [P, STATS] = ccolamd (...)
     Constrained column approximate minimum degree permutation.  `P =
     ccolamd (S)' returns the column approximate minimum degree
     permutation vector for the sparse matrix S.  For a non-symmetric
     matrix S, `S (:, P)' tends to have sparser LU factors than S.
     `chol (S (:, P)' * S (:, P))' also tends to be sparser than `chol
     (S' * S)'.  `P = ccolamd (S, 1)' optimizes the ordering for `lu (S
     (:, P))'.  The ordering is followed by a column elimination tree
     post-ordering.

     KNOBS is an optional one- to five-element input vector, with a
     default value of `[0 10 10 1 0]' if not present or empty.  Entries
     not present are set to their defaults.

    `KNOBS(1)'
          if nonzero, the ordering is optimized for `lu (S (:, p))'.
          It will be a poor ordering for `chol (S (:, P)' * S (:, P))'.
          This is the most important knob for ccolamd.

    `KNOB(2)'
          if S is m-by-n, rows with more than `max (16, KNOBS (2) *
          sqrt (n))' entries are ignored.

    `KNOB(3)'
          columns with more than `max (16, KNOBS (3) * sqrt (min (M,
          N)))' entries are ignored and ordered last in the output
          permutation (subject to the cmember constraints).

    `KNOB(4)'
          if nonzero, aggressive absorption is performed.

    `KNOB(5)'
          if nonzero, statistics and knobs are printed.


     CMEMBER is an optional vector of length n.  It defines the
     constraints on the column ordering.  If `CMEMBER (j) = C', then
     column J is in constraint set C (C must be in the range 1 to N).
     In the output permutation P, all columns in set 1 appear first,
     followed by all columns in set 2, and so on.  `CMEMBER =
     ones(1,n)' if not present or empty.  `ccolamd (S, [], 1 : N)'
     returns `1 : N'

     `P = ccolamd (S)' is about the same as `P = colamd (S)'.  KNOBS
     and its default values differ.  `colamd' always does aggressive
     absorption, and it finds an ordering suitable for both `lu (S (:,
     P))' and `chol (S (:, P)' * S (:, P))'; it cannot optimize its
     ordering for `lu (S (:, P))' to the extent that `ccolamd (S, 1)'
     can.

     STATS is an optional 20-element output vector that provides data
     about the ordering and the validity of the input matrix S.
     Ordering statistics are in `STATS (1 : 3)'.  `STATS (1)' and
     `STATS (2)' are the number of dense or empty rows and columns
     ignored by CCOLAMD and `STATS (3)' is the number of garbage
     collections performed on the internal data structure used by
     CCOLAMD (roughly of size `2.2 * nnz (S) + 4 * M + 7 * N' integers).

     `STATS (4 : 7)' provide information if CCOLAMD was able to
     continue.  The matrix is OK if `STATS (4)' is zero, or 1 if
     invalid.  `STATS (5)' is the rightmost column index that is
     unsorted or contains duplicate entries, or zero if no such column
     exists.  `STATS (6)' is the last seen duplicate or out-of-order row
     index in the column index given by `STATS (5)', or zero if no such
     row index exists.  `STATS (7)' is the number of duplicate or
     out-of-order row indices.  `STATS (8 : 20)' is always zero in the
     current version of CCOLAMD (reserved for future use).

     The authors of the code itself are S. Larimore, T. Davis (Uni of
     Florida) and S. Rajamanickam in collaboration with J. Bilbert and
     E. Ng.  Supported by the National Science Foundation (DMS-9504974,
     DMS-9803599, CCR-0203270), and a grant from Sandia National Lab.
     See `http://www.cise.ufl.edu/research/sparse' for ccolamd,
     csymamd, amd, colamd, symamd, and other related orderings.

     *See also:* *note colamd: doc-colamd, *note csymamd: doc-csymamd.

 -- Loadable Function: P = colamd (S)
 -- Loadable Function: P = colamd (S, KNOBS)
 -- Loadable Function: [P, STATS] = colamd (S)
 -- Loadable Function: [P, STATS] = colamd (S, KNOBS)
     Column approximate minimum degree permutation.  `P = colamd (S)'
     returns the column approximate minimum degree permutation vector
     for the sparse matrix S.  For a non-symmetric matrix S, `S (:,P)'
     tends to have sparser LU factors than S.  The Cholesky
     factorization of `S (:,P)' * S (:,P)' also tends to be sparser
     than that of `S' * S'.

     KNOBS is an optional one- to three-element input vector.  If S is
     m-by-n, then rows with more than `max(16,KNOBS(1)*sqrt(n))' entries
     are ignored.  Columns with more than
     `max(16,knobs(2)*sqrt(min(m,n)))' entries are removed prior to
     ordering, and ordered last in the output permutation P.  Only
     completely dense rows or columns are removed if `KNOBS (1)' and
     `KNOBS (2)' are < 0, respectively.  If `KNOBS (3)' is nonzero,
     STATS and KNOBS are printed.  The default is `KNOBS = [10 10 0]'.
     Note that KNOBS differs from earlier versions of colamd

     STATS is an optional 20-element output vector that provides data
     about the ordering and the validity of the input matrix S.
     Ordering statistics are in `STATS (1:3)'.  `STATS (1)' and `STATS
     (2)' are the number of dense or empty rows and columns ignored by
     COLAMD and `STATS (3)' is the number of garbage collections
     performed on the internal data structure used by COLAMD (roughly
     of size `2.2 * nnz(S) + 4 * M + 7 * N' integers).

     Octave built-in functions are intended to generate valid sparse
     matrices, with no duplicate entries, with ascending row indices of
     the nonzeros in each column, with a non-negative number of entries
     in each column (!)  and so on.  If a matrix is invalid, then
     COLAMD may or may not be able to continue.  If there are duplicate
     entries (a row index appears two or more times in the same column)
     or if the row indices in a column are out of order, then COLAMD
     can correct these errors by ignoring the duplicate entries and
     sorting each column of its internal copy of the matrix S (the
     input matrix S is not repaired, however).  If a matrix is invalid
     in other ways then COLAMD cannot continue, an error message is
     printed, and no output arguments (P or STATS) are returned.
     COLAMD is thus a simple way to check a sparse matrix to see if it's
     valid.

     `STATS (4:7)' provide information if COLAMD was able to continue.
     The matrix is OK if `STATS (4)' is zero, or 1 if invalid.  `STATS
     (5)' is the rightmost column index that is unsorted or contains
     duplicate entries, or zero if no such column exists.  `STATS (6)'
     is the last seen duplicate or out-of-order row index in the column
     index given by `STATS (5)', or zero if no such row index exists.
     `STATS (7)' is the number of duplicate or out-of-order row
     indices.  `STATS (8:20)' is always zero in the current version of
     COLAMD (reserved for future use).

     The ordering is followed by a column elimination tree
     post-ordering.

     The authors of the code itself are Stefan I. Larimore and Timothy
     A.  Davis (davis@cise.ufl.edu), University of Florida.  The
     algorithm was developed in collaboration with John Gilbert, Xerox
     PARC, and Esmond Ng, Oak Ridge National Laboratory.  (see
     `http://www.cise.ufl.edu/research/sparse/colamd')

     *See also:* *note colperm: doc-colperm, *note symamd: doc-symamd.

 -- Function File: P = colperm (S)
     Returns the column permutations such that the columns of `S (:,
     P)' are ordered in terms of increase number of non-zero elements.
     If S is symmetric, then P is chosen such that `S (P, P)' orders
     the rows and columns with increasing number of non zeros elements.

 -- Loadable Function: P = csymamd (S)
 -- Loadable Function: P = csymamd (S, KNOBS)
 -- Loadable Function: P = csymamd (S, KNOBS, CMEMBER)
 -- Loadable Function: [P, STATS] = csymamd (...)
     For a symmetric positive definite matrix S, returns the permutation
     vector P such that `S(P,P)' tends to have a sparser Cholesky
     factor than S.  Sometimes `csymamd' works well for symmetric
     indefinite matrices too.  The matrix S is assumed to be symmetric;
     only the strictly lower triangular part is referenced.  S must be
     square.  The ordering is followed by an elimination tree
     post-ordering.

     KNOBS is an optional one- to three-element input vector, with a
     default value of `[10 1 0]' if present or empty.  Entries not
     present are set to their defaults.

    `KNOBS(1)'
          If S is n-by-n, then rows and columns with more than
          `max(16,KNOBS(1)*sqrt(n))' entries are ignored, and ordered
          last in the output permutation (subject to the cmember
          constraints).

    `KNOBS(2)'
          If nonzero, aggressive absorption is performed.

    `KNOBS(3)'
          If nonzero, statistics and knobs are printed.


     CMEMBER is an optional vector of length n. It defines the
     constraints on the ordering.  If `CMEMBER(j) = S', then row/column
     j is in constraint set C (C must be in the range 1 to n).  In the
     output permutation P, rows/columns in set 1 appear first, followed
     by all rows/columns in set 2, and so on.  `CMEMBER = ones(1,n)' if
     not present or empty.  `csymamd(S,[],1:n)' returns `1:n'.

     `P = csymamd(S)' is about the same as `P = symamd(S)'.  KNOBS and
     its default values differ.

     `STATS (4:7)' provide information if CCOLAMD was able to continue.
     The matrix is OK if `STATS (4)' is zero, or 1 if invalid.  `STATS
     (5)' is the rightmost column index that is unsorted or contains
     duplicate entries, or zero if no such column exists.  `STATS (6)'
     is the last seen duplicate or out-of-order row index in the column
     index given by `STATS (5)', or zero if no such row index exists.
     `STATS (7)' is the number of duplicate or out-of-order row
     indices.  `STATS (8:20)' is always zero in the current version of
     CCOLAMD (reserved for future use).

     The authors of the code itself are S. Larimore, T. Davis (Uni of
     Florida) and S. Rajamanickam in collaboration with J. Bilbert and
     E. Ng.  Supported by the National Science Foundation (DMS-9504974,
     DMS-9803599, CCR-0203270), and a grant from Sandia National Lab.
     See `http://www.cise.ufl.edu/research/sparse' for ccolamd,
     csymamd, amd, colamd, symamd, and other related orderings.

     *See also:* *note symamd: doc-symamd, *note ccolamd: doc-ccolamd.

 -- Loadable Function: P = dmperm (S)
 -- Loadable Function: [P, Q, R, S] = dmperm (S)
     Perform a Dulmage-Mendelsohn permutation on the sparse matrix S.
     With a single output argument "dmperm" performs the row
     permutations P such that `S (P,:)' has no zero elements on the
     diagonal.

     Called with two or more output arguments, returns the row and
     column permutations, such that `S (P, Q)' is in block triangular
     form.  The values of R and S define the boundaries of the blocks.
     If S is square then `R == S'.

     The method used is described in: A. Pothen & C.-J. Fan. Computing
     the block triangular form of a sparse matrix. ACM Trans. Math.
     Software, 16(4):303-324, 1990.

     *See also:* *note colamd: doc-colamd, *note ccolamd: doc-ccolamd.

 -- Loadable Function: P = symamd (S)
 -- Loadable Function: P = symamd (S, KNOBS)
 -- Loadable Function: [P, STATS] = symamd (S)
 -- Loadable Function: [P, STATS] = symamd (S, KNOBS)
     For a symmetric positive definite matrix S, returns the permutation
     vector p such that `S (P, P)' tends to have a sparser Cholesky
     factor than S.  Sometimes SYMAMD works well for symmetric
     indefinite matrices too.  The matrix S is assumed to be symmetric;
     only the strictly lower triangular part is referenced.  S must be
     square.

     KNOBS is an optional one- to two-element input vector.  If S is
     n-by-n, then rows and columns with more than
     `max(16,KNOBS(1)*sqrt(n))' entries are removed prior to ordering,
     and ordered last in the output permutation P.  No rows/columns are
     removed if `KNOBS(1) < 0'.  If `KNOBS (2)' is nonzero, `stats' and
     KNOBS are printed.  The default is `KNOBS = [10 0]'.  Note that
     KNOBS differs from earlier versions of symamd.

     STATS is an optional 20-element output vector that provides data
     about the ordering and the validity of the input matrix S.
     Ordering statistics are in `STATS (1:3)'.  `STATS (1) = STATS (2)'
     is the number of dense or empty rows and columns ignored by SYMAMD
     and `STATS (3)' is the number of garbage collections performed on
     the internal data structure used by SYMAMD (roughly of size `8.4 *
     nnz (tril (S, -1)) + 9 * N' integers).

     Octave built-in functions are intended to generate valid sparse
     matrices, with no duplicate entries, with ascending row indices of
     the nonzeros in each column, with a non-negative number of entries
     in each column (!)  and so on.  If a matrix is invalid, then
     SYMAMD may or may not be able to continue.  If there are duplicate
     entries (a row index appears two or more times in the same column)
     or if the row indices in a column are out of order, then SYMAMD
     can correct these errors by ignoring the duplicate entries and
     sorting each column of its internal copy of the matrix S (the
     input matrix S is not repaired, however).  If a matrix is invalid
     in other ways then SYMAMD cannot continue, an error message is
     printed, and no output arguments (P or STATS) are returned.
     SYMAMD is thus a simple way to check a sparse matrix to see if
     it's valid.

     `STATS (4:7)' provide information if SYMAMD was able to continue.
     The matrix is OK if `STATS (4)' is zero, or 1 if invalid.  `STATS
     (5)' is the rightmost column index that is unsorted or contains
     duplicate entries, or zero if no such column exists.  `STATS (6)'
     is the last seen duplicate or out-of-order row index in the column
     index given by `STATS (5)', or zero if no such row index exists.
     `STATS (7)' is the number of duplicate or out-of-order row
     indices.  `STATS (8:20)' is always zero in the current version of
     SYMAMD (reserved for future use).

     The ordering is followed by a column elimination tree
     post-ordering.

     The authors of the code itself are Stefan I. Larimore and Timothy
     A.  Davis (davis@cise.ufl.edu), University of Florida.  The
     algorithm was developed in collaboration with John Gilbert, Xerox
     PARC, and Esmond Ng, Oak Ridge National Laboratory.  (see
     `http://www.cise.ufl.edu/research/sparse/colamd')

     *See also:* *note colperm: doc-colperm, *note colamd: doc-colamd.

 -- Loadable Function: P = symrcm (S)
     Symmetric reverse Cuthill-McKee permutation of S.  Return a
     permutation vector P such that `S (P, P)' tends to have its
     diagonal elements closer to the diagonal than S.  This is a good
     preordering for LU or Cholesky factorization of matrices that come
     from 'long, skinny' problems.  It works for both symmetric and
     asymmetric S.

     The algorithm represents a heuristic approach to the NP-complete
     bandwidth minimization problem.  The implementation is based in the
     descriptions found in

     E. Cuthill, J. McKee: Reducing the Bandwidth of Sparse Symmetric
     Matrices. Proceedings of the 24th ACM National Conference, 157-172
     1969, Brandon Press, New Jersey.

     Alan George, Joseph W. H. Liu: Computer Solution of Large Sparse
     Positive Definite Systems, Prentice Hall Series in Computational
     Mathematics, ISBN 0-13-165274-5, 1981.

     *See also:* *note colperm: doc-colperm, *note colamd: doc-colamd,
     *note symamd: doc-symamd.


File: octave.info,  Node: Sparse Linear Algebra,  Next: Iterative Techniques,  Prev: Basics,  Up: Sparse Matrices

21.2 Linear Algebra on Sparse Matrices
======================================

Octave includes a polymorphic solver for sparse matrices, where the
exact solver used to factorize the matrix, depends on the properties of
the sparse matrix itself.  Generally, the cost of determining the
matrix type is small relative to the cost of factorizing the matrix
itself, but in any case the matrix type is cached once it is
calculated, so that it is not re-determined each time it is used in a
linear equation.

   The selection tree for how the linear equation is solve is

  1. If the matrix is diagonal, solve directly and goto 8

  2. If the matrix is a permuted diagonal, solve directly taking into
     account the permutations.  Goto 8

  3. If the matrix is square, banded and if the band density is less
     than that given by `spparms ("bandden")' continue, else goto 4.

       a. If the matrix is tridiagonal and the right-hand side is not
          sparse continue, else goto 3b.

            1. If the matrix is hermitian, with a positive real
               diagonal, attempt       Cholesky factorization using
               LAPACK xPTSV.

            2. If the above failed or the matrix is not hermitian with
               a positive       real diagonal use Gaussian elimination
               with pivoting using       LAPACK xGTSV, and goto 8.

       b. If the matrix is hermitian with a positive real diagonal,
          attempt       Cholesky factorization using LAPACK xPBTRF.

       c. if the above failed or the matrix is not hermitian with a
          positive       real diagonal use Gaussian elimination with
          pivoting using       LAPACK xGBTRF, and goto 8.

  4. If the matrix is upper or lower triangular perform a sparse forward
     or backward substitution, and goto 8

  5. If the matrix is a upper triangular matrix with column permutations
     or lower triangular matrix with row permutations, perform a sparse
     forward or backward substitution, and goto 8

  6. If the matrix is square, hermitian with a real positive diagonal,
     attempt sparse Cholesky factorization using CHOLMOD.

  7. If the sparse Cholesky factorization failed or the matrix is not
     hermitian with a real positive diagonal, and the matrix is square,
     factorize using UMFPACK.

  8. If the matrix is not square, or any of the previous solvers flags
     a singular or near singular matrix, find a minimum norm solution
     using CXSPARSE(1).

   The band density is defined as the number of non-zero values in the
matrix divided by the number of non-zero values in the matrix.  The
banded matrix solvers can be entirely disabled by using "spparms" to
set `bandden' to 1 (i.e., `spparms ("bandden", 1)').

   The QR solver factorizes the problem with a Dulmage-Mendelsohn, to
separate the problem into blocks that can be treated as over-determined,
multiple well determined blocks, and a final over-determined block.  For
matrices with blocks of strongly connected nodes this is a big win as
LU decomposition can be used for many blocks.  It also significantly
improves the chance of finding a solution to over-determined problems
rather than just returning a vector of "NaN"'s.

   All of the solvers above, can calculate an estimate of the condition
number.  This can be used to detect numerical stability problems in the
solution and force a minimum norm solution to be used.  However, for
narrow banded, triangular or diagonal matrices, the cost of calculating
the condition number is significant, and can in fact exceed the cost of
factoring the matrix.  Therefore the condition number is not calculated
in these cases, and Octave relies on simpler techniques to detect
singular matrices or the underlying LAPACK code in the case of banded
matrices.

   The user can force the type of the matrix with the `matrix_type'
function.  This overcomes the cost of discovering the type of the
matrix.  However, it should be noted that identifying the type of the
matrix incorrectly will lead to unpredictable results, and so
`matrix_type' should be used with care.

 -- Function File: [N, C] = normest (A, TOL)
     Estimate the 2-norm of the matrix A using a power series analysis.
     This is typically used for large matrices, where the cost of
     calculating the `norm (A)' is prohibitive and an approximation to
     the 2-norm is acceptable.

     TOL is the tolerance to which the 2-norm is calculated.  By default
     TOL is 1e-6.  C returns the number of iterations needed for
     `normest' to converge.

 -- Function File: [EST, V, W, ITER] = onenormest (A, T)
 -- Function File: [EST, V, W, ITER] = onenormest (APPLY, APPLY_T, N, T)
     Apply Higham and Tisseur's randomized block 1-norm estimator to
     matrix A using T test vectors.  If T exceeds 5, then only 5 test
     vectors are used.

     If the matrix is not explicit, e.g., when estimating the norm of
     `inv (A)' given an LU factorization, `onenormest' applies A and
     its conjugate transpose through a pair of functions APPLY and
     APPLY_T, respectively, to a dense matrix of size N by T.  The
     implicit version requires an explicit dimension N.

     Returns the norm estimate EST, two vectors V and W related by norm
     `(W, 1) = EST * norm (V, 1)', and the number of iterations ITER.
     The number of iterations is limited to 10 and is at least 2.

     References:
        * Nicholas J. Higham and Franoise Tisseur, "A Block Algorithm
          for Matrix 1-Norm Estimation, with an Application to 1-Norm
          Pseudospectra." SIMAX vol 21, no 4, pp 1185-1201.
          `http://dx.doi.org/10.1137/S0895479899356080'

        * Nicholas J. Higham and Franoise Tisseur, "A Block Algorithm
          for Matrix 1-Norm Estimation, with an Application to 1-Norm
          Pseudospectra." `http://citeseer.ist.psu.edu/223007.html'

     *See also:* *note condest: doc-condest, *note norm: doc-norm,
     *note cond: doc-cond.

 -- Function File: [EST, V] = condest (A, T)
 -- Function File: [EST, V] = condest (A, SOLVE, SOLVE_T, T)
 -- Function File: [EST, V] = condest (APPLY, APPLY_T, SOLVE, SOLVE_T,
          N, T)
     Estimate the 1-norm condition number of a matrix A using T test
     vectors using a randomized 1-norm estimator.  If T exceeds 5, then
     only 5 test vectors are used.

     If the matrix is not explicit, e.g., when estimating the condition
     number of A given an LU factorization, `condest' uses the
     following functions:

    APPLY
          `A*x' for a matrix `x' of size N by T.

    APPLY_T
          `A'*x' for a matrix `x' of size N by T.

    SOLVE
          `A \ b' for a matrix `b' of size N by T.

    SOLVE_T
          `A' \ b' for a matrix `b' of size N by T.

     The implicit version requires an explicit dimension N.

     `condest' uses a randomized algorithm to approximate the 1-norms.

     `condest' returns the 1-norm condition estimate EST and a vector V
     satisfying `norm (A*v, 1) == norm (A, 1) * norm (V, 1) / EST'.
     When EST is large, V is an approximate null vector.

     References:
        * Nicholas J. Higham and Franoise Tisseur, "A Block Algorithm
          for Matrix 1-Norm Estimation, with an Application to 1-Norm
          Pseudospectra." SIMAX vol 21, no 4, pp 1185-1201.
          `http://dx.doi.org/10.1137/S0895479899356080'

        * Nicholas J. Higham and Franoise Tisseur, "A Block Algorithm
          for Matrix 1-Norm Estimation, with an Application to 1-Norm
          Pseudospectra." `http://citeseer.ist.psu.edu/223007.html'

     *See also:* *note norm: doc-norm, *note cond: doc-cond, *note
     onenormest: doc-onenormest.

 -- Loadable Function:   spparms ()
 -- Loadable Function: VALS = spparms ()
 -- Loadable Function: [KEYS, VALS] = spparms ()
 -- Loadable Function: VAL = spparms (KEY)
 -- Loadable Function:   spparms (VALS)
 -- Loadable Function:   spparms ('defaults')
 -- Loadable Function:   spparms ('tight')
 -- Loadable Function:   spparms (KEY, VAL)
     Sets or displays the parameters used by the sparse solvers and
     factorization functions.  The first four calls above get
     information about the current settings, while the others change
     the current settings.  The parameters are stored as pairs of keys
     and values, where the values are all floats and the keys are one
     of the following strings:

    `spumoni'
          Printing level of debugging information of the solvers
          (default 0)

    `ths_rel'
          Included for compatibility.  Not used.  (default 1)

    `ths_abs'
          Included for compatibility.  Not used.  (default 1)

    `exact_d'
          Included for compatibility.  Not used.  (default 0)

    `supernd'
          Included for compatibility.  Not used.  (default 3)

    `rreduce'
          Included for compatibility.  Not used.  (default 3)

    `wh_frac'
          Included for compatibility.  Not used.  (default 0.5)

    `autommd'
          Flag whether the LU/QR and the '\' and '/' operators will
          automatically use the sparsity preserving mmd functions
          (default 1)

    `autoamd'
          Flag whether the LU and the '\' and '/' operators will
          automatically use the sparsity preserving amd functions
          (default 1)

    `piv_tol'
          The pivot tolerance of the UMFPACK solvers (default 0.1)

    `sym_tol'
          The pivot tolerance of the UMFPACK symmetric solvers (default
          0.001)

    `bandden'
          The density of non-zero elements in a banded matrix before it
          is treated by the LAPACK banded solvers (default 0.5)

    `umfpack'
          Flag whether the UMFPACK or mmd solvers are used for the LU,
          '\' and '/' operations (default 1)

     The value of individual keys can be set with `spparms (KEY, VAL)'.
     The default values can be restored with the special keyword
     'defaults'.  The special keyword 'tight' can be used to set the
     mmd solvers to attempt for a sparser solution at the potential
     cost of longer running time.

 -- Loadable Function: P = sprank (S)
     Calculates the structural rank of a sparse matrix S.  Note that
     only the structure of the matrix is used in this calculation based
     on a Dulmage-Mendelsohn permutation to block triangular form.  As
     such the numerical rank of the matrix S is bounded by `sprank (S)
     >= rank (S)'.  Ignoring floating point errors `sprank (S) == rank
     (S)'.

     *See also:* *note dmperm: doc-dmperm.

 -- Loadable Function: [COUNT, H, PARENT, POST, R] = symbfact (S, TYP,
          MODE)
     Performs a symbolic factorization analysis on the sparse matrix S.
     Where

    S
          S is a complex or real sparse matrix.

    TYP
          Is the type of the factorization and can be one of

         `sym'
               Factorize S.  This is the default.

         `col'
               Factorize `S' * S'.

         `row'
               Factorize `S * S''.

         `lo'
               Factorize `S''

    MODE
          The default is to return the Cholesky factorization for R,
          and if MODE is 'L', the conjugate transpose of the Cholesky
          factorization is returned.  The conjugate transpose version
          is faster and uses less memory, but returns the same values
          for COUNT, H, PARENT and POST outputs.

     The output variables are

    COUNT
          The row counts of the Cholesky factorization as determined by
          TYP.

    H
          The height of the elimination tree.

    PARENT
          The elimination tree itself.

    POST
          A sparse boolean matrix whose structure is that of the
          Cholesky factorization as determined by TYP.

   For non square matrices, the user can also utilize the `spaugment'
function to find a least squares solution to a linear equation.

 -- Function File: S = spaugment (A, C)
     Creates the augmented matrix of A.  This is given by

          [C * eye(M, M),A; A', zeros(N,
          N)]

     This is related to the least squares solution of `A \\ B', by

          S * [ R / C; x] = [B, zeros(N,
          columns(B)]

     where R is the residual error

          R = B - A * X

     As the matrix S is symmetric indefinite it can be factorized with
     `lu', and the minimum norm solution can therefore be found without
     the need for a `qr' factorization.  As the residual error will be
     `zeros (M, M)' for under determined problems, and example can be

          m = 11; n = 10; mn = max(m ,n);
          a = spdiags ([ones(mn,1), 10*ones(mn,1), -ones(mn,1)],
                       [-1, 0, 1], m, n);
          x0 = a \ ones (m,1);
          s = spaugment (a);
          [L, U, P, Q] = lu (s);
          x1 = Q * (U \ (L \ (P  * [ones(m,1); zeros(n,1)])));
          x1 = x1(end - n + 1 : end);

     To find the solution of an overdetermined problem needs an estimate
     of the residual error R and so it is more complex to formulate a
     minimum norm solution using the `spaugment' function.

     In general the left division operator is more stable and faster
     than using the `spaugment' function.

   Finally, the function `eigs' can be used to calculate a limited
number of eigenvalues and eigenvectors based on a selection criteria
and likewise for `svds' which calculates a limited number of singular
values and vectors.

 -- Loadable Function: D = eigs (A)
 -- Loadable Function: D = eigs (A, K)
 -- Loadable Function: D = eigs (A, K, SIGMA)
 -- Loadable Function: D = eigs (A, K, SIGMA,OPTS)
 -- Loadable Function: D = eigs (A, B)
 -- Loadable Function: D = eigs (A, B, K)
 -- Loadable Function: D = eigs (A, B, K, SIGMA)
 -- Loadable Function: D = eigs (A, B, K, SIGMA, OPTS)
 -- Loadable Function: D = eigs (AF, N)
 -- Loadable Function: D = eigs (AF, N, B)
 -- Loadable Function: D = eigs (AF, N, K)
 -- Loadable Function: D = eigs (AF, N, B, K)
 -- Loadable Function: D = eigs (AF, N, K, SIGMA)
 -- Loadable Function: D = eigs (AF, N, B, K, SIGMA)
 -- Loadable Function: D = eigs (AF, N, K, SIGMA, OPTS)
 -- Loadable Function: D = eigs (AF, N, B, K, SIGMA, OPTS)
 -- Loadable Function: [V, D] = eigs (A, ...)
 -- Loadable Function: [V, D] = eigs (AF, N, ...)
 -- Loadable Function: [V, D, FLAG] = eigs (A, ...)
 -- Loadable Function: [V, D, FLAG] = eigs (AF, N, ...)
     Calculate a limited number of eigenvalues and eigenvectors of A,
     based on a selection criteria.  The number eigenvalues and
     eigenvectors to calculate is given by K whose default value is 6.

     By default `eigs' solve the equation `A * v = lambda * v' , where
     `lambda' is a scalar representing one of the eigenvalues, and `v'
     is the corresponding eigenvector.  If given the positive definite
     matrix B then `eigs' solves the general eigenvalue equation `A * v
     = lambda * B * v' .

     The argument SIGMA determines which eigenvalues are returned.
     SIGMA can be either a scalar or a string.  When SIGMA is a scalar,
     the K eigenvalues closest to SIGMA are returned.  If SIGMA is a
     string, it must have one of the values

    'lm'
          Largest magnitude (default).

    'sm'
          Smallest magnitude.

    'la'
          Largest Algebraic (valid only for real symmetric problems).

    'sa'
          Smallest Algebraic (valid only for real symmetric problems).

    'be'
          Both ends, with one more from the high-end if K is odd (valid
          only for real symmetric problems).

    'lr'
          Largest real part (valid only for complex or unsymmetric
          problems).

    'sr'
          Smallest real part (valid only for complex or unsymmetric
          problems).

    'li'
          Largest imaginary part (valid only for complex or unsymmetric
          problems).

    'si'
          Smallest imaginary part (valid only for complex or
          unsymmetric problems).

     If OPTS is given, it is a structure defining some of the options
     that `eigs' should use.  The fields of the structure OPTS are

    `issym'
          If AF is given, then flags whether the function AF defines a
          symmetric problem.  It is ignored if A is given.  The default
          is false.

    `isreal'
          If AF is given, then flags whether the function AF defines a
          real problem.  It is ignored if A is given.  The default is
          true.

    `tol'
          Defines the required convergence tolerance, given as `tol *
          norm (A)'.  The default is `eps'.

    `maxit'
          The maximum number of iterations.  The default is 300.

    `p'
          The number of Lanzcos basis vectors to use.  More vectors
          will result in faster convergence, but a larger amount of
          memory.  The optimal value of 'p' is problem dependent and
          should be in the range K to N.  The default value is `2 * K'.

    `v0'
          The starting vector for the computation.  The default is to
          have ARPACK randomly generate a starting vector.

    `disp'
          The level of diagnostic printout.  If `disp' is 0 then there
          is no printout.  The default value is 1.

    `cholB'
          Flag if `chol (B)' is passed rather than B.  The default is
          false.

    `permB'
          The permutation vector of the Cholesky factorization of B if
          `cholB' is true.  That is `chol ( B (permB, permB))'.  The
          default is `1:N'.


     It is also possible to represent A by a function denoted AF.  AF
     must be followed by a scalar argument N defining the length of the
     vector argument accepted by AF.  AF can be passed either as an
     inline function, function handle or as a string.  In the case where
     AF is passed as a string, the name of the string defines the
     function to use.

     AF is a function of the form `function y = af (x), y = ...;
     endfunction', where the required return value of AF is determined
     by the value of SIGMA, and are

    `A * x'
          If SIGMA is not given or is a string other than 'sm'.

    `A \ x'
          If SIGMA is 'sm'.

    `(A - sigma * I) \ x'
          for standard eigenvalue problem, where `I' is the identity
          matrix of the same size as `A'.  If SIGMA is zero, this
          reduces the `A \ x'.

    `(A - sigma * B) \ x'
          for the general eigenvalue problem.

     The return arguments of `eigs' depends on the number of return
     arguments.  With a single return argument, a vector D of length K
     is returned, represent the K eigenvalues that have been found.
     With two return arguments, V is a N-by-K matrix whose columns are
     the K eigenvectors corresponding to the returned eigenvalues.  The
     eigenvalues themselves are then returned in D in the form of a
     N-by-K matrix, where the elements on the diagonal are the
     eigenvalues.

     Given a third return argument FLAG, `eigs' also returns the status
     of the convergence.  If FLAG is 0, then all eigenvalues have
     converged, otherwise not.

     This function is based on the ARPACK package, written by R Lehoucq,
     K Maschhoff, D Sorensen and C Yang.  For more information see
     `http://www.caam.rice.edu/software/ARPACK/'.


*See also:* *note eig: doc-eig, *note svds: doc-svds.

 -- Function File: S = svds (A)
 -- Function File: S = svds (A, K)
 -- Function File: S = svds (A, K, SIGMA)
 -- Function File: S = svds (A, K, SIGMA, OPTS)
 -- Function File: [U, S, V, FLAG] = svds (...)
     Find a few singular values of the matrix A.  The singular values
     are calculated using

          [M, N] = size(A)
          S = eigs([sparse(M, M), A; ...
                          A', sparse(N, N)])

     The eigenvalues returned by `eigs' correspond to the singular
     values of A.  The number of singular values to calculate is given
     by K, whose default value is 6.

     The argument SIGMA can be used to specify which singular values to
     find.  SIGMA can be either the string 'L', the default, in which
     case the largest singular values of A are found.  Otherwise SIGMA
     should be a real scalar, in which case the singular values closest
     to SIGMA are found.  Note that for relatively small values of
     SIGMA, there is the chance that the requested number of singular
     values are not returned.  In that case SIGMA should be increased.

     If OPTS is given, then it is a structure that defines options that
     `svds' will pass to EIGS.  The possible fields of this structure
     are therefore determined by `eigs'.  By default three fields of
     this structure are set by `svds'.

    `tol'
          The required convergence tolerance for the singular values.
          `eigs' is passed TOL divided by `sqrt(2)'.  The default value
          is 1e-10.

    `maxit'
          The maximum number of iterations.  The default is 300.

    `disp'
          The level of diagnostic printout.  If `disp' is 0 then there
          is no printout.  The default value is 0.

     If more than one output argument is given, then `svds' also
     calculates the left and right singular vectors of A.  FLAG is used
     to signal the convergence of `svds'.  If `svds' converges to the
     desired tolerance, then FLAG given by

          norm (A * V - U * S, 1) <= ...
                  TOL * norm (A, 1)

     will be zero.

*See also:* *note eigs: doc-eigs.

   ---------- Footnotes ----------

   (1) The CHOLMOD, UMFPACK and CXSPARSE packages were written by Tim
Davis and are available at http://www.cise.ufl.edu/research/sparse/


File: octave.info,  Node: Iterative Techniques,  Next: Real Life Example,  Prev: Sparse Linear Algebra,  Up: Sparse Matrices

21.3 Iterative Techniques applied to sparse matrices
====================================================

The left division `\' and right division `/' operators, discussed in
the previous section, use direct solvers to resolve a linear equation
of the form `X = A \ B' or `X = B / A'.  Octave equally includes a
number of functions to solve sparse linear equations using iterative
techniques.

 -- Function File: X = pcg (A, B, TOL, MAXIT, M1, M2, X0, ...)
 -- Function File: [X, FLAG, RELRES, ITER, RESVEC, EIGEST] = pcg (...)
     Solves the linear system of equations `A * X = B' by means of the
     Preconditioned Conjugate Gradient iterative method.  The input
     arguments are

        * A can be either a square (preferably sparse) matrix or a
          function handle, inline function or string containing the name
          of a function which computes `A * X'.  In principle A should
          be symmetric and positive definite; if `pcg' finds A to not
          be positive definite, you will get a warning message and the
          FLAG output parameter will be set.

        * B is the right hand side vector.

        * TOL is the required relative tolerance for the residual error,
          `B - A * X'.  The iteration stops if `norm (B - A * X) <= TOL
          * norm (B - A * X0)'.  If TOL is empty or is omitted, the
          function sets `TOL = 1e-6' by default.

        * MAXIT is the maximum allowable number of iterations; if `[]'
          is supplied for `maxit', or `pcg' has less arguments, a
          default value equal to 20 is used.

        * M = M1 * M2 is the (left) preconditioning matrix, so that the
          iteration is (theoretically) equivalent to solving by `pcg'
          `P * X = M \ B', with `P = M \ A'.  Note that a proper choice
          of the preconditioner may dramatically improve the overall
          performance of the method.  Instead of matrices M1 and M2,
          the user may pass two functions which return the results of
          applying the inverse of M1 and M2 to a vector (usually this
          is the preferred way of using the preconditioner).  If `[]'
          is supplied for M1, or M1 is omitted, no preconditioning is
          applied.  If M2 is omitted, M = M1 will be used as
          preconditioner.

        * X0 is the initial guess.  If X0 is empty or omitted, the
          function sets X0 to a zero vector by default.

     The arguments which follow X0 are treated as parameters, and
     passed in a proper way to any of the functions (A or M) which are
     passed to `pcg'.  See the examples below for further details.  The
     output arguments are

        * X is the computed approximation to the solution of `A * X =
          B'.

        * FLAG reports on the convergence.  `FLAG = 0' means the
          solution converged and the tolerance criterion given by TOL
          is satisfied.  `FLAG = 1' means that the MAXIT limit for the
          iteration count was reached.  `FLAG = 3' reports that the
          (preconditioned) matrix was found not positive definite.

        * RELRES is the ratio of the final residual to its initial
          value, measured in the Euclidean norm.

        * ITER is the actual number of iterations performed.

        * RESVEC describes the convergence history of the method.
          `RESVEC (i,1)' is the Euclidean norm of the residual, and
          `RESVEC (i,2)' is the preconditioned residual norm, after the
          (I-1)-th iteration, `I = 1, 2, ..., ITER+1'.  The
          preconditioned residual norm is defined as `norm (R) ^ 2 = R'
          * (M \ R)' where `R = B - A * X', see also the description of
          M.  If EIGEST is not required, only `RESVEC (:,1)' is
          returned.

        * EIGEST returns the estimate for the smallest `EIGEST (1)' and
          largest `EIGEST (2)' eigenvalues of the preconditioned matrix
          `P = M \ A'.  In particular, if no preconditioning is used,
          the estimates for the extreme eigenvalues of A are returned.
          `EIGEST (1)' is an overestimate and `EIGEST (2)' is an
          underestimate, so that `EIGEST (2) / EIGEST (1)' is a lower
          bound for `cond (P, 2)', which nevertheless in the limit
          should theoretically be equal to the actual value of the
          condition number.  The method which computes EIGEST works
          only for symmetric positive definite A and M, and the user is
          responsible for verifying this assumption.

     Let us consider a trivial problem with a diagonal matrix (we
     exploit the sparsity of A)

          	n = 10;
          	a = diag (sparse (1:n));
          	b = rand (n, 1);
               [l, u, p, q] = luinc (a, 1.e-3);

     EXAMPLE 1: Simplest use of `pcg'

            x = pcg(A,b)

     EXAMPLE 2: `pcg' with a function which computes `A * X'

            function y = apply_a (x)
              y = [1:N]'.*x;
            endfunction

            x = pcg ("apply_a", b)

     EXAMPLE 3: `pcg' with a preconditioner: L * U

          x = pcg (a, b, 1.e-6, 500, l*u);

     EXAMPLE 4: `pcg' with a preconditioner: L * U.  Faster than
     EXAMPLE 3 since lower and upper triangular matrices are easier to
     invert

          x = pcg (a, b, 1.e-6, 500, l, u);

     EXAMPLE 5: Preconditioned iteration, with full diagnostics.  The
     preconditioner (quite strange, because even the original matrix A
     is trivial) is defined as a function

            function y = apply_m (x)
              k = floor (length (x) - 2);
              y = x;
              y(1:k) = x(1:k)./[1:k]';
            endfunction

            [x, flag, relres, iter, resvec, eigest] = ...
                               pcg (a, b, [], [], "apply_m");
            semilogy (1:iter+1, resvec);

     EXAMPLE 6: Finally, a preconditioner which depends on a parameter
     K.

            function y = apply_M (x, varargin)
            K = varargin{1};
            y = x;
            y(1:K) = x(1:K)./[1:K]';
            endfunction

            [x, flag, relres, iter, resvec, eigest] = ...
                 pcg (A, b, [], [], "apply_m", [], [], 3)

     REFERENCES

     	[1] C.T.Kelley, 'Iterative methods for linear and nonlinear
     equations', 	SIAM, 1995 (the base PCG algorithm)

     	[2] Y.Saad, 'Iterative methods for sparse linear systems', PWS
     1996 	(condition number estimate from PCG) Revised version of
     this book is 	available online at
     http://www-users.cs.umn.edu/~saad/books.html

     *See also:* *note sparse: doc-sparse, *note pcr: doc-pcr.

 -- Function File: X = pcr (A, B, TOL, MAXIT, M, X0, ...)
 -- Function File: [X, FLAG, RELRES, ITER, RESVEC] = pcr (...)
     Solves the linear system of equations `A * X = B' by means of the
     Preconditioned Conjugate Residuals iterative method.  The input
     arguments are

        * A can be either a square (preferably sparse) matrix or a
          function handle, inline function or string containing the name
          of a function which computes `A * X'.  In principle A should
          be symmetric and non-singular; if `pcr' finds A to be
          numerically singular, you will get a warning message and the
          FLAG output parameter will be set.

        * B is the right hand side vector.

        * TOL is the required relative tolerance for the residual error,
          `B - A * X'.  The iteration stops if `norm (B - A * X) <= TOL
          * norm (B - A * X0)'.  If TOL is empty or is omitted, the
          function sets `TOL = 1e-6' by default.

        * MAXIT is the maximum allowable number of iterations; if `[]'
          is supplied for `maxit', or `pcr' has less arguments, a
          default value equal to 20 is used.

        * M is the (left) preconditioning matrix, so that the iteration
          is (theoretically) equivalent to solving by `pcr' `P * X = M
          \ B', with `P = M \ A'.  Note that a proper choice of the
          preconditioner may dramatically improve the overall
          performance of the method.  Instead of matrix M, the user may
          pass a function which returns the results of applying the
          inverse of M to a vector (usually this is the preferred way
          of using the preconditioner).  If `[]' is supplied for M, or
          M is omitted, no preconditioning is applied.

        * X0 is the initial guess.  If X0 is empty or omitted, the
          function sets X0 to a zero vector by default.

     The arguments which follow X0 are treated as parameters, and
     passed in a proper way to any of the functions (A or M) which are
     passed to `pcr'.  See the examples below for further details.  The
     output arguments are

        * X is the computed approximation to the solution of `A * X =
          B'.

        * FLAG reports on the convergence.  `FLAG = 0' means the
          solution converged and the tolerance criterion given by TOL
          is satisfied.  `FLAG = 1' means that the MAXIT limit for the
          iteration count was reached.  `FLAG = 3' reports t `pcr'
          breakdown, see [1] for details.

        * RELRES is the ratio of the final residual to its initial
          value, measured in the Euclidean norm.

        * ITER is the actual number of iterations performed.

        * RESVEC describes the convergence history of the method, so
          that `RESVEC (i)' contains the Euclidean norms of the
          residual after the (I-1)-th iteration, `I = 1,2, ..., ITER+1'.

     Let us consider a trivial problem with a diagonal matrix (we
     exploit the sparsity of A)

          	n = 10;
          	a = sparse (diag (1:n));
          	b = rand (N, 1);

     EXAMPLE 1: Simplest use of `pcr'

            x = pcr(A, b)

     EXAMPLE 2: `pcr' with a function which computes `A * X'.

            function y = apply_a (x)
              y = [1:10]'.*x;
            endfunction

            x = pcr ("apply_a", b)

     EXAMPLE 3:  Preconditioned iteration, with full diagnostics.  The
     preconditioner (quite strange, because even the original matrix A
     is trivial) is defined as a function

            function y = apply_m (x)
              k = floor (length(x)-2);
              y = x;
              y(1:k) = x(1:k)./[1:k]';
            endfunction

            [x, flag, relres, iter, resvec] = ...
                               pcr (a, b, [], [], "apply_m")
            semilogy([1:iter+1], resvec);

     EXAMPLE 4: Finally, a preconditioner which depends on a parameter
     K.

            function y = apply_m (x, varargin)
              k = varargin{1};
              y = x; y(1:k) = x(1:k)./[1:k]';
            endfunction

            [x, flag, relres, iter, resvec] = ...
                               pcr (a, b, [], [], "apply_m"', [], 3)

     REFERENCES

     	[1] W. Hackbusch, "Iterative Solution of Large Sparse Systems of
     	Equations", section 9.5.4; Springer, 1994

     *See also:* *note sparse: doc-sparse, *note pcg: doc-pcg.

   The speed with which an iterative solver converges to a solution can
be accelerated with the use of a pre-conditioning matrix M.  In this
case the linear equation `M^-1 * X = M^-1 * A \ B' is solved instead.
Typical pre-conditioning matrices are partial factorizations of the
original matrix.

 -- Loadable Function: [L, U, P, Q] = luinc (A, '0')
 -- Loadable Function: [L, U, P, Q] = luinc (A, DROPTOL)
 -- Loadable Function: [L, U, P, Q] = luinc (A, OPTS)
     Produce the incomplete LU factorization of the sparse matrix A.
     Two types of incomplete factorization are possible, and the type
     is determined by the second argument to "luinc".

     Called with a second argument of '0', the zero-level incomplete LU
     factorization is produced.  This creates a factorization of A
     where the position of the non-zero arguments correspond to the same
     positions as in the matrix A.

     Alternatively, the fill-in of the incomplete LU factorization can
     be controlled through the variable DROPTOL or the structure OPTS.
     The UMFPACK multifrontal factorization code by Tim A.  Davis is
     used for the incomplete LU factorization, (availability
     `http://www.cise.ufl.edu/research/sparse/umfpack/')

     DROPTOL determines the values below which the values in the LU
     factorization are dropped and replaced by zero.  It must be a
     positive scalar, and any values in the factorization whose
     absolute value are less than this value are dropped, expect if
     leaving them increase the sparsity of the matrix.  Setting DROPTOL
     to zero results in a complete LU factorization which is the
     default.

     OPTS is a structure containing one or more of the fields

    `droptol'
          The drop tolerance as above.  If OPTS only contains `droptol'
          then this is equivalent to using the variable DROPTOL.

    `milu'
          A logical variable flagging whether to use the modified
          incomplete LU factorization.  In the case that `milu' is
          true, the dropped values are subtracted from the diagonal of
          the matrix U of the factorization.  The default is `false'.

    `udiag'
          A logical variable that flags whether zero elements on the
          diagonal of U should be replaced with DROPTOL to attempt to
          avoid singular factors.  The default is `false'.

    `thresh'
          Defines the pivot threshold in the interval [0,1].  Values
          outside that range are ignored.

     All other fields in OPTS are ignored.  The outputs from "luinc"
     are the same as for "lu".

     Given the string argument 'vector', "luinc" returns the values of P
     Q as vector values.

     *See also:* *note sparse: doc-sparse, *note lu: doc-lu.


File: octave.info,  Node: Real Life Example,  Prev: Iterative Techniques,  Up: Sparse Matrices

21.4 Real Life Example of the use of Sparse Matrices
====================================================

A common application for sparse matrices is in the solution of Finite
Element Models.  Finite element models allow numerical solution of
partial differential equations that do not have closed form solutions,
typically because of the complex shape of the domain.

   In order to motivate this application, we consider the boundary value
Laplace equation.  This system can model scalar potential fields, such
as heat or electrical potential.  Given a medium Omega with boundary
dOmega . At all points on the dOmega the boundary conditions are known,
and we wish to calculate the potential in Omega . Boundary conditions
may specify the potential (Dirichlet boundary condition), its normal
derivative across the boundary (Neumann boundary condition), or a
weighted sum of the potential and its derivative (Cauchy boundary
condition).

   In a thermal model, we want to calculate the temperature in Omega
and know the boundary temperature (Dirichlet condition) or heat flux
(from which we can calculate the Neumann condition by dividing by the
thermal conductivity at the boundary).  Similarly, in an electrical
model, we want to calculate the voltage in Omega and know the boundary
voltage (Dirichlet) or current (Neumann condition after diving by the
electrical conductivity).  In an electrical model, it is common for
much of the boundary to be electrically isolated; this is a Neumann
boundary condition with the current equal to zero.

   The simplest finite element models will divide Omega into simplexes
(triangles in 2D, pyramids in 3D).

   The following example creates a simple rectangular 2D electrically
conductive medium with 10 V and 20 V imposed on opposite sides
(Dirichlet boundary conditions).  All other edges are electrically
isolated.

        node_y= [1;1.2;1.5;1.8;2]*ones(1,11);
        node_x= ones(5,1)*[1,1.05,1.1,1.2, ...
                  1.3,1.5,1.7,1.8,1.9,1.95,2];
        nodes= [node_x(:), node_y(:)];

        [h,w]= size(node_x);
        elems= [];
        for idx= 1:w-1
          widx= (idx-1)*h;
          elems= [elems; ...
            widx+[(1:h-1);(2:h);h+(1:h-1)]'; ...
            widx+[(2:h);h+(2:h);h+(1:h-1)]' ];
        endfor

        E= size(elems,1); # No. of simplices
        N= size(nodes,1); # No. of vertices
        D= size(elems,2); # dimensions+1

   This creates a N-by-2 matrix `nodes' and a E-by-3 matrix `elems'
with values, which define finite element triangles:

       nodes(1:7,:)'
         1.00 1.00 1.00 1.00 1.00 1.05 1.05 ...
         1.00 1.20 1.50 1.80 2.00 1.00 1.20 ...

       elems(1:7,:)'
         1    2    3    4    2    3    4 ...
         2    3    4    5    7    8    9 ...
         6    7    8    9    6    7    8 ...

   Using a first order FEM, we approximate the electrical conductivity
distribution in Omega as constant on each simplex (represented by the
vector `conductivity').  Based on the finite element geometry, we first
calculate a system (or stiffness) matrix for each simplex (represented
as 3-by-3 elements on the diagonal of the element-wise system matrix
`SE'.  Based on `SE' and a N-by-DE connectivity matrix `C',
representing the connections between simplices and vertices, the global
connectivity matrix `S' is calculated.

       # Element conductivity
       conductivity= [1*ones(1,16), ...
              2*ones(1,48), 1*ones(1,16)];

       # Connectivity matrix
       C = sparse ((1:D*E), reshape (elems', ...
              D*E, 1), 1, D*E, N);

       # Calculate system matrix
       Siidx = floor ([0:D*E-1]'/D) * D * ...
              ones(1,D) + ones(D*E,1)*(1:D) ;
       Sjidx = [1:D*E]'*ones(1,D);
       Sdata = zeros(D*E,D);
       dfact = factorial(D-1);
       for j=1:E
          a = inv([ones(D,1), ...
              nodes(elems(j,:), :)]);
          const = conductivity(j) * 2 / ...
              dfact / abs(det(a));
          Sdata(D*(j-1)+(1:D),:) = const * ...
              a(2:D,:)' * a(2:D,:);
       endfor
       # Element-wise system matrix
       SE= sparse(Siidx,Sjidx,Sdata);
       # Global system matrix
       S= C'* SE *C;

   The system matrix acts like the conductivity `S' in Ohm's law `S * V
= I'.  Based on the Dirichlet and Neumann boundary conditions, we are
able to solve for the voltages at each vertex `V'.

       # Dirichlet boundary conditions
       D_nodes=[1:5, 51:55];
       D_value=[10*ones(1,5), 20*ones(1,5)];

       V= zeros(N,1);
       V(D_nodes) = D_value;
       idx = 1:N; # vertices without Dirichlet
                  # boundary condns
       idx(D_nodes) = [];

       # Neumann boundary conditions.  Note that
       # N_value must be normalized by the
       # boundary length and element conductivity
       N_nodes=[];
       N_value=[];

       Q = zeros(N,1);
       Q(N_nodes) = N_value;

       V(idx) = S(idx,idx) \ ( Q(idx) - ...
                 S(idx,D_nodes) * V(D_nodes));

   Finally, in order to display the solution, we show each solved
voltage value in the z-axis for each simplex vertex.

       elemx = elems(:,[1,2,3,1])';
       xelems = reshape (nodes(elemx, 1), 4, E);
       yelems = reshape (nodes(elemx, 2), 4, E);
       velems = reshape (V(elemx), 4, E);
       plot3 (xelems,yelems,velems,'k');
       print ('grid.eps');


File: octave.info,  Node: Numerical Integration,  Next: Differential Equations,  Prev: Sparse Matrices,  Up: Top

22 Numerical Integration
************************

Octave comes with several built-in functions for computing the integral
of a function numerically.  These functions all solve 1-dimensional
integration problems.

* Menu:

* Functions of One Variable::
* Functions of Multiple Variables::
* Orthogonal Collocation::


File: octave.info,  Node: Functions of One Variable,  Next: Functions of Multiple Variables,  Up: Numerical Integration

22.1 Functions of One Variable
==============================

Octave supports three different algorithms for computing the integral
of a function f over the interval from a to b.  These are

`quad'
     Numerical integration based on Gaussian quadrature.

`quadl'
     Numerical integration using an adaptive Lobatto rule.

`quadgk'
     Numerical integration using an adaptive Gauss-Konrod rule.

`quadv'
     Numerical integration using an adaptive vectorized Simpson's rule.

`trapz'
     Numerical integration using the trapezoidal method.

Besides these functions Octave also allows you to perform cumulative
numerical integration using the trapezoidal method through the
`cumtrapz' function.

 -- Loadable Function: [V, IER, NFUN, ERR] = quad (F, A, B, TOL, SING)
     Integrate a nonlinear function of one variable using Quadpack.
     The first argument is the name of the function, the function
     handle or the inline function to call to compute the value of the
     integrand.  It must have the form

          y = f (x)

     where Y and X are scalars.

     The second and third arguments are limits of integration.  Either
     or both may be infinite.

     The optional argument TOL is a vector that specifies the desired
     accuracy of the result.  The first element of the vector is the
     desired absolute tolerance, and the second element is the desired
     relative tolerance.  To choose a relative test only, set the
     absolute tolerance to zero.  To choose an absolute test only, set
     the relative tolerance to zero.

     The optional argument SING is a vector of values at which the
     integrand is known to be singular.

     The result of the integration is returned in V and IER contains an
     integer error code (0 indicates a successful integration).  The
     value of NFUN indicates how many function evaluations were
     required, and ERR contains an estimate of the error in the
     solution.

     You can use the function `quad_options' to set optional parameters
     for `quad'.

     It should be noted that since `quad' is written in Fortran it
     cannot be called recursively.

 -- Loadable Function:  quad_options (OPT, VAL)
     When called with two arguments, this function allows you set
     options parameters for the function `quad'.  Given one argument,
     `quad_options' returns the value of the corresponding option.  If
     no arguments are supplied, the names of all the available options
     and their current values are displayed.

     Options include

    `"absolute tolerance"'
          Absolute tolerance; may be zero for pure relative error test.

    `"relative tolerance"'
          Nonnegative relative tolerance.  If the absolute tolerance is
          zero, the relative tolerance must be greater than or equal to
          `max (50*eps, 0.5e-28)'.

    `"single precision absolute tolerance"'
          Absolute tolerance for single precision; may be zero for pure
          relative error test.

    `"single precision relative tolerance"'
          Nonnegative relative tolerance for single precision.  If the
          absolute tolerance is zero, the relative tolerance must be
          greater than or equal to `max (50*eps, 0.5e-28)'.

   Here is an example of using `quad' to integrate the function

       F(X) = X * sin (1/X) * sqrt (abs (1 - X))

from X = 0 to X = 3.

   This is a fairly difficult integration (plot the function over the
range of integration to see why).

   The first step is to define the function:

     function y = f (x)
       y = x .* sin (1 ./ x) .* sqrt (abs (1 - x));
     endfunction

   Note the use of the `dot' forms of the operators.  This is not
necessary for the call to `quad', but it makes it much easier to
generate a set of points for plotting (because it makes it possible to
call the function with a vector argument to produce a vector result).

   Then we simply call quad:

     [v, ier, nfun, err] = quad ("f", 0, 3)
          => 1.9819
          => 1
          => 5061
          => 1.1522e-07

   Although `quad' returns a nonzero value for IER, the result is
reasonably accurate (to see why, examine what happens to the result if
you move the lower bound to 0.1, then 0.01, then 0.001, etc.).

 -- Function File: Q = quadl (F, A, B)
 -- Function File: Q = quadl (F, A, B, TOL)
 -- Function File: Q = quadl (F, A, B, TOL, TRACE)
 -- Function File: Q = quadl (F, A, B, TOL, TRACE, P1, P2, ...)
     Numerically evaluate integral using adaptive Lobatto rule.  `quadl
     (F, A, B)' approximates the integral of `F(X)' to machine
     precision.  F is either a function handle, inline function or
     string containing the name of the function to evaluate.  The
     function F must return a vector of output values if given a vector
     of input values.

     If defined, TOL defines the relative tolerance to which to which
     to integrate `F(X)'.  While if TRACE is defined, displays the left
     end point of the current interval, the interval length, and the
     partial integral.

     Additional arguments P1, etc., are passed directly to F.  To use
     default values for TOL and TRACE, one may pass empty matrices.

     Reference: W. Gander and W. Gautschi, 'Adaptive Quadrature -
     Revisited', BIT Vol. 40, No. 1, March 2000, pp. 84-101.
     `http://www.inf.ethz.ch/personal/gander/'


 -- Function File:  quadgk (F, A, B, ABSTOL, TRACE)
 -- Function File:  quadgk (F, A, B, PROP, VAL, ...)
 -- Function File: [Q, ERR] = quadgk (...)
     Numerically evaluate integral using adaptive Gauss-Konrod
     quadrature.  The formulation is based on a proposal by L.F.
     Shampine, `"Vectorized adaptive quadrature in MATLAB", Journal of
     Computational and Applied Mathematics, pp131-140, Vol 211, Issue 2,
     Feb 2008' where all function evaluations at an iteration are
     calculated with a single call to F.  Therefore the function F must
     be of the form `F (X)' and accept vector values of X and return a
     vector of the same length representing the function evaluations at
     the given values of X.  The function F can be defined in terms of
     a function handle, inline function or string.

     The bounds of the quadrature `[A, B]' can be finite or infinite
     and contain weak end singularities.  Variable transformation will
     be used to treat infinite intervals and weaken the singularities.
     For example

          quadgk(@(x) 1 ./ (sqrt (x) .* (x + 1)), 0, Inf)

     Note that the formulation of the integrand uses the
     element-by-element operator `./' and all user functions to
     `quadgk' should do the same.

     The absolute tolerance can be passed as a fourth argument in a
     manner compatible with `quadv'.  Equally the user can request that
     information on the convergence can be printed is the fifth argument
     is logically true.

     Alternatively, certain properties of `quadgk' can be passed as
     pairs `PROP, VAL'.  Valid properties are

    `AbsTol'
          Defines the absolute error tolerance for the quadrature.  The
          default absolute tolerance is 1e-10.

    `RelTol'
          Defines the relative error tolerance for the quadrature.  The
          default relative tolerance is 1e-5.

    `MaxIntervalCount'
          `quadgk' initially subdivides the interval on which to perform
          the quadrature into 10 intervals.  Sub-intervals that have an
          unacceptable error are sub-divided and re-evaluated.  If the
          number of sub-intervals exceeds at any point 650
          sub-intervals then a poor convergence is signaled and the
          current estimate of the integral is returned.  The property
          'MaxIntervalCount' can be used to alter the number of
          sub-intervals that can exist before exiting.

    `WayPoints'
          If there exists discontinuities in the first derivative of the
          function to integrate, then these can be flagged with the
          `"WayPoints"' property.  This forces the ends of a
          sub-interval to fall on the breakpoints of the function and
          can result in significantly improved estimated of the error
          in the integral, faster computation or both.  For example,

               quadgk (@(x) abs (1 - x .^ 2), 0, 2, 'Waypoints', 1)

          signals the breakpoint in the integrand at `X = 1'.

    `Trace'
          If logically true, then `quadgk' prints information on the
          convergence of the quadrature at each iteration.

     If any of A, B or WAYPOINTS is complex, then the quadrature is
     treated as a contour integral along a piecewise continuous path
     defined by the above.  In this case the integral is assumed to
     have no edge singularities.  For example

          quadgk (@(z) log (z), 1+1i, 1+1i, "WayPoints",
                  [1-1i, -1,-1i, -1+1i])

     integrates `log (z)' along the square defined by `[1+1i,  1-1i,
     -1-1i, -1+1i]'

     If two output arguments are requested, then ERR returns the
     approximate bounds on the error in the integral `abs (Q - I)',
     where I is the exact value of the integral.

     *See also:* *note triplequad: doc-triplequad, *note dblquad:
     doc-dblquad, *note quad: doc-quad, *note quadl: doc-quadl, *note
     quadv: doc-quadv, *note trapz: doc-trapz.

 -- Function File: Q = quadv (F, A, B)
 -- Function File: Q = quadl (F, A, B, TOL)
 -- Function File: Q = quadl (F, A, B, TOL, TRACE)
 -- Function File: Q = quadl (F, A, B, TOL, TRACE, P1, P2, ...)
 -- Function File: [Q, FCNT] = quadl (...)
     Numerically evaluate integral using adaptive Simpson's rule.
     `quadv (F, A, B)' approximates the integral of `F(X)' to the
     default absolute tolerance of `1e-6'.  F is either a function
     handle, inline function or string containing the name of the
     function to evaluate.  The function F must accept a string, and
     can return a vector representing the approximation to N different
     sub-functions.

     If defined, TOL defines the absolute tolerance to which to which
     to integrate each sub-interval of `F(X)'.  While if TRACE is
     defined, displays the left end point of the current interval, the
     interval length, and the partial integral.

     Additional arguments P1, etc., are passed directly to F.  To use
     default values for TOL and TRACE, one may pass empty matrices.

     *See also:* *note triplequad: doc-triplequad, *note dblquad:
     doc-dblquad, *note quad: doc-quad, *note quadl: doc-quadl, *note
     quadgk: doc-quadgk, *note trapz: doc-trapz.

 -- Function File: Z = trapz (Y)
 -- Function File: Z = trapz (X, Y)
 -- Function File: Z = trapz (..., DIM)
     Numerical integration using trapezoidal method.  `trapz (Y)'
     computes the integral of the Y along the first non-singleton
     dimension.  If the argument X is omitted a equally spaced vector
     is assumed.  `trapz (X, Y)' evaluates the integral with respect to
     X.

     *See also:* *note cumtrapz: doc-cumtrapz.

 -- Function File: Z = cumtrapz (Y)
 -- Function File: Z = cumtrapz (X, Y)
 -- Function File: Z = cumtrapz (..., DIM)
     Cumulative numerical integration using trapezoidal method.
     `cumtrapz (Y)' computes the cumulative integral of the Y along the
     first non-singleton dimension.  If the argument X is omitted a
     equally spaced vector is assumed.  `cumtrapz (X, Y)' evaluates the
     cumulative integral with respect to X.

     *See also:* *note trapz: doc-trapz, *note cumsum: doc-cumsum.


File: octave.info,  Node: Orthogonal Collocation,  Prev: Functions of Multiple Variables,  Up: Numerical Integration

22.2 Orthogonal Collocation
===========================

 -- Loadable Function: [R, AMAT, BMAT, Q] = colloc (N, "left", "right")
     Compute derivative and integral weight matrices for orthogonal
     collocation using the subroutines given in J. Villadsen and M. L.
     Michelsen, `Solution of Differential Equation Models by Polynomial
     Approximation'.

   Here is an example of using `colloc' to generate weight matrices for
solving the second order differential equation U' - ALPHA * U" = 0 with
the boundary conditions U(0) = 0 and U(1) = 1.

   First, we can generate the weight matrices for N points (including
the endpoints of the interval), and incorporate the boundary conditions
in the right hand side (for a specific value of ALPHA).

     n = 7;
     alpha = 0.1;
     [r, a, b] = colloc (n-2, "left", "right");
     at = a(2:n-1,2:n-1);
     bt = b(2:n-1,2:n-1);
     rhs = alpha * b(2:n-1,n) - a(2:n-1,n);

   Then the solution at the roots R is

     u = [ 0; (at - alpha * bt) \ rhs; 1]
          => [ 0.00; 0.004; 0.01 0.00; 0.12; 0.62; 1.00 ]


File: octave.info,  Node: Functions of Multiple Variables,  Next: Orthogonal Collocation,  Prev: Functions of One Variable,  Up: Numerical Integration

22.3 Functions of Multiple Variables
====================================

Octave does not have built-in functions for computing the integral of
functions of multiple variables directly.  It is however possible to
compute the integral of a function of multiple variables using the
functions for one-dimensional integrals.

   To illustrate how the integration can be performed, we will integrate
the function
     f(x, y) = sin(pi*x*y)*sqrt(x*y)
   for x and y between 0 and 1.

   The first approach creates a function that integrates f with respect
to x, and then integrates that function with respect to y.  Since
`quad' is written in Fortran it cannot be called recursively.  This
means that `quad' cannot integrate a function that calls `quad', and
hence cannot be used to perform the double integration.  It is however
possible with `quadl', which is what the following code does.

     function I = g(y)
       I = ones(1, length(y));
       for i = 1:length(y)
         f = @(x) sin(pi.*x.*y(i)).*sqrt(x.*y(i));
         I(i) = quadl(f, 0, 1);
       endfor
     endfunction

     I = quadl("g", 0, 1)
           => 0.30022

   The above process can be simplified with the `dblquad' and
`triplequad' functions for integrals over two and three variables.  For
example

     I =  dblquad (@(x, y) sin(pi.*x.*y).*sqrt(x.*y), 0, 1, 0, 1)
           => 0.30022

 -- Function File:  dblquad (F, XA, XB, YA, YB, TOL, QUADF, ...)
     Numerically evaluate a double integral.  The function over with to
     integrate is defined by `F', and the interval for the integration
     is defined by `[XA, XB, YA, YB]'.  The function F must accept a
     vector X and a scalar Y, and return a vector of the same length as
     X.

     If defined, TOL defines the absolute tolerance to which to which
     to integrate each sub-integral.

     Additional arguments, are passed directly to F.  To use the default
     value for TOL one may pass an empty matrix.

     *See also:* *note triplequad: doc-triplequad, *note quad:
     doc-quad, *note quadv: doc-quadv, *note quadl: doc-quadl, *note
     quadgk: doc-quadgk, *note trapz: doc-trapz.

 -- Function File:  triplequad (F, XA, XB, YA, YB, ZA, ZB, TOL, QUADF,
          ...)
     Numerically evaluate a triple integral.  The function over which to
     integrate is defined by `F', and the interval for the integration
     is defined by `[XA, XB, YA, YB, ZA, ZB]'.  The function F must
     accept a vector X and a scalar Y, and return a vector of the same
     length as X.

     If defined, TOL defines the absolute tolerance to which to which
     to integrate each sub-integral.

     Additional arguments, are passed directly to F.  To use the default
     value for TOL one may pass an empty matrix.

     *See also:* *note dblquad: doc-dblquad, *note quad: doc-quad,
     *note quadv: doc-quadv, *note quadl: doc-quadl, *note quadgk:
     doc-quadgk, *note trapz: doc-trapz.

   The above mentioned approach works but is fairly slow, and that
problem increases exponentially with the dimensionality the problem.
Another possible solution is to use Orthogonal Collocation as described
in the previous section.  The integral of a function f(x,y) for x and y
between 0 and 1 can be approximated using n points by the sum over
`i=1:n' and `j=1:n' of `q(i)*q(j)*f(r(i),r(j))', where q and r is as
returned by `colloc(n)'.  The generalization to more than two variables
is straight forward.  The following code computes the studied integral
using n=7 points.

     f = @(x,y) sin(pi*x*y').*sqrt(x*y');
     n = 7;
     [t, A, B, q] = colloc(n);
     I = q'*f(t,t)*q;
           => 0.30022

It should be noted that the number of points determines the quality of
the approximation.  If the integration needs to be performed between a
and b instead of 0 and 1, a change of variables is needed.


File: octave.info,  Node: Differential Equations,  Next: Optimization,  Prev: Numerical Integration,  Up: Top

23 Differential Equations
*************************

Octave has built-in functions for solving ordinary differential
equations, and differential-algebraic equations.  All solvers are based
on reliable ODE routines written in Fortran.

* Menu:

* Ordinary Differential Equations::
* Differential-Algebraic Equations::


File: octave.info,  Node: Ordinary Differential Equations,  Next: Differential-Algebraic Equations,  Up: Differential Equations

23.1 Ordinary Differential Equations
====================================

The function `lsode' can be used to solve ODEs of the form

     dx
     -- = f (x, t)
     dt

using Hindmarsh's ODE solver LSODE.

 -- Loadable Function: [X, ISTATE, MSG] = lsode (FCN, X_0, T, T_CRIT)
     Solve the set of differential equations

          dx
          -- = f(x, t)
          dt

     with

          x(t_0) = x_0

     The solution is returned in the matrix X, with each row
     corresponding to an element of the vector T.  The first element of
     T should be t_0 and should correspond to the initial state of the
     system X_0, so that the first row of the output is X_0.

     The first argument, FCN, is a string, inline, or function handle
     that names the function f to call to compute the vector of right
     hand sides for the set of equations.  The function must have the
     form

          XDOT = f (X, T)

     in which XDOT and X are vectors and T is a scalar.

     If FCN is a two-element string array or a two-element cell array
     of strings, inline functions, or function handles, the first
     element names the function f described above, and the second
     element names a function to compute the Jacobian of f.  The
     Jacobian function must have the form

          JAC = j (X, T)

     in which JAC is the matrix of partial derivatives

                       | df_1  df_1       df_1 |
                       | ----  ----  ...  ---- |
                       | dx_1  dx_2       dx_N |
                       |                       |
                       | df_2  df_2       df_2 |
                       | ----  ----  ...  ---- |
                df_i   | dx_1  dx_2       dx_N |
          jac = ---- = |                       |
                dx_j   |  .    .     .    .    |
                       |  .    .      .   .    |
                       |  .    .       .  .    |
                       |                       |
                       | df_N  df_N       df_N |
                       | ----  ----  ...  ---- |
                       | dx_1  dx_2       dx_N |

     The second and third arguments specify the initial state of the
     system, x_0, and the initial value of the independent variable t_0.

     The fourth argument is optional, and may be used to specify a set
     of times that the ODE solver should not integrate past.  It is
     useful for avoiding difficulties with singularities and points
     where there is a discontinuity in the derivative.

     After a successful computation, the value of ISTATE will be 2
     (consistent with the Fortran version of LSODE).

     If the computation is not successful, ISTATE will be something
     other than 2 and MSG will contain additional information.

     You can use the function `lsode_options' to set optional
     parameters for `lsode'.

     *See also:* *note daspk: doc-daspk, *note dassl: doc-dassl, *note
     dasrt: doc-dasrt.

 -- Loadable Function:  lsode_options (OPT, VAL)
     When called with two arguments, this function allows you set
     options parameters for the function `lsode'.  Given one argument,
     `lsode_options' returns the value of the corresponding option.  If
     no arguments are supplied, the names of all the available options
     and their current values are displayed.

     Options include

    `"absolute tolerance"'
          Absolute tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector.

    `"relative tolerance"'
          Relative tolerance parameter.  Unlike the absolute tolerance,
          this parameter may only be a scalar.

          The local error test applied at each integration step is

                 abs (local error in x(i)) <= ...
                     rtol * abs (y(i)) + atol(i)

    `"integration method"'
          A string specifying the method of integration to use to solve
          the ODE system.  Valid values are

         "adams"
         "non-stiff"
               No Jacobian used (even if it is available).

         "bdf"

         "stiff"
               Use stiff backward differentiation formula (BDF) method.
               If a function to compute the Jacobian is not supplied,
               `lsode' will compute a finite difference approximation
               of the Jacobian matrix.

    `"initial step size"'
          The step size to be attempted on the first step (default is
          determined automatically).

    `"maximum order"'
          Restrict the maximum order of the solution method.  If using
          the Adams method, this option must be between 1 and 12.
          Otherwise, it must be between 1 and 5, inclusive.

    `"maximum step size"'
          Setting the maximum stepsize will avoid passing over very
          large regions  (default is not specified).

    `"minimum step size"'
          The minimum absolute step size allowed (default is 0).

    `"step limit"'
          Maximum number of steps allowed (default is 100000).

   Here is an example of solving a set of three differential equations
using `lsode'.  Given the function

     function xdot = f (x, t)

       xdot = zeros (3,1);

       xdot(1) = 77.27 * (x(2) - x(1)*x(2) + x(1) \
                 - 8.375e-06*x(1)^2);
       xdot(2) = (x(3) - x(1)*x(2) - x(2)) / 77.27;
       xdot(3) = 0.161*(x(1) - x(3));

     endfunction

and the initial condition `x0 = [ 4; 1.1; 4 ]', the set of equations
can be integrated using the command

     t = linspace (0, 500, 1000);

     y = lsode ("f", x0, t);

   If you try this, you will see that the value of the result changes
dramatically between T = 0 and 5, and again around T = 305.  A more
efficient set of output points might be

     t = [0, logspace (-1, log10(303), 150), \
             logspace (log10(304), log10(500), 150)];

   See Alan C. Hindmarsh, `ODEPACK, A Systematized Collection of ODE
Solvers', in Scientific Computing, R. S. Stepleman, editor, (1983) for
more information about the inner workings of `lsode'.


File: octave.info,  Node: Differential-Algebraic Equations,  Prev: Ordinary Differential Equations,  Up: Differential Equations

23.2 Differential-Algebraic Equations
=====================================

The function `daspk' can be used to solve DAEs of the form

     0 = f (x-dot, x, t),    x(t=0) = x_0, x-dot(t=0) = x-dot_0

where x-dot is the derivative of x.  The equation is solved using
Petzold's DAE solver DASPK.

 -- Loadable Function: [X, XDOT, ISTATE, MSG] = daspk (FCN, X_0,
          XDOT_0, T, T_CRIT)
     Solve the set of differential-algebraic equations

          0 = f (x, xdot, t)

     with

          x(t_0) = x_0, xdot(t_0) = xdot_0

     The solution is returned in the matrices X and XDOT, with each row
     in the result matrices corresponding to one of the elements in the
     vector T.  The first element of T should be t_0 and correspond to
     the initial state of the system X_0 and its derivative XDOT_0, so
     that the first row of the output X is X_0 and the first row of the
     output XDOT is XDOT_0.

     The first argument, FCN, is a string, inline, or function handle
     that names the function f to call to compute the vector of
     residuals for the set of equations.  It must have the form

          RES = f (X, XDOT, T)

     in which X, XDOT, and RES are vectors, and T is a scalar.

     If FCN is a two-element string array or a two-element cell array
     of strings, inline functions, or function handles, the first
     element names the function f described above, and the second
     element names a function to compute the modified Jacobian

                df       df
          jac = -- + c ------
                dx     d xdot

     The modified Jacobian function must have the form


          JAC = j (X, XDOT, T, C)

     The second and third arguments to `daspk' specify the initial
     condition of the states and their derivatives, and the fourth
     argument specifies a vector of output times at which the solution
     is desired, including the time corresponding to the initial
     condition.

     The set of initial states and derivatives are not strictly
     required to be consistent.  If they are not consistent, you must
     use the `daspk_options' function to provide additional information
     so that `daspk' can compute a consistent starting point.

     The fifth argument is optional, and may be used to specify a set of
     times that the DAE solver should not integrate past.  It is useful
     for avoiding difficulties with singularities and points where
     there is a discontinuity in the derivative.

     After a successful computation, the value of ISTATE will be
     greater than zero (consistent with the Fortran version of DASPK).

     If the computation is not successful, the value of ISTATE will be
     less than zero and MSG will contain additional information.

     You can use the function `daspk_options' to set optional
     parameters for `daspk'.

     *See also:* *note dassl: doc-dassl.

 -- Loadable Function:  daspk_options (OPT, VAL)
     When called with two arguments, this function allows you set
     options parameters for the function `daspk'.  Given one argument,
     `daspk_options' returns the value of the corresponding option.  If
     no arguments are supplied, the names of all the available options
     and their current values are displayed.

     Options include

    `"absolute tolerance"'
          Absolute tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the relative tolerance must also be a vector of the same
          length.

    `"relative tolerance"'
          Relative tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the absolute tolerance must also be a vector of the same
          length.

          The local error test applied at each integration step is

                 abs (local error in x(i))
                      <= rtol(i) * abs (Y(i)) + atol(i)

    `"compute consistent initial condition"'
          Denoting the differential variables in the state vector by
          `Y_d' and the algebraic variables by `Y_a', `ddaspk' can solve
          one of two initialization problems:

            1. Given Y_d, calculate Y_a and Y'_d

            2. Given Y', calculate Y.

          In either case, initial values for the given components are
          input, and initial guesses for the unknown components must
          also be provided as input.  Set this option to 1 to solve the
          first problem, or 2 to solve the second (the default is 0, so
          you must provide a set of initial conditions that are
          consistent).

          If this option is set to a nonzero value, you must also set
          the `"algebraic variables"' option to declare which variables
          in the problem are algebraic.

    `"use initial condition heuristics"'
          Set to a nonzero value to use the initial condition
          heuristics options described below.

    `"initial condition heuristics"'
          A vector of the following parameters that can be used to
          control the initial condition calculation.

         `MXNIT'
               Maximum number of Newton iterations (default is 5).

         `MXNJ'
               Maximum number of Jacobian evaluations (default is 6).

         `MXNH'
               Maximum number of values of the artificial stepsize
               parameter to be tried if the `"compute consistent
               initial condition"' option has been set to 1 (default is
               5).

               Note that the maximum total number of Newton iterations
               allowed is `MXNIT*MXNJ*MXNH' if the `"compute consistent
               initial condition"' option has been set to 1 and
               `MXNIT*MXNJ' if it is set to 2.

         `LSOFF'
               Set to a nonzero value to disable the linesearch
               algorithm (default is 0).

         `STPTOL'
               Minimum scaled step in linesearch algorithm (default is
               eps^(2/3)).

         `EPINIT'
               Swing factor in the Newton iteration convergence test.
               The test is applied to the residual vector,
               premultiplied by the approximate Jacobian.  For
               convergence, the weighted RMS norm of this vector
               (scaled by the error weights) must be less than
               `EPINIT*EPCON', where `EPCON' = 0.33 is the analogous
               test constant used in the time steps.  The default is
               `EPINIT' = 0.01.

    `"print initial condition info"'
          Set this option to a nonzero value to display detailed
          information about the initial condition calculation (default
          is 0).

    `"exclude algebraic variables from error test"'
          Set to a nonzero value to exclude algebraic variables from
          the error test.  You must also set the `"algebraic
          variables"' option to declare which variables in the problem
          are algebraic (default is 0).

    `"algebraic variables"'
          A vector of the same length as the state vector.  A nonzero
          element indicates that the corresponding element of the state
          vector is an algebraic variable (i.e., its derivative does
          not appear explicitly in the equation set.

          This option is required by the `compute consistent initial
          condition"' and `"exclude algebraic variables from error
          test"' options.

    `"enforce inequality constraints"'
          Set to one of the following values to enforce the inequality
          constraints specified by the `"inequality constraint types"'
          option (default is 0).

            1. To have constraint checking only in the initial
               condition calculation.

            2. To enforce constraint checking during the integration.

            3. To enforce both options 1 and 2.

    `"inequality constraint types"'
          A vector of the same length as the state specifying the type
          of inequality constraint.  Each element of the vector
          corresponds to an element of the state and should be assigned
          one of the following codes

         -2
               Less than zero.

         -1
               Less than or equal to zero.

         0
               Not constrained.

         1
               Greater than or equal to zero.

         2
               Greater than zero.

          This option only has an effect if the `"enforce inequality
          constraints"' option is nonzero.

    `"initial step size"'
          Differential-algebraic problems may occasionally suffer from
          severe scaling difficulties on the first step.  If you know a
          great deal about the scaling of your problem, you can help to
          alleviate this problem by specifying an initial stepsize
          (default is computed automatically).

    `"maximum order"'
          Restrict the maximum order of the solution method.  This
          option must be between 1 and 5, inclusive (default is 5).

    `"maximum step size"'
          Setting the maximum stepsize will avoid passing over very
          large regions (default is not specified).

   Octave also includes DASSL, an earlier version of DASPK, and DASRT,
which can be used to solve DAEs with constraints (stopping conditions).

 -- Loadable Function: [X, XDOT, ISTATE, MSG] = dassl (FCN, X_0,
          XDOT_0, T, T_CRIT)
     Solve the set of differential-algebraic equations

          0 = f (x, xdot, t)

     with

          x(t_0) = x_0, xdot(t_0) = xdot_0

     The solution is returned in the matrices X and XDOT, with each row
     in the result matrices corresponding to one of the elements in the
     vector T.  The first element of T should be t_0 and correspond to
     the initial state of the system X_0 and its derivative XDOT_0, so
     that the first row of the output X is X_0 and the first row of the
     output XDOT is XDOT_0.

     The first argument, FCN, is a string, inline, or function handle
     that names the function f to call to compute the vector of
     residuals for the set of equations.  It must have the form

          RES = f (X, XDOT, T)

     in which X, XDOT, and RES are vectors, and T is a scalar.

     If FCN is a two-element string array or a two-element cell array
     of strings, inline functions, or function handles, the first
     element names the function f described above, and the second
     element names a function to compute the modified Jacobian

                df       df
          jac = -- + c ------
                dx     d xdot

     The modified Jacobian function must have the form


          JAC = j (X, XDOT, T, C)

     The second and third arguments to `dassl' specify the initial
     condition of the states and their derivatives, and the fourth
     argument specifies a vector of output times at which the solution
     is desired, including the time corresponding to the initial
     condition.

     The set of initial states and derivatives are not strictly
     required to be consistent.  In practice, however, DASSL is not
     very good at determining a consistent set for you, so it is best
     if you ensure that the initial values result in the function
     evaluating to zero.

     The fifth argument is optional, and may be used to specify a set of
     times that the DAE solver should not integrate past.  It is useful
     for avoiding difficulties with singularities and points where
     there is a discontinuity in the derivative.

     After a successful computation, the value of ISTATE will be
     greater than zero (consistent with the Fortran version of DASSL).

     If the computation is not successful, the value of ISTATE will be
     less than zero and MSG will contain additional information.

     You can use the function `dassl_options' to set optional
     parameters for `dassl'.

     *See also:* *note daspk: doc-daspk, *note dasrt: doc-dasrt, *note
     lsode: doc-lsode.

 -- Loadable Function:  dassl_options (OPT, VAL)
     When called with two arguments, this function allows you set
     options parameters for the function `dassl'.  Given one argument,
     `dassl_options' returns the value of the corresponding option.  If
     no arguments are supplied, the names of all the available options
     and their current values are displayed.

     Options include

    `"absolute tolerance"'
          Absolute tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the relative tolerance must also be a vector of the same
          length.

    `"relative tolerance"'
          Relative tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the absolute tolerance must also be a vector of the same
          length.

          The local error test applied at each integration step is

                 abs (local error in x(i))
                      <= rtol(i) * abs (Y(i)) + atol(i)

    `"compute consistent initial condition"'
          If nonzero, `dassl' will attempt to compute a consistent set
          of initial conditions.  This is generally not reliable, so it
          is best to provide a consistent set and leave this option set
          to zero.

    `"enforce nonnegativity constraints"'
          If you know that the solutions to your equations will always
          be nonnegative, it may help to set this parameter to a nonzero
          value.  However, it is probably best to try leaving this
          option set to zero first, and only setting it to a nonzero
          value if that doesn't work very well.

    `"initial step size"'
          Differential-algebraic problems may occasionally suffer from
          severe scaling difficulties on the first step.  If you know a
          great deal about the scaling of your problem, you can help to
          alleviate this problem by specifying an initial stepsize.

    `"maximum order"'
          Restrict the maximum order of the solution method.  This
          option must be between 1 and 5, inclusive.

    `"maximum step size"'
          Setting the maximum stepsize will avoid passing over very
          large regions  (default is not specified).

    `"step limit"'
          Maximum number of integration steps to attempt on a single
          call to the underlying Fortran code.

 -- Loadable Function: [X, XDOT, T_OUT, ISTAT, MSG] = dasrt (FCN [, G],
          X_0, XDOT_0, T [, T_CRIT])
     Solve the set of differential-algebraic equations

          0 = f (x, xdot, t)

     with

          x(t_0) = x_0, xdot(t_0) = xdot_0

     with functional stopping criteria (root solving).

     The solution is returned in the matrices X and XDOT, with each row
     in the result matrices corresponding to one of the elements in the
     vector T_OUT.  The first element of T should be t_0 and correspond
     to the initial state of the system X_0 and its derivative XDOT_0,
     so that the first row of the output X is X_0 and the first row of
     the output XDOT is XDOT_0.

     The vector T provides an upper limit on the length of the
     integration.  If the stopping condition is met, the vector T_OUT
     will be shorter than T, and the final element of T_OUT will be the
     point at which the stopping condition was met, and may not
     correspond to any element of the vector T.

     The first argument, FCN, is a string, inline, or function handle
     that names the function f to call to compute the vector of
     residuals for the set of equations.  It must have the form

          RES = f (X, XDOT, T)

     in which X, XDOT, and RES are vectors, and T is a scalar.

     If FCN is a two-element string array or a two-element cell array
     of strings, inline functions, or function handles, the first
     element names the function f described above, and the second
     element names a function to compute the modified Jacobian

                df       df
          jac = -- + c ------
                dx     d xdot

     The modified Jacobian function must have the form


          JAC = j (X, XDOT, T, C)

     The optional second argument names a function that defines the
     constraint functions whose roots are desired during the
     integration.  This function must have the form

          G_OUT = g (X, T)

     and return a vector of the constraint function values.  If the
     value of any of the constraint functions changes sign, DASRT will
     attempt to stop the integration at the point of the sign change.

     If the name of the constraint function is omitted, `dasrt' solves
     the same problem as `daspk' or `dassl'.

     Note that because of numerical errors in the constraint functions
     due to roundoff and integration error, DASRT may return false
     roots, or return the same root at two or more nearly equal values
     of T.  If such false roots are suspected, the user should consider
     smaller error tolerances or higher precision in the evaluation of
     the constraint functions.

     If a root of some constraint function defines the end of the
     problem, the input to DASRT should nevertheless allow integration
     to a point slightly past that root, so that DASRT can locate the
     root by interpolation.

     The third and fourth arguments to `dasrt' specify the initial
     condition of the states and their derivatives, and the fourth
     argument specifies a vector of output times at which the solution
     is desired, including the time corresponding to the initial
     condition.

     The set of initial states and derivatives are not strictly
     required to be consistent.  In practice, however, DASSL is not
     very good at determining a consistent set for you, so it is best
     if you ensure that the initial values result in the function
     evaluating to zero.

     The sixth argument is optional, and may be used to specify a set of
     times that the DAE solver should not integrate past.  It is useful
     for avoiding difficulties with singularities and points where
     there is a discontinuity in the derivative.

     After a successful computation, the value of ISTATE will be
     greater than zero (consistent with the Fortran version of DASSL).

     If the computation is not successful, the value of ISTATE will be
     less than zero and MSG will contain additional information.

     You can use the function `dasrt_options' to set optional
     parameters for `dasrt'.

     *See also:* *note daspk: doc-daspk, *note dasrt: doc-dasrt, *note
     lsode: doc-lsode.

 -- Loadable Function:  dasrt_options (OPT, VAL)
     When called with two arguments, this function allows you set
     options parameters for the function `dasrt'.  Given one argument,
     `dasrt_options' returns the value of the corresponding option.  If
     no arguments are supplied, the names of all the available options
     and their current values are displayed.

     Options include

    `"absolute tolerance"'
          Absolute tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the relative tolerance must also be a vector of the same
          length.

    `"relative tolerance"'
          Relative tolerance.  May be either vector or scalar.  If a
          vector, it must match the dimension of the state vector, and
          the absolute tolerance must also be a vector of the same
          length.

          The local error test applied at each integration step is
                 abs (local error in x(i)) <= ...
                     rtol(i) * abs (Y(i)) + atol(i)

    `"initial step size"'
          Differential-algebraic problems may occasionally suffer from
          severe scaling difficulties on the first step.  If you know a
          great deal about the scaling of your problem, you can help to
          alleviate this problem by specifying an initial stepsize.

    `"maximum order"'
          Restrict the maximum order of the solution method.  This
          option must be between 1 and 5, inclusive.

    `"maximum step size"'
          Setting the maximum stepsize will avoid passing over very
          large regions.

    `"step limit"'
          Maximum number of integration steps to attempt on a single
          call to the underlying Fortran code.

   See K. E. Brenan, et al., `Numerical Solution of Initial-Value
Problems in Differential-Algebraic Equations', North-Holland (1989) for
more information about the implementation of DASSL.

